<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>ELT approach for Data Pipelines | Learnings in IT</title>
<meta name="keywords" content="DBT, ELT, ETL, Python, Go, PostgreSQL, CSV, data pipeline, nsetools">
<meta name="description" content="Introduction
While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,

Extract - typically involves retrieving data from source. This could also be via streaming
Transform - Apply transformation to the extracted data.
Load -  Loading the data in Operation Data store (ODS) or data warehouse
Refer here for more details on ETL. ETL has been made easy by tools like Talend, SSIS and so on.

However, there has been shift from above approach due to,">
<meta name="author" content="Sachin Sunkle">
<link rel="canonical" href="http://localhost:1313/posts/elt/">
<link crossorigin="anonymous" href="/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css" integrity="sha256-j&#43;ECM6cGvIfy4Is8&#43;XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/posts/elt/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><meta property="og:url" content="http://localhost:1313/posts/elt/">
  <meta property="og:site_name" content="Learnings in IT">
  <meta property="og:title" content="ELT approach for Data Pipelines">
  <meta property="og:description" content="Introduction While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,
Extract - typically involves retrieving data from source. This could also be via streaming Transform - Apply transformation to the extracted data. Load - Loading the data in Operation Data store (ODS) or data warehouse Refer here for more details on ETL. ETL has been made easy by tools like Talend, SSIS and so on. However, there has been shift from above approach due to,">
  <meta property="og:locale" content="en">
  <meta property="og:type" content="article">
    <meta property="article:section" content="posts">
    <meta property="article:published_time" content="2021-03-14T00:00:00+05:30">
    <meta property="article:modified_time" content="2021-03-14T00:00:00+05:30">
    <meta property="article:tag" content="DBT">
    <meta property="article:tag" content="ELT">
    <meta property="article:tag" content="ETL">
    <meta property="article:tag" content="Python">
    <meta property="article:tag" content="Go">
    <meta property="article:tag" content="Postgresql">
      <meta property="og:image" content="http://localhost:1313/images/papermod-cover.png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="http://localhost:1313/images/papermod-cover.png">
<meta name="twitter:title" content="ELT approach for Data Pipelines">
<meta name="twitter:description" content="Introduction
While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,

Extract - typically involves retrieving data from source. This could also be via streaming
Transform - Apply transformation to the extracted data.
Load -  Loading the data in Operation Data store (ODS) or data warehouse
Refer here for more details on ETL. ETL has been made easy by tools like Talend, SSIS and so on.

However, there has been shift from above approach due to,">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "http://localhost:1313/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "ELT approach for Data Pipelines",
      "item": "http://localhost:1313/posts/elt/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "ELT approach for Data Pipelines",
  "name": "ELT approach for Data Pipelines",
  "description": "Introduction While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,\nExtract - typically involves retrieving data from source. This could also be via streaming Transform - Apply transformation to the extracted data. Load - Loading the data in Operation Data store (ODS) or data warehouse Refer here for more details on ETL. ETL has been made easy by tools like Talend, SSIS and so on. However, there has been shift from above approach due to,\n",
  "keywords": [
    "DBT", "ELT", "ETL", "Python", "Go", "PostgreSQL", "CSV", "data pipeline", "nsetools"
  ],
  "articleBody": "Introduction While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,\nExtract - typically involves retrieving data from source. This could also be via streaming Transform - Apply transformation to the extracted data. Load - Loading the data in Operation Data store (ODS) or data warehouse Refer here for more details on ETL. ETL has been made easy by tools like Talend, SSIS and so on. However, there has been shift from above approach due to,\nNeed to handle different kinds of data (Structured and Unstructured) hugh volumes of data (IOT, Customer data management) Availability of cheaper storage and compute along with availability of internet scale cloud based data warehouses has recently caused wide adoption of ELT (Extract-transform-load) over ETL.\nELT offers an alternative to ETL in which data is loaded into the warehouse (sometimes in storage area called as data lake) before transforming it. It allows focussing on extraction and loading with heavy transformation offloaded to later stage. Since the transformation happens in the warehouse, it can potentially be defined using SQL (thus using same language across the pipeline). This allows more roles (say Data Analysts) to contribute to (or entirely own) the transformation logic. Data warehouse becomes single source of truth for data. Ref: ETL vs ELT\nTypically, Data flow pipeline consists of below phases (it also lists available tools for each phase),\nIngestion - Airbyte, Fivetran, Stitch Warehousing - Snowflake, BigQuery, Redshift, PostgreSQL Transformation - dbt Orchestration - Airflow, Prefect, Dagster BI - Superset, Metabase, Redash, Looker etc. I think the best way to understand the landscape is to use above tools. So i decided to implement below problem statement. The requirement is to run a weekly process that,\nDownloads list of CNX 500 companies from Exchange’s web site For each of the company , get Last traded price(ltp) and 52 week high price (yearlyhigh) Exclude companies having ltp \u003c 20 or ltp \u003e 50000 Rank companies by closeness of ltp to yearlyhigh Prepare buy list of up to 20 such companies. Earlier short listed stocks, which are not in top 20 this week or further than 5% from their yearlyhigh, should be marked for sell. Above is hypothetical example and using full fledged data stack may be overkill but should suffice the purpose of this article.\nE \u0026 L in ELT - Get the list of CNX 500 Companies and also get stock price for each of them Below are some of the options available for this task under extract and load category,\nUse Python to download list of stocks and then use yfinance to get the price and yearly high. Use tool like Airbyte which provides declarative way of importing the data via HTTP. I am planning to explore this option later. Use Go to perform the task. I decided to go with this one and code is available at here. It downloads CSV file from Exchange’s website (containing list of stocks in Index) and loads them to database. Since Yahoo finance no longer provides Free tier for API, It uses htmlquery library to parse HTML and retrieve stock price and yearly high value. T in ELT - Transform the company-wise data to arrive at weekly list of momentum stocks This is implemented using dbt. dbt (Data Build Tool) is a framework to facilitate transformations using SQL along with version control, automates tests, support for incremental load, snapshots and so on. It has notion of project or workspace that many developers are familiar with. It is offered as Command line interface (CLI) as well as on cloud which also provides web based UI. I have used CLI for this exercise. For a quick recap of dbt folder structure, refer [here]https://towardsdatascience.com/data-stacks-for-fun-nonprofit-part-ii-d375d824abf3).\nSource code of dbt project here. We will go through key part of this project which are Models that carry out the transformation. After the initial setup of dbt like configuring target (i.e. data source which in this case is a PostgreSQL database), below are Models used,\nSince Loading of company-wise data is already done in earlier step, next step is to rank the companies w.r.t. closeness to their yearly high. Below is dbt SQL which does it (At run time, dbt converts below SQL to the one understood by the Target database),\n``` {{ config( materialized='incremental', ) }} with cnxcompanies as ( select symbol, company, ltp, yearlyhigh, updatedat, rank() over (order by yearlyhigh-ltp) as diff_rank from {{ source('datastore', 'cnx500companies') }} where yearlyhigh::money::numeric::float8 - ltp::money::numeric::float8 \u003e 0 and ltp::money::numeric::float8 \u003e 20 and ltp::money::numeric::float8 \u003c 50000 ), cnxtopstocks as ( select symbol, company, ltp, yearlyhigh, updatedat, diff_rank from cnxcompanies order by updatedat desc,diff_rank ) select * from cnxtopstocks ``` Above model creates corresponding table in database (as such dbt abstracts changes to database from developer and manages it on its own). Note that model is marked incremental so that it doesn’t overwrite the table on every run but rather incrementally applies changes.\nNext step is to arrive at Weekly list of stocks to buy and even sell those which are lacking momentum.\n``` {{ config( materialized='incremental', unique_key='concat(symbol,updatedat)' ) }} with currentlist as ( select distinct symbol, company, ltp, yearlyhigh, updatedat,diff_rank,'buy' as buyorsell from {{ref('rankstocks')}} where (yearlyhigh-ltp)/ltp*100 \u003c= 5 order by updatedat desc, diff_rank limit 20 ), finallist as ( {% if is_incremental() %} select symbol, company, ltp, yearlyhigh, updatedat,diff_rank,'sell' as buyorsell from {{this}} as oldlist where not exists (select symbol from currentlist where symbol=oldlist.symbol and (yearlyhigh-ltp)/ltp*100 \u003c= 5 ) union select symbol, company, ltp, yearlyhigh, updatedat,diff_rank,'buy' as buyorsell from currentlist where not exists (select symbol from {{this}} where symbol=currentlist.symbol and buyorsell='buy') {% else %} select * from currentlist {% endif %} ) select * from finallist ``` This model refers to earlier one using {{..}} jinja directive. It also refers to itself using {{this}} directive.\nAmong others, below are key feature of DBT that were observed,\nConcept of Project/Workspace which programmers are typically familiar with Using SQL for Data Transformation Support for Version control Support for testing Support for incremental load Support for snapshots Automatic schema updates Out of the box Documentation browser covering traceability across sources and models. Orchestration After completing ELT aspects, now it’s time to orchestrate this pipeline wherein the whole process will run every week. Typically, one can use task scheduler like Airflow or Prefect to do this. But for the purpose of this article, lets use at on windows (or cron if you are using Linux).\nso a simplest possible batch file (as below),\nset http_proxy= set https_proxy= .\\gover\\go run . .\\.venv\\scripts\\activate \u0026 .\\dbt\\dbt run will run the whole process and generate weekly list in weeklylist table in database. This batch file can be scheduled to run on weekly basis using command at 23:00 /every:F runscript.bat.\nThis is very basic approach to scheduling (with no error handling/retries or monitoring). Hopefully, i will be able to work on these part (something like this). Till then…\nUseful References Reverse ETL Data stacks for Fun and Profit What warehouse to use Build Data Lake in PostgreSQL using FDW, Singer, Metabase Happy Coding !!\n",
  "wordCount" : "1183",
  "inLanguage": "en",
  "image": "http://localhost:1313/images/papermod-cover.png","datePublished": "2021-03-14T00:00:00+05:30",
  "dateModified": "2021-03-14T00:00:00+05:30",
  "author":{
    "@type": "Person",
    "name": "Sachin Sunkle"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "http://localhost:1313/posts/elt/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Learnings in IT",
    "logo": {
      "@type": "ImageObject",
      "url": "http://localhost:1313/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/" accesskey="h" title="Learnings in IT (Alt + H)">Learnings in IT</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/about/" title="About">
                    <span>About</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/projects/" title="Projects">
                    <span>Projects</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/links/home" title="Useful Links">
                    <span>Useful Links</span>
                </a>
            </li>
            <li>
                <a href="https://gist.github.com/sachinsu" title="Gists">
                    <span>Gists</span>&nbsp;
                    <svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round"
                        stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12">
                        <path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"></path>
                        <path d="M15 3h6v6"></path>
                        <path d="M10 14L21 3"></path>
                    </svg>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      ELT approach for Data Pipelines
    </h1>
    <div class="post-meta"><span title='2021-03-14 00:00:00 +0530 IST'>March 14, 2021</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Sachin Sunkle

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#introduction" aria-label="Introduction">Introduction</a><ul>
                        
                <li>
                    <a href="#e--l-in-elt----get-the-list-of-cnx-500-companies-and-also-get-stock-price-for-each-of-them" aria-label="E &amp; L in ELT - Get the list of CNX 500 Companies and also get stock price for each of them">E &amp; L in ELT - Get the list of CNX 500 Companies and also get stock price for each of them</a></li>
                <li>
                    <a href="#t-in-elt---transform-the-company-wise-data-to-arrive-at-weekly-list-of-momentum-stocks" aria-label="T in ELT - Transform the company-wise data to arrive at weekly list of momentum stocks">T in ELT - Transform the company-wise data to arrive at weekly list of momentum stocks</a></li>
                <li>
                    <a href="#orchestration" aria-label="Orchestration">Orchestration</a></li>
                <li>
                    <a href="#useful-references" aria-label="Useful References">Useful References</a>
                </li>
            </ul>
            </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="introduction">Introduction<a hidden class="anchor" aria-hidden="true" href="#introduction">#</a></h2>
<p>While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,</p>
<ul>
<li><strong>Extract</strong> - typically involves retrieving data from source. This could also be via streaming</li>
<li><strong>Transform</strong> - Apply transformation to the extracted data.</li>
<li><strong>Load</strong> -  Loading the data in Operation Data store (ODS) or data warehouse
Refer <a href=https://www.sas.com/en_us/insights/data-management/what-is-etl.html#close
    
    target=_blank rel="noopener noreferrer"
>here</a> for more details on ETL. ETL has been made easy by tools like <a href=https://www.talend.com/products/talend-open-studio/
    
    target=_blank rel="noopener noreferrer"
>Talend</a>, <a href=https://docs.microsoft.com/en-us/sql/integration-services/sql-server-integration-services
    
    target=_blank rel="noopener noreferrer"
>SSIS</a> and so on.</li>
</ul>
<p>However, there has been shift from above approach due to,</p>
<ul>
<li>Need to handle different kinds of data (Structured and Unstructured)</li>
<li>hugh volumes of data (IOT, Customer data management)</li>
<li>Availability of cheaper storage and compute along with availability of internet scale cloud based data warehouses</li>
</ul>
<p>has recently caused wide adoption of ELT (Extract-transform-load) over ETL.</p>
<p>ELT offers an alternative to ETL in which data is loaded into the warehouse (sometimes in storage area called as data lake) before transforming it. It allows focussing on extraction and loading with heavy transformation offloaded to later stage. Since the transformation happens in the warehouse, it can potentially be defined using SQL (thus using same language across the pipeline). This allows more roles (say Data Analysts) to contribute to (or entirely own) the transformation logic. Data warehouse becomes single source of truth for data. Ref: <a href=https://dataschool.com/data-governance/etl-vs-elt/
    
    target=_blank rel="noopener noreferrer"
>ETL vs ELT</a></p>
<p>Typically, Data flow pipeline consists of below phases (it also lists available tools for each phase),</p>
<ul>
<li>Ingestion - <a href=https://airbyte.io
    
    target=_blank rel="noopener noreferrer"
>Airbyte</a>, <a href=https://fivetran.com
    
    target=_blank rel="noopener noreferrer"
>Fivetran</a>, <a href=https://stitchdata.com
    
    target=_blank rel="noopener noreferrer"
>Stitch</a></li>
<li>Warehousing - <a href=https://snowflake.com
    
    target=_blank rel="noopener noreferrer"
>Snowflake</a>, <a href=https://cloud.google.com/bigquery
    
    target=_blank rel="noopener noreferrer"
>BigQuery</a>, <a href=https://aws.amazon.com/redshift
    
    target=_blank rel="noopener noreferrer"
>Redshift</a>, <a href=https://postgresql.org
    
    target=_blank rel="noopener noreferrer"
>PostgreSQL</a></li>
<li>Transformation - <a href=https://getdbt.com
    
    target=_blank rel="noopener noreferrer"
>dbt</a></li>
<li>Orchestration - <a href=airflow.apache.org
    
    
>Airflow</a>, <a href=https://prefect.io
    
    target=_blank rel="noopener noreferrer"
>Prefect</a>, <a href=https://dagster.io
    
    target=_blank rel="noopener noreferrer"
>Dagster</a></li>
<li>BI - <a href=superset.apache.org
    
    
>Superset</a>, <a href=https://metabase.com
    
    target=_blank rel="noopener noreferrer"
>Metabase</a>, <a href=redash.io
    
    
>Redash</a>, <a href=looker.com
    
    
>Looker</a> etc.</li>
</ul>
<p>I think the best way to understand the landscape is to use above tools. So i decided to implement below problem statement. The requirement is to run a weekly process that,</p>
<ol>
<li>Downloads list of CNX 500 companies from Exchange&rsquo;s web site</li>
<li>For each of the company , get Last traded price(<code>ltp</code>) and 52 week high price (<code>yearlyhigh</code>)</li>
<li>Exclude companies having ltp &lt; 20 or ltp &gt; 50000</li>
<li>Rank companies by closeness of <code>ltp</code> to <code>yearlyhigh</code></li>
<li>Prepare <code>buy</code> list of up to 20 such companies. Earlier short listed stocks, which are not in top 20 this week or further than 5% from their <code>yearlyhigh</code>, should be marked for <code>sell</code>.</li>
</ol>
<p>Above is hypothetical example and using full fledged data stack may be overkill but should suffice the purpose of this article.</p>
<h3 id="e--l-in-elt----get-the-list-of-cnx-500-companies-and-also-get-stock-price-for-each-of-them"><code>E</code> &amp; <code>L</code> in ELT -  Get the list of CNX 500 Companies and also get stock price for each of them<a hidden class="anchor" aria-hidden="true" href="#e--l-in-elt----get-the-list-of-cnx-500-companies-and-also-get-stock-price-for-each-of-them">#</a></h3>
<p>Below are some of the options available for this task under <code>extract</code> and <code>load</code> category,</p>
<ul>
<li>Use Python to download list of stocks and then use <a href=https://pypi.org/project/yfinance/
    
    target=_blank rel="noopener noreferrer"
>yfinance</a> to get the price and yearly high.</li>
<li>Use tool like <a href=https://airbyte.io
    
    target=_blank rel="noopener noreferrer"
>Airbyte</a> which provides declarative way of importing the data via HTTP. I am planning to explore this option later.</li>
<li>Use Go to perform the task. I decided to go with this one and code is available at <a href=https://github.com/sachinsu/momentumflow/tree/main/gover
    
    target=_blank rel="noopener noreferrer"
>here</a>. It downloads CSV file from Exchange&rsquo;s website (containing list of stocks in Index) and loads them to database. Since Yahoo finance no longer provides Free tier for API, It uses <a href=github.com/antchfx/htmlquery
    
    
>htmlquery</a> library to parse HTML and retrieve stock price and yearly high value.</li>
</ul>
<h3 id="t-in-elt---transform-the-company-wise-data-to-arrive-at-weekly-list-of-momentum-stocks"><code>T</code> in ELT - Transform the company-wise data to arrive at weekly list of momentum stocks<a hidden class="anchor" aria-hidden="true" href="#t-in-elt---transform-the-company-wise-data-to-arrive-at-weekly-list-of-momentum-stocks">#</a></h3>
<p>This is implemented using <a href=https://getdbt.com
    
    target=_blank rel="noopener noreferrer"
>dbt</a>. dbt (Data Build Tool) is a framework to facilitate transformations using SQL along with version control, automates tests, support for incremental load, snapshots and so on. It has <code>notion</code> of <strong>project</strong> or <strong>workspace</strong> that many developers are familiar with.
It is offered as Command line interface (CLI)  as well as on cloud which also provides web based UI. I have used CLI for this exercise. For a quick recap of dbt folder structure, refer [here]https://towardsdatascience.com/data-stacks-for-fun-nonprofit-part-ii-d375d824abf3).</p>
<p>Source code of dbt project <a href=https://github.com/sachinsu/momentumflow/tree/main/dbt
    
    target=_blank rel="noopener noreferrer"
>here</a>.  We will go through key part of this project which are Models that carry out the transformation. After the initial setup of dbt like configuring target (i.e. data source which in this case is a PostgreSQL database), below are Models used,</p>
<ul>
<li>
<p>Since Loading  of company-wise data is already done in earlier step, next step is to rank the companies w.r.t. <code>closeness</code> to their yearly high. Below is <code>dbt</code> SQL which does it (At run time, dbt converts below SQL to the one understood by the Target database),</p>
<pre><code> ```

 {{
     config(
         materialized='incremental',
     )
 }}

 with
     cnxcompanies
     as
     (

         select
             symbol,
             company,
             ltp,
             yearlyhigh,
             updatedat,
             rank() over (order by yearlyhigh-ltp) as diff_rank
         from {{ source('datastore', 'cnx500companies') }}
     where yearlyhigh::money::numeric::float8 - ltp::money::numeric::float8 &gt; 0 and ltp::money::numeric::float8 &gt; 20 and ltp::money::numeric::float8 &lt; 50000

 ),
 cnxtopstocks as
 (

     select
     symbol,
     company,
     ltp,
     yearlyhigh,
     updatedat,
     diff_rank
     from  cnxcompanies
     order by updatedat desc,diff_rank 
 )

 select * from cnxtopstocks

 ```
</code></pre>
<p>Above model creates corresponding table in database (as such dbt abstracts changes to database from developer and manages it on its own). Note that model is marked <code>incremental</code> so that it doesn&rsquo;t overwrite the table on every run but rather incrementally applies changes.</p>
</li>
<li>
<p>Next step is to arrive at Weekly list of stocks to <code>buy</code> and even <code>sell</code> those which are lacking momentum.</p>
<pre><code>  ```

  {{
  config(
  materialized='incremental',
  unique_key='concat(symbol,updatedat)'
      )
  }}

  with currentlist as (
      select distinct symbol,
              company,
              ltp,
              yearlyhigh,
              updatedat,diff_rank,'buy' as buyorsell
      from  {{ref('rankstocks')}} 
      where (yearlyhigh-ltp)/ltp*100 &lt;= 5
      order by updatedat desc, diff_rank
      limit 20
  ),
  finallist as (
      {% if is_incremental() %}
          select symbol,
              company,
              ltp,
              yearlyhigh,
              updatedat,diff_rank,'sell' as buyorsell from {{this}} as oldlist
              where not exists (select symbol from currentlist where symbol=oldlist.symbol and (yearlyhigh-ltp)/ltp*100 &lt;= 5 )
          union 
          select  symbol,
              company,
              ltp,
              yearlyhigh,
              updatedat,diff_rank,'buy' as buyorsell  from  currentlist 
              where not exists (select symbol from {{this}} where symbol=currentlist.symbol and buyorsell='buy')   
      {% else %}
          select * from currentlist
      {% endif %}
  )


  select * from finallist

  ```
</code></pre>
<p>This model refers to earlier one using <code>{{..}}</code> jinja directive. It also refers to itself using <code>{{this}}</code> directive.</p>
<p>Among others, below are key feature of DBT that were observed,</p>
<ul>
<li>Concept of Project/Workspace which programmers are typically familiar with</li>
<li>Using SQL for Data Transformation</li>
<li>Support for Version control</li>
<li>Support for testing</li>
<li>Support for incremental load</li>
<li>Support for snapshots</li>
<li>Automatic schema updates</li>
<li>Out of the box Documentation browser covering traceability across sources and models.</li>
</ul>
</li>
</ul>
<h3 id="orchestration">Orchestration<a hidden class="anchor" aria-hidden="true" href="#orchestration">#</a></h3>
<p>After completing <code>ELT</code> aspects, now it&rsquo;s time to  orchestrate this pipeline wherein the whole process will run every week. Typically, one can use task scheduler like Airflow or Prefect to do this. But for the purpose of this article, lets use <a href=https://docs.microsoft.com/en-us/troubleshoot/windows-client/system-management-components/use-at-command-to-schedule-tasks
    
    target=_blank rel="noopener noreferrer"
>at</a> on windows (or <a href=https://en.wikipedia.org/wiki/Cron
    
    target=_blank rel="noopener noreferrer"
>cron</a> if you are using Linux).</p>
<p>so a simplest possible batch file (as below),</p>
<pre tabindex="0"><code>set http_proxy=
set https_proxy=

.\gover\go run .

.\.venv\scripts\activate &amp; .\dbt\dbt run
</code></pre><p>will run the whole process and generate weekly list in <code>weeklylist</code> table in database. This batch file can be scheduled to run on weekly basis using command <code>at 23:00 /every:F runscript.bat</code>.</p>
<p>This is very basic approach to scheduling (with no error handling/retries or monitoring). Hopefully, i will be able to work on these part (something like <a href=https://docs.airbyte.io/tutorials/connecting-el-with-t-using-dbt
    
    target=_blank rel="noopener noreferrer"
>this</a>). Till then&hellip;</p>
<h3 id="useful-references">Useful References<a hidden class="anchor" aria-hidden="true" href="#useful-references">#</a></h3>
<ul>
<li><a href=https://medium.com/memory-leak/reverse-etl-a-primer-4e6694dcc7fb
    
    target=_blank rel="noopener noreferrer"
>Reverse ETL</a></li>
<li><a href=https://towardsdatascience.com/data-stacks-for-fun-nonprofit-part-ii-d375d824abf3
    
    target=_blank rel="noopener noreferrer"
>Data stacks for Fun and Profit</a></li>
<li><a href=https://dataschool.com/data-governance/what-warehouse-to-use/
    
    target=_blank rel="noopener noreferrer"
>What warehouse to use</a></li>
<li><a href=https://tech.fretlink.com/build-your-own-data-lake-for-reporting-purposes/
    
    target=_blank rel="noopener noreferrer"
>Build Data Lake in PostgreSQL using FDW, Singer, Metabase</a></li>
</ul>
<p>Happy Coding !!</p>
<hr>
<script src="https://utteranc.es/client.js" repo="sachinsu/sachinsu.github.io" issue-term="title" label="blogcomment"
    theme="github-light" crossorigin="anonymous" async></script>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/tags/dbt/">DBT</a></li>
      <li><a href="http://localhost:1313/tags/elt/">ELT</a></li>
      <li><a href="http://localhost:1313/tags/etl/">ETL</a></li>
      <li><a href="http://localhost:1313/tags/python/">Python</a></li>
      <li><a href="http://localhost:1313/tags/go/">Go</a></li>
      <li><a href="http://localhost:1313/tags/postgresql/">Postgresql</a></li>
      <li><a href="http://localhost:1313/tags/csv/">CSV</a></li>
      <li><a href="http://localhost:1313/tags/data-pipeline/">Data Pipeline</a></li>
      <li><a href="http://localhost:1313/tags/nsetools/">Nsetools</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/posts/presto/">
    <span class="title">« Prev</span>
    <br>
    <span>Presto - A distributed SQL Engine for variety of data stores</span>
  </a>
  <a class="next" href="http://localhost:1313/posts/restapiversioning/">
    <span class="title">Next »</span>
    <br>
    <span>Learnings from Jeff Richter&#39;s Designing and Versioning HTTP REST APIs Video Course</span>
  </a>
</nav>


<ul class="share-buttons">
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share ELT approach for Data Pipelines on x"
            href="https://x.com/intent/tweet/?text=ELT%20approach%20for%20Data%20Pipelines&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2felt%2f&amp;hashtags=DBT%2cELT%2cETL%2cPython%2cGo%2cPostgreSQL%2cCSV%2cdatapipeline%2cnsetools">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M512 62.554 L 512 449.446 C 512 483.97 483.97 512 449.446 512 L 62.554 512 C 28.03 512 0 483.97 0 449.446 L 0 62.554 C 0 28.03 28.029 0 62.554 0 L 449.446 0 C 483.971 0 512 28.03 512 62.554 Z M 269.951 190.75 L 182.567 75.216 L 56 75.216 L 207.216 272.95 L 63.9 436.783 L 125.266 436.783 L 235.9 310.383 L 332.567 436.783 L 456 436.783 L 298.367 228.367 L 432.367 75.216 L 371.033 75.216 Z M 127.633 110 L 164.101 110 L 383.481 400.065 L 349.5 400.065 Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share ELT approach for Data Pipelines on linkedin"
            href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2felt%2f&amp;title=ELT%20approach%20for%20Data%20Pipelines&amp;summary=ELT%20approach%20for%20Data%20Pipelines&amp;source=http%3a%2f%2flocalhost%3a1313%2fposts%2felt%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share ELT approach for Data Pipelines on reddit"
            href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a1313%2fposts%2felt%2f&title=ELT%20approach%20for%20Data%20Pipelines">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share ELT approach for Data Pipelines on facebook"
            href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a1313%2fposts%2felt%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share ELT approach for Data Pipelines on whatsapp"
            href="https://api.whatsapp.com/send?text=ELT%20approach%20for%20Data%20Pipelines%20-%20http%3a%2f%2flocalhost%3a1313%2fposts%2felt%2f">
            <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve" height="30px" width="30px" fill="currentColor">
                <path
                    d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share ELT approach for Data Pipelines on telegram"
            href="https://telegram.me/share/url?text=ELT%20approach%20for%20Data%20Pipelines&amp;url=http%3a%2f%2flocalhost%3a1313%2fposts%2felt%2f">
            <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28" height="30px" width="30px" fill="currentColor">
                <path
                    d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
            </svg>
        </a>
    </li>
    <li>
        <a target="_blank" rel="noopener noreferrer" aria-label="share ELT approach for Data Pipelines on ycombinator"
            href="https://news.ycombinator.com/submitlink?t=ELT%20approach%20for%20Data%20Pipelines&u=http%3a%2f%2flocalhost%3a1313%2fposts%2felt%2f">
            <svg version="1.1" xml:space="preserve" width="30px" height="30px" viewBox="0 0 512 512" fill="currentColor"
                xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape">
                <path
                    d="M449.446 0C483.971 0 512 28.03 512 62.554L512 449.446C512 483.97 483.97 512 449.446 512L62.554 512C28.03 512 0 483.97 0 449.446L0 62.554C0 28.03 28.029 0 62.554 0L449.446 0ZM183.8767 87.9921H121.8427L230.6673 292.4508V424.0079H281.3328V292.4508L390.1575 87.9921H328.1233L256 238.2489z" />
            </svg>
        </a>
    </li>
</ul>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>©</span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
