<!DOCTYPE html>
<html class="no-js" lang="en">
<head>
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<title>Using local LLM with Ollama and Semantic Kernel - Learnings in IT</title>
	<script>(function(d,e){d[e]=d[e].replace("no-js","js");})(document.documentElement,"className");</script>
	<meta name="description" content="">
		<meta property="og:title" content="Using local LLM with Ollama and Semantic Kernel" />
<meta property="og:description" content="Introduction Artificial Intelligence, especially Large language models (LLMs) are all in high demand. Since OpenAI released ChatGPT, interest has gone up multi-fold. Since 2023, Powerful LLMs can be run on local machines. Local Large Language Models offer advantages in terms of data privacy and security and can be enriched using enterprise-specific data using Retrieval augmentation generation (RAG).Several tools exist that make it relatively easy to obtain, run and manage such models locally on our machines." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://sachinsu.github.io/posts/ollamasemantickernel/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2024-05-11T10:25:04+05:30" />
<meta property="article:modified_time" content="2024-05-11T10:25:04+05:30" />


		<meta itemprop="name" content="Using local LLM with Ollama and Semantic Kernel">
<meta itemprop="description" content="Introduction Artificial Intelligence, especially Large language models (LLMs) are all in high demand. Since OpenAI released ChatGPT, interest has gone up multi-fold. Since 2023, Powerful LLMs can be run on local machines. Local Large Language Models offer advantages in terms of data privacy and security and can be enriched using enterprise-specific data using Retrieval augmentation generation (RAG).Several tools exist that make it relatively easy to obtain, run and manage such models locally on our machines."><meta itemprop="datePublished" content="2024-05-11T10:25:04+05:30" />
<meta itemprop="dateModified" content="2024-05-11T10:25:04+05:30" />
<meta itemprop="wordCount" content="1411">
<meta itemprop="keywords" content="localai,golang,.net core,llm,ollama,phi-3,semantickernel,gpt,gemini," />
		<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Using local LLM with Ollama and Semantic Kernel"/>
<meta name="twitter:description" content="Introduction Artificial Intelligence, especially Large language models (LLMs) are all in high demand. Since OpenAI released ChatGPT, interest has gone up multi-fold. Since 2023, Powerful LLMs can be run on local machines. Local Large Language Models offer advantages in terms of data privacy and security and can be enriched using enterprise-specific data using Retrieval augmentation generation (RAG).Several tools exist that make it relatively easy to obtain, run and manage such models locally on our machines."/>

	<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
	<link rel="dns-prefetch" href="//fonts.googleapis.com">
	<link rel="dns-prefetch" href="//fonts.gstatic.com">
	<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700">

	<link rel="stylesheet" href="/css/style.css">
	

	<link rel="shortcut icon" href="/favicon.ico">
		
</head>
<body class="body">
	<div class="container container--outer">
		<header class="header">
	<div class="container header__container">
		
	<div class="logo">
		<a class="logo__link" href="/" title="Learnings in IT" rel="home">
			<div class="logo__item logo__text">
					<div class="logo__title">Learnings in IT</div>
					<div class="logo__tagline">A Simple Technical Blog</div>
				</div>
		</a>
	</div>
		
<nav class="menu">
	<button class="menu__btn" aria-haspopup="true" aria-expanded="false" tabindex="0">
		<span class="menu__btn-title" tabindex="-1">Menu</span>
	</button>
	<ul class="menu__list">
		<li class="menu__item">
			<a class="menu__link" href="/about/">
				
				<span class="menu__text">About</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/posts/">
				
				<span class="menu__text">Blog</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/projects/">
				
				<span class="menu__text">Projects</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="https://gist.github.com/sachinsu">
				
				<span class="menu__text">Gists</span>
				
			</a>
		</li>
		<li class="menu__item">
			<a class="menu__link" href="/links/home">
				
				<span class="menu__text">Useful Links</span>
				
			</a>
		</li>
	</ul>
</nav>

	</div>
</header>
		<div class="wrapper flex">
			<div class="primary">
			
<main class="main" role="main">
	<article class="post">
		<header class="post__header">
			<h1 class="post__title">Using local LLM with Ollama and Semantic Kernel</h1>
			<div class="post__meta meta"><div class="meta__item-author meta__item">
	<svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 16 16"><path d="M8 1c2 0 3.5 2 3.5 4.5S10 9 10 9c3 1 4 2 4 6H2c0-4 1-5 4-6 0 0-1.5-1-1.5-3.5S6 1 8 1"/></svg><span class="meta__text">Sachin Sunkle</span>
</div>
<div class="meta__item-datetime meta__item">
	<svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0a14 14 0 1 1 0 28 1 1 0 0 1 0-28m0 3a3 3 0 1 0 0 22 3 3 0 0 0 0-22m1 4h-2v8.4l6.8 4.4L22 18l-6-3.8z"/></svg><time class="meta__text" datetime="2024-05-11T10:25:04&#43;05:30">May 11 2024</time></div></div>
		</header>
		
<div class="post__toc toc">
	<div class="toc__title">Page content</div>
	<div class="toc__menu">
		<nav id="TableOfContents">
  <ul>
    <li><a href="#introduction">Introduction</a></li>
    <li><a href="#ollama">Ollama</a></li>
    <li><a href="#why-semantickernel-">Why SemanticKernel ?</a></li>
    <li><a href="#using-ollama">Using Ollama</a></li>
    <li><a href="#integrating-with-semantickernel">Integrating with SemanticKernel</a></li>
    <li><a href="#summary">Summary</a></li>
    <li><a href="#helpful-links">Helpful Links</a></li>
  </ul>
</nav>
	</div>
</div><div class="content post__content clearfix">
			<h2 id="introduction">Introduction</h2>
<p>Artificial Intelligence, especially Large language models (LLMs) are all in high demand. Since OpenAI released ChatGPT, interest has gone up multi-fold. Since 2023, Powerful LLMs can be run on local machines. Local Large Language Models  offer advantages in terms of data privacy and security and can be enriched using enterprise-specific data using Retrieval augmentation generation (RAG).Several tools exist that make it relatively easy to obtain, run and manage such models locally on our machines. Few examples are <a href=https://ollama.com/
    
    target=_blank rel="noopener noreferrer"
>Ollama</a>, <a href=https://github.com/hwchase17/langchain
    
    target=_blank rel="noopener noreferrer"
>Langchain</a>,  <a href=localai.io
    
    
>LocalAI</a>.</p>
<p><a href=https://github.com/microsoft/semantic-kernel
    
    target=_blank rel="noopener noreferrer"
>Semantic Kernel</a> is an SDK from Microsoft that integrates Large Language Models (LLMs) like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C#, Python, and Java. Semantic Kernel also has plugins that can be chained together to integrate with other tools like Ollama.</p>
<p>This post describes usage of Ollama to run  model locally, communicate with it using REST API from Semantic kernel SDK.</p>
<h2 id="ollama">Ollama</h2>
<p>To setup Ollama follow the installation and setup instructions from the Ollama <a href=https://ollama.ai
    
    target=_blank rel="noopener noreferrer"
>website</a>. Ollama runs as a service, exposing a REST API on a localhost port.Once installed, you can invoke ollama run <!-- raw HTML omitted --> to talk to this model; the model is downloaded, if not already and cached the first time it&rsquo;s requested.</p>
<p>For the sake of this post, we can use Phi3 model, so run <code>ollama run phi3</code>. This will download phi3 model, if not already, and once done, it will present a prompt. Using this prompt, one can start chatting with the model.</p>
<h2 id="why-semantickernel-">Why SemanticKernel ?</h2>
<p>As such , Ollama can be integrated with from any application via REST API. Then why go for SemanticKernel SDK?  It provides a simplified integration of AI capabilities into existing applications, lowering the barrier of entry for new developers and supporting the ability to fine-tune models. It supports multiple languages like C#, Python and Java.</p>
<h2 id="using-ollama">Using Ollama</h2>
<p>Install Ollama by following instructions <a href=https://github.com/ollama/ollama/blob/main/README.md#quickstart
    
    target=_blank rel="noopener noreferrer"
>here</a>.Ollama exposes set of REST APIs, check Documentation <a href=https://github.com/ollama/ollama/blob/main/docs/api.md
    
    target=_blank rel="noopener noreferrer"
>here</a>. It provides range of functions like get response for Prompt, get Chat response. for Specific operations, it supports streaming and non-streaming response. First step is to download/pull  using <code>ollama run phi3</code>. This will pull, if required, the model and set it up locally. In the end, it will show prompt where user can interact with model.</p>
<p>Now Ollama API can be easily accessed. Below is the gateway class.</p>
<pre tabindex="0"><code>public class OllamaApiClient 
{

    private HttpClient _client = new();

	public Configuration Config { get; }

	public interface IResponseStreamer&lt;T&gt;
	{
		void Stream(T stream);
	}
	public class ChatMessage { 

			[JsonPropertyName(&quot;role&quot;)]
			public string Role { get; set;}
			
			[JsonPropertyName(&quot;content&quot;)]
			public string Content {get;set;}

	}

	public class ChatResponse
	{
		[JsonPropertyName(&quot;model&quot;)]
		public string Model { get; set; }

		[JsonPropertyName(&quot;created_at&quot;)]
		public string CreatedAt { get; set; }

		[JsonPropertyName(&quot;response&quot;)]
		public string Response { get; set; }


		[JsonPropertyName(&quot;message&quot;)]
		public ChatMessage? Message { get; set; }


		[JsonPropertyName(&quot;messages&quot;)]
		public List&lt;ChatMessage&gt; Messages { get; set; }


		[JsonPropertyName(&quot;embedding&quot;)]
		public List&lt;Double&gt; Embeddings { get; set; }


		[JsonPropertyName(&quot;done&quot;)]
		public bool Done { get; set; }
	}

	public class ChatRequest { 
		[JsonPropertyName(&quot;model&quot;)]
		public string Model { get;set;}

		[JsonPropertyName(&quot;prompt&quot;)]
		[JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
		public string Prompt {get; set;}


		[JsonPropertyName(&quot;format&quot;)]
		[JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
		public string Format {get; set;}


		[JsonPropertyName(&quot;messages&quot;)]
		[JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)]
		public IList&lt;ChatMessage&gt; Messages {get; set;}

		[JsonPropertyName(&quot;stream&quot;)]
		public bool Stream {get; set;} = false;
	}


    public class Configuration
		{
			public Uri Uri { get; set; }

			public string Model { get; set; }
		}


    public OllamaApiClient(string uriString, string defaultModel = &quot;&quot;)
        : this(new Uri(uriString), defaultModel)
		{
		}

    public OllamaApiClient(Uri uri, string defaultModel = &quot;&quot;)
			: this(new Configuration { Uri = uri, Model = defaultModel })
		{
		}

    public OllamaApiClient(Configuration config)
			: this(new HttpClient() { BaseAddress = config.Uri }, config.Model)
		{
    		Config = config;

			}

    public OllamaApiClient(HttpClient client, string defaultModel = &quot;&quot;)
		{
			_client = client ?? throw new ArgumentNullException(nameof(client));
			_client.Timeout = TimeSpan.FromMinutes(10);

			(Config ??=  new Configuration()).Model = defaultModel;
			
		}

	public async Task&lt;ChatResponse&gt; GetEmbeddingsAsync(ChatRequest message, CancellationToken token) {
		message.Model = this.Config.Model;
		return await PostAsync&lt;ChatRequest,ChatResponse&gt;(&quot;/api/embeddings&quot;,message,token);
	}


	public async Task&lt;ChatResponse&gt; GetResponseForChatAsync(ChatRequest message, CancellationToken token) {
		message.Model = this.Config.Model;
		return await PostAsync&lt;ChatRequest,ChatResponse&gt;(&quot;/api/chat&quot;,message,token);
	}


	public async Task&lt;ChatResponse&gt; GetResponseForPromptAsync(ChatRequest message, CancellationToken token) {
		message.Model = this.Config.Model;
		return await PostAsync&lt;ChatRequest,ChatResponse&gt;(&quot;/api/generate&quot;,message,token);
	}

	public async IAsyncEnumerable&lt;ChatResponse&gt; GetStreamForPromptAsync(ChatRequest message, CancellationToken token) {
		message.Model = this.Config.Model;
		message.Stream = true;
		await foreach(ChatResponse resp in  StreamPostAsync&lt;ChatRequest,ChatResponse&gt;(&quot;/api/generate&quot;,message,token)) {
			yield return resp;
		}
	}

	public async IAsyncEnumerable&lt;ChatResponse&gt; GetStreamForChatAsync(ChatRequest message, CancellationToken token) {
		message.Model = this.Config.Model;
		message.Stream = true;
		await foreach(ChatResponse resp in  StreamPostAsync&lt;ChatRequest,ChatResponse&gt;(&quot;/api/chat&quot;,message,token)) {
			yield return resp;
		}
	}

    private async Task&lt;TResponse&gt; GetAsync&lt;TResponse&gt;(string endpoint, CancellationToken cancellationToken)
		{
			var response = await _client.GetAsync(endpoint, cancellationToken);
			response.EnsureSuccessStatusCode();

			var responseBody = await response.Content.ReadAsStringAsync(cancellationToken);
			return JsonSerializer.Deserialize&lt;TResponse&gt;(responseBody);
		}

    private async Task PostAsync&lt;TRequest&gt;(string endpoint, TRequest request, CancellationToken cancellationToken)
		{
			var content = new StringContent(JsonSerializer.Serialize(request), Encoding.UTF8, &quot;application/json&quot;);
			var response = await _client.PostAsync(endpoint, content, cancellationToken);
			response.EnsureSuccessStatusCode();
		}



    private async IAsyncEnumerable&lt;TResponse&gt; StreamPostAsync&lt;TRequest,TResponse&gt;(string endpoint, TRequest request, CancellationToken cancellationToken)
		{
			var content = new StringContent(JsonSerializer.Serialize(request), Encoding.UTF8, &quot;application/json&quot;);
			var response = await _client.PostAsync(endpoint, content, cancellationToken);

 			using Stream stream = await response.Content.ReadAsStreamAsync();

			using StreamReader reader = new StreamReader(stream);

			while (!reader.EndOfStream) {
				var jsonString = await reader.ReadLineAsync(cancellationToken);
				TResponse  result =  JsonSerializer.Deserialize&lt;TResponse&gt;(jsonString);
				yield return result;
			}

			yield break;
	}


    private async Task&lt;TResponse&gt; PostAsync&lt;TRequest, TResponse&gt;(string endpoint, TRequest request, CancellationToken cancellationToken)
		{
			var content = new StringContent(JsonSerializer.Serialize(request), Encoding.UTF8, &quot;application/json&quot;);
			var response = await _client.PostAsync(endpoint, content, cancellationToken);
			response.EnsureSuccessStatusCode();

			var responseBody = await response.Content.ReadAsStringAsync(cancellationToken);

			return JsonSerializer.Deserialize&lt;TResponse&gt;(responseBody);
		}
}

</code></pre><p>With this class in place, now it can be integrated with SemanticKernel.</p>
<h2 id="integrating-with-semantickernel">Integrating with SemanticKernel</h2>
<p>Semantickernel SDK operates on a plug-in system, where developers can use pre-built plugins or create their own. These plugins consist of prompts that the AI model should respond to, as well as functions that can complete specialized tasks. Accordingly, it provides interfaces for (Chat completion)[https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel.chatcompletion.ichatcompletionservice?view=semantic-kernel-dotnet] and <a href=https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel.textgeneration.itextgenerationservice?view&#61;semantic-kernel-dotnet
    
    target=_blank rel="noopener noreferrer"
>Text Generation</a> tasks which can be use
d to integrate with external implementation like Ollama.</p>
<p>Below are implementations of these interfaces that use Ollama API,</p>
<ul>
<li>Text Generation</li>
</ul>
<pre tabindex="0"><code>public class TextGenerationService : ITextGenerationService
{
    
    public string ModelApiEndPoint { get; set; }
    public string ModelName { get; set; }

    public IReadOnlyDictionary&lt;string, object?&gt; Attributes =&gt; throw new NotImplementedException();

    public async Task&lt;IReadOnlyList&lt;TextContent&gt;&gt; GetTextContentsAsync(string prompt, PromptExecutionSettings? executionSettings = null, Kernel? kernel = null, CancellationToken cancellationToken = default)
    {
          
        var client = new OllamaApiClient(ModelApiEndPoint, ModelName);

        OllamaApiClient.ChatRequest req = new OllamaApiClient.ChatRequest() {
                Model=ModelName,
                Prompt=prompt,
        };

        OllamaApiClient.ChatResponse resp = await client.GetResponseForPromptAsync(req
            , cancellationToken);

        return new List&lt;TextContent&gt;() { new TextContent(resp.Response) };
    }

    public async IAsyncEnumerable&lt;StreamingTextContent&gt; GetStreamingTextContentsAsync(string prompt, PromptExecutionSettings? executionSettings = null, Kernel? kernel = null, CancellationToken cancellationToken = default)
    {
            var ollama = new OllamaApiClient(ModelApiEndPoint, ModelName);

            OllamaApiClient.ChatRequest req = new OllamaApiClient.ChatRequest() {
                    Prompt=prompt,
                    Stream=true
            };

            await foreach( OllamaApiClient.ChatResponse resp in ollama.GetStreamForPromptAsync(req, cancellationToken)) {
                    yield return new StreamingTextContent( text:  resp.Response) ;
            } 

    }
}

</code></pre><ul>
<li>Chat Completion</li>
</ul>
<pre tabindex="0"><code>
public class OllamaChatCompletionService : IChatCompletionService
{
    public string ModelApiEndPoint { get; set; }
    public string ModelName { get; set; }

    public IReadOnlyDictionary&lt;string, object?&gt; Attributes =&gt; throw new NotImplementedException();
    
    public async Task&lt;IReadOnlyList&lt;ChatMessageContent&gt;&gt; GetChatMessageContentsAsync(ChatHistory chatHistory, PromptExecutionSettings? executionSettings = null, Kernel? kernel = null, CancellationToken cancellationToken = default)
   {


        var client = new OllamaApiClient(ModelApiEndPoint, ModelName);

        OllamaApiClient.ChatRequest req = new OllamaApiClient.ChatRequest() {
                Model=ModelName
        };

        req.Messages = new List&lt;OllamaApiClient.ChatMessage&gt;();

        // iterate though chatHistory Messages
        foreach (var history in chatHistory)
        {
            req.Messages.Add(new OllamaApiClient.ChatMessage{
                Role=history.Role.ToString(),
                Content=history.Content
            });
        }

        OllamaApiClient.ChatResponse resp = await client.GetResponseForChatAsync(req
            , cancellationToken);

        List&lt;ChatMessageContent&gt; content = new();
        content.Add( new(role:resp.Message.Role.Equals(&quot;system&quot;,StringComparison.InvariantCultureIgnoreCase)?AuthorRole.System:AuthorRole.User,content:resp.Message.Content));

        return content;
    }

    public async IAsyncEnumerable&lt;StreamingChatMessageContent&gt; GetStreamingChatMessageContentsAsync(ChatHistory chatHistory, PromptExecutionSettings? executionSettings = null, Kernel? kernel = null, CancellationToken cancellationToken = default)
    {

        var client = new OllamaApiClient(ModelApiEndPoint, ModelName);

        OllamaApiClient.ChatRequest req = new OllamaApiClient.ChatRequest() {
                Model=ModelName
        };

        req.Messages = new List&lt;OllamaApiClient.ChatMessage&gt;();

        // iterate though chatHistory Messages
        foreach (var history in chatHistory)
        {
            req.Messages.Add(new OllamaApiClient.ChatMessage{
                Role=history.Role.ToString(),
                Content=history.Content
            });
        }

        CancellationTokenSource source = new CancellationTokenSource();
        CancellationToken token = source.Token;

        await foreach (OllamaApiClient.ChatResponse resp in  client.GetStreamForChatAsync(req,token)) { 
            yield return new(role:resp.Message.Role.Equals(&quot;system&quot;,StringComparison.InvariantCultureIgnoreCase)?AuthorRole.System:AuthorRole.User,
            content:resp.Message.Content ?? string.Empty); 
        }

     }
}


</code></pre><p>Above implementation is for demonstration purposes only. I am sure further optimization is certainly possible.</p>
<p>After this, it is time to use it as client of SemanticKernel SDK. Below is the test case for chat completion service,</p>
<pre tabindex="0"><code>
    [Fact]
    public async void TestChatGenerationviaSK() 
    {
        var ollamachat = ServiceProvider.GetChatCompletionService();


        // semantic kernel builder
        var builder = Kernel.CreateBuilder();
        builder.Services.AddKeyedSingleton&lt;IChatCompletionService&gt;(&quot;ollamaChat&quot;, ollamachat);
        // builder.Services.AddKeyedSingleton&lt;ITextGenerationService&gt;(&quot;ollamaText&quot;, ollamaText);
        var kernel = builder.Build();


        // chat generation
        var chatGen = kernel.GetRequiredService&lt;IChatCompletionService&gt;();
        ChatHistory chat = new(&quot;You are an AI assistant that helps people find information.&quot;);
        chat.AddUserMessage(&quot;What is Sixth Sense?&quot;);
        var answer = await chatGen.GetChatMessageContentAsync(chat);
        Assert.NotNull(answer);
        Assert.NotEmpty(answer.Content!);
        System.Diagnostics.Debug.WriteLine(answer.Content!);


    }


</code></pre><p>Full Source code of this post is available <a href=https://github.com/sachinsu/ollamaskernel
    
    target=_blank rel="noopener noreferrer"
>here</a>.</p>
<h2 id="summary">Summary</h2>
<p>Local AI combined with Retrieval Augmented Generation is powerful combination that any one get started with without need for subscriptions while conserving data privacy. Next step in this is to Use RAG for augmenting the results using enterprise/private data.</p>
<p>Happy Coding !!</p>
<h2 id="helpful-links">Helpful Links</h2>
<ul>
<li><a href=https://devblogs.microsoft.com/dotnet/demystifying-retrieval-augmented-generation-with-dotnet/
    
    target=_blank rel="noopener noreferrer"
>Demystifying Retrieval Augmented Generation with .NET</a></li>
<li><a href=https://eli.thegreenplace.net/2024/gemma-ollama-and-langchaingo/
    
    target=_blank rel="noopener noreferrer"
>Gemma, ollama and Langchaingo</a></li>
</ul>

		</div>
		<footer class="post__footer">
			
<div class="post__tags tags clearfix">
	<svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M4 0h8s2 0 4 2l15 15s2 2 0 4L21 31s-2 2-4 0L2 16s-2-2-2-4V3s0-3 4-3m3 10a3 3 0 0 0 0-6 3 3 0 0 0 0 6"/></svg>
	<ul class="tags__list">
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/localai/" rel="tag">localai</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/golang/" rel="tag">golang</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/.net-core/" rel="tag">.net core</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/llm/" rel="tag">llm</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/ollama/" rel="tag">ollama</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/phi-3/" rel="tag">phi-3</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/semantickernel/" rel="tag">semantickernel</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/gpt/" rel="tag">gpt</a>
		</li>
		<li class="tags__item">
			<a class="tags__link btn" href="/tags/gemini/" rel="tag">gemini</a>
		</li>
	</ul>
</div>
		</footer>
	</article>
</main>

<div class="authorbox clearfix">
	<div class="authorbox__header">
		<span class="authorbox__name">About Sachin Sunkle</span>
	</div>
	<div class="authorbox__description">
		A Coder in IT
	</div>
</div>

<nav class="pager flex">
	<div class="pager__item pager__item--prev">
		<a class="pager__link" href="/posts/tlshandsharefailure/" rel="prev">
			<span class="pager__subtitle">Â«&thinsp;Previous</span>
			<p class="pager__title">Troubleshooting TLS handshake issue</p>
		</a>
	</div>
</nav>


			</div>
			
		</div>
		<footer class="footer">
	<div class="container footer__container flex">
		
		<div class="footer__copyright">
			&copy; 2024 Sachin Sunkle.
			<span class="footer__copyright-credits">Generated with <a href="https://gohugo.io/" rel="nofollow noopener" target="_blank">Hugo</a> and <a href="https://github.com/Vimux/Mainroad/" rel="nofollow noopener" target="_blank">Mainroad</a> theme.</span>
		</div>
	</div>
</footer>
	</div>
<script async defer src="/js/menu.js"></script>
</body>
</html>