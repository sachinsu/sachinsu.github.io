[{"id":0,"href":"/posts/parallelprocessing/","title":"Using Asynchronous programming to manage parallel processing ","section":"Posts","content":" Background # There was a requirement to perform series of tasks, involving generation of output files, such that the required throughput is achieved. These tasks involve database read operation, external API invocation and file i/o. Generally, benchmarking showed that executing them in sequential way was not helpful. What if asynchronous programming be used to perform this task.\nSo Lets Start.\nApproach # Lets assume that this typical use case requires,\nfetching data from database for the purpose of merging placeholders in a Template and perform mail merge\nGenerate PDF file from mail-merged output of last step (say HTML to PDF)\nsend notification to users via third party API.\nThe requirement is to perform these steps in such a way that 50 or more notifications (with file) are sent per minute.\nFor the purpose of simplicity, lets assume that,\nDatabase read operation and HTML generation basis template, takes upto 2 seconds per iteration We will use Puppeteer Sharp library for PDF Generation External API Integration takes up to 2 seconds per call Since current approach of sequential execution is not helpful, lets try below (both the methods process 5 requests[i.e. generate 5 pdf files] per iteration),\nUsing Task asynchronous programming model - This uses Task library to start tasks in parallel and subsequently process them as each completes. Using Task Async. Library Using Dataflow - Task Parallel Library - This uses Dataflow Library to orchestrate each step in the process and use parallelism for performance. Using DataFlow Library Below is the Report from Benchmarkdotnet for both the approaches.\nBenchmark Results As one can see, using above techniques, It is straightforward to write asynchronous code that performs parallel execution and achieves better performance compared to sequential alternate approach.\nReferences:\nBenchmarkDotNet Introduction to Benchmarking C# Code with Benchmark .NET Happy Coding !!\n"},{"id":1,"href":"/now/","title":"What am i doing *NOW*","section":"","content":" Location # Mumbai, Maharashtra, India\nProfessional Title # Senior Technical Architect at Worldline India\nWhat do you do ? # I write code. In my current role, I am responsible for Architecture for all the systems in Payments space covering Acquiring and Issuance of Cards, UPI (QR based payment system in India). I am involved in creating and maintaining the Architecture of various systems, integration between them, primarily focussing on non-functional (ilities) aspects.\nWhy ? # I like building things and understanding patterns.\nWhat should we read? # Poor Charlies Almanack by Charlie Munger\nURLS: # Linkedin\nMy Blog\nI appreciate any ideas/suggestions you have on how I can improve this site.\n"},{"id":2,"href":"/posts/ollamasemantickernel/","title":"Using local LLM with Ollama and Semantic Kernel","section":"Posts","content":" Introduction # Artificial Intelligence, especially Large language models (LLMs) are all in high demand. Since OpenAI released ChatGPT, interest has gone up multi-fold. Since 2023, Powerful LLMs can be run on local machines. Local Large Language Models offer advantages in terms of data privacy and security and can be enriched using enterprise-specific data using Retrieval augmentation generation (RAG).Several tools exist that make it relatively easy to obtain, run and manage such models locally on our machines. Few examples are Ollama, Langchain, LocalAI.\nSemantic Kernel is an SDK from Microsoft that integrates Large Language Models (LLMs) like OpenAI, Azure OpenAI, and Hugging Face with conventional programming languages like C#, Python, and Java. Semantic Kernel also has plugins that can be chained together to integrate with other tools like Ollama.\nThis post describes usage of Ollama to run model locally, communicate with it using REST API from Semantic kernel SDK.\nOllama # To setup Ollama follow the installation and setup instructions from the Ollama website. Ollama runs as a service, exposing a REST API on a localhost port.Once installed, you can invoke ollama run to talk to this model; the model is downloaded, if not already and cached the first time it\u0026rsquo;s requested.\nFor the sake of this post, we can use Phi3 model, so run ollama run phi3. This will download phi3 model, if not already, and once done, it will present a prompt. Using this prompt, one can start chatting with the model.\nWhy SemanticKernel ? # As such , Ollama can be integrated with from any application via REST API. Then why go for SemanticKernel SDK? It provides a simplified integration of AI capabilities into existing applications, lowering the barrier of entry for new developers and supporting the ability to fine-tune models. It supports multiple languages like C#, Python and Java.\nUsing Ollama # Install Ollama by following instructions here.Ollama exposes set of REST APIs, check Documentation here. It provides range of functions like get response for Prompt, get Chat response. for Specific operations, it supports streaming and non-streaming response. First step is to download/pull using ollama run phi3. This will pull, if required, the model and set it up locally. In the end, it will show prompt where user can interact with model.\nNow Ollama API can be easily accessed. Below is the gateway class.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 ``` public class OllamaApiClient { private HttpClient _client = new(); public Configuration Config { get; } public interface IResponseStreamer\u0026lt;T\u0026gt; { void Stream(T stream); } public class ChatMessage { [JsonPropertyName(\u0026#34;role\u0026#34;)] public string Role { get; set;} [JsonPropertyName(\u0026#34;content\u0026#34;)] public string Content {get;set;} } public class ChatResponse { [JsonPropertyName(\u0026#34;model\u0026#34;)] public string Model { get; set; } [JsonPropertyName(\u0026#34;created_at\u0026#34;)] public string CreatedAt { get; set; } [JsonPropertyName(\u0026#34;response\u0026#34;)] public string Response { get; set; } [JsonPropertyName(\u0026#34;message\u0026#34;)] public ChatMessage? Message { get; set; } [JsonPropertyName(\u0026#34;messages\u0026#34;)] public List\u0026lt;ChatMessage\u0026gt; Messages { get; set; } [JsonPropertyName(\u0026#34;embedding\u0026#34;)] public List\u0026lt;Double\u0026gt; Embeddings { get; set; } [JsonPropertyName(\u0026#34;done\u0026#34;)] public bool Done { get; set; } } public class ChatRequest { [JsonPropertyName(\u0026#34;model\u0026#34;)] public string Model { get;set;} [JsonPropertyName(\u0026#34;prompt\u0026#34;)] [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)] public string Prompt {get; set;} [JsonPropertyName(\u0026#34;format\u0026#34;)] [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)] public string Format {get; set;} [JsonPropertyName(\u0026#34;messages\u0026#34;)] [JsonIgnore(Condition = JsonIgnoreCondition.WhenWritingNull)] public IList\u0026lt;ChatMessage\u0026gt; Messages {get; set;} [JsonPropertyName(\u0026#34;stream\u0026#34;)] public bool Stream {get; set;} = false; } public class Configuration { public Uri Uri { get; set; } public string Model { get; set; } } public OllamaApiClient(string uriString, string defaultModel = \u0026#34;\u0026#34;) : this(new Uri(uriString), defaultModel) { } public OllamaApiClient(Uri uri, string defaultModel = \u0026#34;\u0026#34;) : this(new Configuration { Uri = uri, Model = defaultModel }) { } public OllamaApiClient(Configuration config) : this(new HttpClient() { BaseAddress = config.Uri }, config.Model) { Config = config; } public OllamaApiClient(HttpClient client, string defaultModel = \u0026#34;\u0026#34;) { _client = client ?? throw new ArgumentNullException(nameof(client)); _client.Timeout = TimeSpan.FromMinutes(10); (Config ??= new Configuration()).Model = defaultModel; } public async Task\u0026lt;ChatResponse\u0026gt; GetEmbeddingsAsync(ChatRequest message, CancellationToken token) { message.Model = this.Config.Model; return await PostAsync\u0026lt;ChatRequest,ChatResponse\u0026gt;(\u0026#34;/api/embeddings\u0026#34;,message,token); } public async Task\u0026lt;ChatResponse\u0026gt; GetResponseForChatAsync(ChatRequest message, CancellationToken token) { message.Model = this.Config.Model; return await PostAsync\u0026lt;ChatRequest,ChatResponse\u0026gt;(\u0026#34;/api/chat\u0026#34;,message,token); } public async Task\u0026lt;ChatResponse\u0026gt; GetResponseForPromptAsync(ChatRequest message, CancellationToken token) { message.Model = this.Config.Model; return await PostAsync\u0026lt;ChatRequest,ChatResponse\u0026gt;(\u0026#34;/api/generate\u0026#34;,message,token); } public async IAsyncEnumerable\u0026lt;ChatResponse\u0026gt; GetStreamForPromptAsync(ChatRequest message, CancellationToken token) { message.Model = this.Config.Model; message.Stream = true; await foreach(ChatResponse resp in StreamPostAsync\u0026lt;ChatRequest,ChatResponse\u0026gt;(\u0026#34;/api/generate\u0026#34;,message,token)) { yield return resp; } } public async IAsyncEnumerable\u0026lt;ChatResponse\u0026gt; GetStreamForChatAsync(ChatRequest message, CancellationToken token) { message.Model = this.Config.Model; message.Stream = true; await foreach(ChatResponse resp in StreamPostAsync\u0026lt;ChatRequest,ChatResponse\u0026gt;(\u0026#34;/api/chat\u0026#34;,message,token)) { yield return resp; } } private async Task\u0026lt;TResponse\u0026gt; GetAsync\u0026lt;TResponse\u0026gt;(string endpoint, CancellationToken cancellationToken) { var response = await _client.GetAsync(endpoint, cancellationToken); response.EnsureSuccessStatusCode(); var responseBody = await response.Content.ReadAsStringAsync(cancellationToken); return JsonSerializer.Deserialize\u0026lt;TResponse\u0026gt;(responseBody); } private async Task PostAsync\u0026lt;TRequest\u0026gt;(string endpoint, TRequest request, CancellationToken cancellationToken) { var content = new StringContent(JsonSerializer.Serialize(request), Encoding.UTF8, \u0026#34;application/json\u0026#34;); var response = await _client.PostAsync(endpoint, content, cancellationToken); response.EnsureSuccessStatusCode(); } private async IAsyncEnumerable\u0026lt;TResponse\u0026gt; StreamPostAsync\u0026lt;TRequest,TResponse\u0026gt;(string endpoint, TRequest request, CancellationToken cancellationToken) { var content = new StringContent(JsonSerializer.Serialize(request), Encoding.UTF8, \u0026#34;application/json\u0026#34;); var response = await _client.PostAsync(endpoint, content, cancellationToken); using Stream stream = await response.Content.ReadAsStreamAsync(); using StreamReader reader = new StreamReader(stream); while (!reader.EndOfStream) { var jsonString = await reader.ReadLineAsync(cancellationToken); TResponse result = JsonSerializer.Deserialize\u0026lt;TResponse\u0026gt;(jsonString); yield return result; } yield break; } private async Task\u0026lt;TResponse\u0026gt; PostAsync\u0026lt;TRequest, TResponse\u0026gt;(string endpoint, TRequest request, CancellationToken cancellationToken) { var content = new StringContent(JsonSerializer.Serialize(request), Encoding.UTF8, \u0026#34;application/json\u0026#34;); var response = await _client.PostAsync(endpoint, content, cancellationToken); response.EnsureSuccessStatusCode(); var responseBody = await response.Content.ReadAsStringAsync(cancellationToken); return JsonSerializer.Deserialize\u0026lt;TResponse\u0026gt;(responseBody); } } With this class in place, now it can be integrated with SemanticKernel.\nIntegrating with SemanticKernel # Semantickernel SDK operates on a plug-in system, where developers can use pre-built plugins or create their own. These plugins consist of prompts that the AI model should respond to, as well as functions that can complete specialized tasks. Accordingly, it provides interfaces for (Chat completion)[https://learn.microsoft.com/en-us/dotnet/api/microsoft.semantickernel.chatcompletion.ichatcompletionservice?view=semantic-kernel-dotnet] and Text Generation tasks which can be use d to integrate with external implementation like Ollama.\nBelow are implementations of these interfaces that use Ollama API,\nText Generation 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 public class TextGenerationService : ITextGenerationService { public string ModelApiEndPoint { get; set; } public string ModelName { get; set; } public IReadOnlyDictionary\u0026lt;string, object?\u0026gt; Attributes =\u0026gt; throw new NotImplementedException(); public async Task\u0026lt;IReadOnlyList\u0026lt;TextContent\u0026gt;\u0026gt; GetTextContentsAsync(string prompt, PromptExecutionSettings? executionSettings = null, Kernel? kernel = null, CancellationToken cancellationToken = default) { var client = new OllamaApiClient(ModelApiEndPoint, ModelName); OllamaApiClient.ChatRequest req = new OllamaApiClient.ChatRequest() { Model=ModelName, Prompt=prompt, }; OllamaApiClient.ChatResponse resp = await client.GetResponseForPromptAsync(req , cancellationToken); return new List\u0026lt;TextContent\u0026gt;() { new TextContent(resp.Response) }; } public async IAsyncEnumerable\u0026lt;StreamingTextContent\u0026gt; GetStreamingTextContentsAsync(string prompt, PromptExecutionSettings? executionSettings = null, Kernel? kernel = null, CancellationToken cancellationToken = default) { var ollama = new OllamaApiClient(ModelApiEndPoint, ModelName); OllamaApiClient.ChatRequest req = new OllamaApiClient.ChatRequest() { Prompt=prompt, Stream=true }; await foreach( OllamaApiClient.ChatResponse resp in ollama.GetStreamForPromptAsync(req, cancellationToken)) { yield return new StreamingTextContent( text: resp.Response) ; } } } Chat Completion 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 public class OllamaChatCompletionService : IChatCompletionService { public string ModelApiEndPoint { get; set; } public string ModelName { get; set; } public IReadOnlyDictionary\u0026lt;string, object?\u0026gt; Attributes =\u0026gt; throw new NotImplementedException(); public async Task\u0026lt;IReadOnlyList\u0026lt;ChatMessageContent\u0026gt;\u0026gt; GetChatMessageContentsAsync(ChatHistory chatHistory, PromptExecutionSettings? executionSettings = null, Kernel? kernel = null, CancellationToken cancellationToken = default) { var client = new OllamaApiClient(ModelApiEndPoint, ModelName); OllamaApiClient.ChatRequest req = new OllamaApiClient.ChatRequest() { Model=ModelName }; req.Messages = new List\u0026lt;OllamaApiClient.ChatMessage\u0026gt;(); // iterate though chatHistory Messages foreach (var history in chatHistory) { req.Messages.Add(new OllamaApiClient.ChatMessage{ Role=history.Role.ToString(), Content=history.Content }); } OllamaApiClient.ChatResponse resp = await client.GetResponseForChatAsync(req , cancellationToken); List\u0026lt;ChatMessageContent\u0026gt; content = new(); content.Add( new(role:resp.Message.Role.Equals(\u0026#34;system\u0026#34;,StringComparison.InvariantCultureIgnoreCase)?AuthorRole.System:AuthorRole.User,content:resp.Message.Content)); return content; } public async IAsyncEnumerable\u0026lt;StreamingChatMessageContent\u0026gt; GetStreamingChatMessageContentsAsync(ChatHistory chatHistory, PromptExecutionSettings? executionSettings = null, Kernel? kernel = null, CancellationToken cancellationToken = default) { var client = new OllamaApiClient(ModelApiEndPoint, ModelName); OllamaApiClient.ChatRequest req = new OllamaApiClient.ChatRequest() { Model=ModelName }; req.Messages = new List\u0026lt;OllamaApiClient.ChatMessage\u0026gt;(); // iterate though chatHistory Messages foreach (var history in chatHistory) { req.Messages.Add(new OllamaApiClient.ChatMessage{ Role=history.Role.ToString(), Content=history.Content }); } CancellationTokenSource source = new CancellationTokenSource(); CancellationToken token = source.Token; await foreach (OllamaApiClient.ChatResponse resp in client.GetStreamForChatAsync(req,token)) { yield return new(role:resp.Message.Role.Equals(\u0026#34;system\u0026#34;,StringComparison.InvariantCultureIgnoreCase)?AuthorRole.System:AuthorRole.User, content:resp.Message.Content ?? string.Empty); } } } Above implementation is for demonstration purposes only. I am sure further optimization is certainly possible.\nAfter this, it is time to use it as client of SemanticKernel SDK. Below is the test case for chat completion service,\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 [Fact] public async void TestChatGenerationviaSK() { var ollamachat = ServiceProvider.GetChatCompletionService(); // semantic kernel builder var builder = Kernel.CreateBuilder(); builder.Services.AddKeyedSingleton\u0026lt;IChatCompletionService\u0026gt;(\u0026#34;ollamaChat\u0026#34;, ollamachat); // builder.Services.AddKeyedSingleton\u0026lt;ITextGenerationService\u0026gt;(\u0026#34;ollamaText\u0026#34;, ollamaText); var kernel = builder.Build(); // chat generation var chatGen = kernel.GetRequiredService\u0026lt;IChatCompletionService\u0026gt;(); ChatHistory chat = new(\u0026#34;You are an AI assistant that helps people find information.\u0026#34;); chat.AddUserMessage(\u0026#34;What is Sixth Sense?\u0026#34;); var answer = await chatGen.GetChatMessageContentAsync(chat); Assert.NotNull(answer); Assert.NotEmpty(answer.Content!); System.Diagnostics.Debug.WriteLine(answer.Content! } Full Source code of this post is available here.\nSummary # Local AI combined with Retrieval Augmented Generation is powerful combination that any one get started with without need for subscriptions while conserving data privacy. Next step in this is to Use RAG for augmenting the results using enterprise/private data.\nHappy Coding !!\nHelpful Links # Demystifying Retrieval Augmented Generation with .NET Gemma, ollama and Langchaingo "},{"id":3,"href":"/links/java/","title":"Programming Languages - Java","section":"Links","content":" Java Language # Articles, E-books # Getting Started with Java in 2023 Finding Java Thread Leaks With JDK Flight Recorder and a Bit Of SQL "},{"id":4,"href":"/posts/tlshandsharefailure/","title":"Troubleshooting TLS handshake issue","section":"Posts","content":" Background # Ever encountered a scenario where REST API consumption works from tools like curl, Web Browser but not from Application. Lets dive in.\nThe requirement is as simple as consuming REST API from a Application over TLS.\nProblem Statement # The REST API, to be consumed, is standard API interface which requires access over TLS. The client in this case is Windows 2016 server.\nDuring Development, Windows 10 is used to develop and test the code. Later, the same is tested on a Windows 2016 Server. It is at this stage, it fails with cryptic Error \u0026ldquo;The request was aborted: Could not create SSL/TLS secure channel\u0026rdquo;. But it works fine with other tools like curl, PostMan or even from a Web Browser.\nNetwork trace log from Application Causal Analysis # Given that this error was related TLS/SSL and it is standard across platforms. What could be the reason for this behavior? With not much luck with Application level trace, its time to take help of Wireshark. If you are new to Wireshark then refer to this excellent write-up by Julia Evans.\nSo, i used wireshark during test from CURL as well as from the Application and below is what is shows,\nUsing CURL # List of Ciphers Exchanged during \u0026#39;Client Hello\u0026#39; Cipher returned by Server during \u0026#39;Server Hello\u0026#39; With CURl, TLS handshare happens as intended and API works as expected.\nVia Application # Below is list of ciphers exchanged and list is considerably short compared to earlier.\nList of Ciphers Exchanged during \u0026#39;Client Hello\u0026#39; Below is error logged\nTLS handshake Error To understand this behavior, Let\u0026rsquo;s do a quick primer.\nThere are many implementations of TLS/SSL (a.k.a. Security service providers) available across platforms. Notably,\nNetwork Security Services This is used by browsers like Firefox\nLibreSSL - Used by Chrome, curl (in ready-to-use build, refer here)\nRefer here for nice comparison of various implementations in summary format.\n`Microsoft Windows has its own Implementation called Windows SSPI (a.k.a. schannel SSPI). As per TLS/SSL Overview,\nSchannel is a Security Support Provider (SSP) that implements the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) Internet standard authentication protocols. Microsoft Windows and development platforms like .NET use this implementation by default. Via this provider it is possible for Administrators to enforce policies like restrict version of TLS, usage of ciphers and so on. Note that, as part of Security/Compliance requirements, It is often necessary to have these policies enforced and this is exactly what was done.\nBelow is Sample C# code using BouncyCastle (alternate library for cryptography)\nWrap up # Hence, the resolution for this could be,\nMake sure that Server hosting the API complies with any of the ciphers allowed on the client. In case if this is not possible then , Cipher restrictions on the client will have to be modified (assuming its within the requirement for Compliance). Useful References, # 1 - CURL using OpenSSL in default build\nHappy Troubleshooting !!\n"},{"id":5,"href":"/posts/shortidgeneration/","title":"URL Shortener in High Throughput Service","section":"Posts","content":" Background # A Client has E-commerce Application consisting of services aimed at specific domains of business functionality it serves. One of these services is responsible for accepting the order, authenticating it and forwarding it for further processing in terms of inventory checks, payment and so on. For Authentication, this service sends SMS to Customer\u0026rsquo;s Mobile number (and e-mail id) and customer is supposed to confirm this order placement by means of entering Code received in it. This code is valid for a short duration.\nThe requirement is to add a URL to this SMS which customer can use to view the order and confirm it on Mobile itself.\nFor above, there are constraints like,\nService is expected to trigger SMS, with required content like code, URL Etc., instantaneously. This is because time-bound action is expected from customer post receiving this SMS. SMS Message size restrictions to be taken into consideration while adding URL to it (since it already has other content in it). Implementation details # Given the size restrictions on SMS, a URL need to be as short as possible. Hence, URL Shortener will have to be used which reduces length of overall URL. Additionally, very low latency is expected while preparing content of SMS and sending the same (by calling Telecom Service Provider\u0026rsquo;s API) hence external services like Bitly are most probably not useful. This is because the whole response time will then be tied to performance, up-time of this external service. Better alternative is to generate short / nano ID within Service itself. This will work assuming appropriate short domain (like t.me or youtu.be etc.) is available.\nBelow are the alternatives to generate short id within the Service,\nNanoid HashIds Base62 algorithm One can choose any of the above considering tolerance for Collision. With Nanoid, one can check extent to which length can be reduced while avoiding collision using this Calculator.\nThis approach helps with,\nEncapsulation - Keeping logic of short id generation, logging it in storage (ie. database), and responding to request for URL containing this short id within service itself. Keep external dependencies to minimum as much as possible so as to have better control over latency/throughput and easier monitoring. Useful References # Why Nanoids by Planetscale Building highly reliable Web sites System Designer\u0026amp;amp;rsquo;s Interview - Insider\u0026amp;amp;rsquo;s Guide - Has Nice chapter on URL Shorteners Happy Coding !!\n"},{"id":6,"href":"/links/postgresql/","title":"PostgreSQL","section":"Links","content":" PostgreSQL # General Articles # Mistakes to avoid while using PostgreSQL Free Postgres database (or SQLite) from fly.io Using generate_series feature for reporting When Postgres blocks: 7 tips for dealing with locks PostgreSQL - Don\u0026amp;amp;rsquo;t do this Is PostgreSQL good enough? Online event processing by Martin Klepmann PostgreSQL rocks, except when it blocks: Understanding locks Connection handling best practice with PostgreSQL 10 Things I Hate About PostgreSQL PostgreSQL - Advanced Administration by Bruce Momjian Top Tools and Recommendations to Manage Postgres in an Enterprise: Administration, Performance, High Availability, and Migration Using PostgreSQL as Cache and Read Optimization tips Adyen\u0026amp;amp;rsquo;s Use of PostgreSQL PostgreSQL version Upgrade @ Gitlab Zombodb - PostgreSQL and ElasticSearch work together Using pg_timetable for job scheduling Using pg_cron to schedule background tasks Using pg_cron to roll up for Analytics PG Database Configuration Helper Full text search in PostgreSQL Postgres full text search capabilities Full text search (Crunchydata) PostgreSQL - Best practices(Azure) Designing high performance time series data table in (RDS) postgresql while using BRIN Index Informative blog on PostgreSQL Understanding GIN indexes PostgreSQL - Using SQL for Data Analysis Approach to Bulk Import in PostGreSQL Schema updates with zero downtime in PostgreSQL How to JSON in PostgreSQL Grouping, Rollups and Cubes Row level Security Just use postgres Performance tuning, configuration etc. # Partitioning as Query Optimization Strategy Database Configuration Builder Configuration for Diagnosing Performance issues OrioleDB- Solving Wicked problems of PostgreSQL 5 Minutes in PostgreSQL - Videos PostgreSQL Tips Optimizing AutoVaccum in Postgresql 10 Things i hate about PostgreSQL Various index types and their usage Few gotchas for Application Developers Five tips for healthy PostgreSQL database Make PostgreSQL healthy and speedy Diagnose Linux related Disk \u0026amp;amp; RAM issues PostgreSQL database configuration tuning advicer Database configuration for Web Services Online explain analyzer \u0026amp;amp; Generally Good Blog on PostgreSQL Vertically scaling PostgreSQL How PostgreSQL Query Optimizer works A Performance Dashboard Simple script to analyse your PostgreSQL database configuration, and give tuning advice Tuning PostgreSQL for High Write Throughput Postgres is a great pub/sub \u0026amp;amp; job server PostgreSQL - Optimize Configuration Be careful with CTE in PostgreSQL Per core Connection limit guidance for EDB PostgreSQL - Claim unused Index size PgBadger - A fast PostgreSQL Log Analyzer Using CTE to perform binary search on table Top tools to manage PostgreSQL Performance Impact of idle Postgresql connections (usage of Pgbench) How to Manage Connections Efficiently in Postgres, or Any Database How to Audit PostgreSQL Database SQL Optimizations in PostgreSQL: IN vs EXISTS vs ANY/ALL vs JOIN PostgreSQL Scaling advice in 2021 Security Hardening for PostgreSQL Working with Postgres @ Zerodha Using PostgreSQL for Data warehouse Testing PG High availability with Patroni Comparison of PostgreSQL Monitoring tools Using Timeout feature of PostgreSQL Benchmarking bulk data ingestion in PostgreSQL All about indexes in PostgreSQL Use cases for Partitioning Asynchronous Commits for faster data loading Push style Notifications and Background Queue Processing using Listen/Notify and skiplocked Queues in PostgreSQL How postgresql stores data on disk Interesting Extensions/Products # Collection of Postgresql related tools PostgreSql based Message queue End-to-end machine learning solution Incrementally update Materialized Views in real-time using Materialize Artificial Intelligence with PostgreSQL Materialize - Incrementally-updated materialized views - in ANSI Standard SQL and in real time. pg_bulkload - pg_bulkload is a high speed data loading tool for PostgreSQL. pgcenter - Command-line admin tool for observing and troubleshooting Postgres. TOTP implementation in PLPGSQL Connection pooler - Odyssey Connection pooler - PGBouncer Setting up Multiple pgBouncer Instances Connection pooler and much more - Pgpool-II Change data capture in PostgreSQL Swarm64 DA -20x faster PostgreSQL query performance Greenplum - data warehouse, based on PostgreSQL Apache Age - graph database functionality for PostgreSQL Distributed job-queue built specifically for queuing and executing heavy SQL read jobs asynchronously. Supports MySQL and Postgres. Supabase -Listen to PG changes in real time without using Listen/Notify Job queues, Single reader and pub/sub Use cases for scaling out PostgreSQL - Citus Database lab Engine - Fast cloning of Database for dev/QA/staging PGSync - Sync data from one Postgres database to another Neon - Serverless Open source PostgreSQL Push PG Listen/Notify events over Websockets Generate ERD using D2 Diagrams Mathesar - Spreadsheet-like Web interface for PostgreSQL Migration to PostgreSQL # Migration Guide from Oracle to PostgreSQL Lessons while migrating from Oracle to PostgreSQL Reshape - easy-to-use, zero-downtime schema migration tool Migra - diff tool for PostgreSQL Schema High Availability # Tools for Multi-Master Replication PostgreSQL Replication Distributed PostgreSQL Change Data Capture, Asynchronous change processing etc. # Electric SQL - Mobile Local first sync layer with PostgreSQL PGQ - Queueing Solution PGQ - as used by Skype Bucardo - Asynchronous replication for PostgreSQL using Triggers Using Logical Decoding, Wal2json for CDC Webedia\u0026amp;amp;rsquo;s approach of using Customer processor (walparser) to read from Wal2JSON and CDC between PG and Elasticsearch Message queuing using native postgresql Queues in PostgreSQL Message queueing with native postgresql Using PGQ to invalidate caches pgstream- turns your database into an event stream Wal-listener CLI Postgres as Message Queue Data Privacy # PgSodium - Interface to LibSodium from PostgreSQL including Server Key Management PostgreSQL Anonymizer - hides or replaces personally identifiable information (PII) Vector Embeddings # Storing OpenAI embeddings in Postgres with pgvector Overview of pgVector Scalability and performance # Operations cheat sheet Don\u0026amp;amp;rsquo;t do this Data Analysis # Window functions for Data Analysis "},{"id":7,"href":"/posts/is_sqlite_production_ready/","title":"Can SQLite be considered for Server Applications?","section":"Posts","content":" Introduction # While embarking on building any new server application, one of the key requirement is whether it needs durable, persistent storage of data (and in most cases, it does). This is followed by evaluating suitable data store. Likely evaluation criteria is Application\u0026rsquo;s Requirement (Tolerance for eventual consistency, High Availability etc.), Team\u0026rsquo;s familiarity, Costs, Tech. support availability and so on. In case of choices in relational databases, typical go to options are MySQL, PostgreSQL or even proprietary databases like Oracle , SQL Server. Seldom one considers SQLite for this purpose.\nAt the outset, SQLite is well-known as file-based database used in specific use cases like software running on peripheral/low resource devices such as Mobiles, tablets or in browsers for intermediate storage.Recently, i came across session by Ben Johnson on using SQLite in production. In the Video, it is mentioned that SQLite can potentially be used for server applications having 100s of concurrent requests.\nIn SQLite, There can only be a single writer at a time. However, it supports concurrency by allowing multiple connections to be opened to database and it internally serializes the write requests. This limitation (of single writer) was addressed by means of implementing Write ahead log. In this, where transactions are first written to a separate file (called \u0026rsquo;log\u0026rsquo; file) and then moved to database on commit. When WAL Mode is used, it supports much better concurrent reads and writes to the database.\nLets check if SQLite can really be considered for non-trivial, server based applications.\nCode # To have proof of concept (POC) to simulate typical real world use case, lets expose HTTP based API using Go as below,\nimport ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;log\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;os\u0026#34; \u0026#34;runtime\u0026#34; \u0026#34;strings\u0026#34; \u0026#34;github.com/rs/xid\u0026#34; _ \u0026#34;modernc.org/sqlite\u0026#34; ) const dbfilepath string = \u0026#34;./foo.db\u0026#34; const params string = \u0026#34;?_pragma=busy_timeout%3d5000\u0026amp;_pragma=journal_mode%3dwal\u0026#34; // Open opens the database connection. func Open(dsn string) (*sql.DB, error) { db, err := sql.Open(\u0026#34;SQLite\u0026#34;, dsn) if err != nil { log.Fatal(err) return nil, err } // setting this to higher number (i.e.\u0026gt; 1) not only causes constraint violation (probably because the way isolation works in SQLite over same and diff. connections) and but also degrades performances db.SetMaxOpenConns(runtime.NumCPU()) return db, nil } We will use ModernC SQLite library which is CGO free port of SQLite. However, there are other libraries listed here\nNext,we have HTTP handler which exposes POST end point and writes data to SQLite database,\nfunc main() { err := setUpDB() if err != nil { log.Println(\u0026#34;Error while setting up database\u0026#34;) return } db, err := Open(dbfilepath + params) if err != nil { log.Printf(\u0026#34;%q: %s\\n\u0026#34;, err, dbfilepath+params) return } defer db.Close() mux := http.NewServeMux() mux.HandleFunc(\u0026#34;/Addfoo\u0026#34;, FooHandler(db)) log.Println(\u0026#34;Listening on :3000...\u0026#34;) err = http.ListenAndServe(\u0026#34;:3000\u0026#34;, mux) log.Fatal(err) } func genXid() string { guid := xid.New() return guid.String() } func FooHandler(db *sql.DB) func(http.ResponseWriter, *http.Request) { return func(w http.ResponseWriter, r *http.Request) { if r.URL.Path != \u0026#34;/Addfoo\u0026#34; { http.NotFound(w, r) return } switch r.Method { case http.MethodGet: // Handle the GET request... http.Error(w, \u0026#34;method not allowed\u0026#34;, http.StatusMethodNotAllowed) case http.MethodPost: // Handle the POST request... tx, err := db.Begin() if err != nil { log.Printf(\u0026#34;begin. Exec error=%s\\n\u0026#34;, err) return } defer tx.Commit() // intVal := randomSeed.Int63() uid := genXid() _, err = tx.Exec(fmt.Sprintf(\u0026#34;insert into Foo(id, Name) values(\u0026#39;%s\u0026#39;,\u0026#39;name-%s\u0026#39;)\u0026#34;, uid, uid)) if err != nil { log.Printf(\u0026#34;Error inserting record -\u0026gt; %s\\t%s\\n\u0026#34;, err.Error(), strings.HasSuffix(err.Error(), \u0026#34;(SQLite_BUSY)\u0026#34;)) http.Error(w, \u0026#34;Internal Error\u0026#34;, http.StatusInternalServerError) return } w.WriteHeader(http.StatusCreated) case http.MethodOptions: w.Header().Set(\u0026#34;Allow\u0026#34;, \u0026#34;GET, POST, OPTIONS\u0026#34;) w.WriteHeader(http.StatusNoContent) default: w.Header().Set(\u0026#34;Allow\u0026#34;, \u0026#34;GET, POST, OPTIONS\u0026#34;) http.Error(w, \u0026#34;method not allowed\u0026#34;, http.StatusMethodNotAllowed) } } } Above is simple HTTP handler function which is invoked on call to /Addfoo endpoint and adds a record to a table in database.\nNext is to check throughput provided by this HTTP API. We can use benchmarking tool for this purpose. It generates a load against API and records the response times, errors and so on and presents analysis based on it. One such tool is Bombardier and there are others like wrk, wrk2 and so on. I used Bombardier primarily because it is cross-platform (Golang based) and works on Windows, which i am using to conduct this POC.\nFirst, application is started as go run . which starts the HTTP server, ready to receive requests.\nNext is to use Bombardier to assess throughput of the API,\nWith limit of 100 requests per second, result shows, average latency of 2.23ms with no errors thrown Database has around 1252 records. bombardier.exe -m POST -l -r 100 http://localhost:3000/Addfoo Bombarding http://localhost:3000/Addfoo for 10s using 125 connection(s) [===========================================================================================================================================================] 10s Done! Statistics Avg Stdev Max Reqs/sec 99.96 32.72 254.39 Latency 2.23ms 4.39ms 119.61ms Latency Distribution 50% 1.72ms 75% 2.32ms 90% 3.12ms 95% 3.31ms 99% 4.44ms HTTP codes: 1xx - 0, 2xx - 1001, 3xx - 0, 4xx - 0, 5xx - 0 others - 0 Throughput: 21.20KB/s With limit of 100 requests per second, result shows, average latency of 2.23ms with no errors thrown Database has around 1252 records. bombardier.exe -m POST -l -r 100 http://localhost:3000/Addfoo Bombarding http://localhost:3000/Addfoo for 10s using 125 connection(s) [===========================================================================================================================================================] 10s Done! Statistics Avg Stdev Max Reqs/sec 99.96 32.72 254.39 Latency 2.23ms 4.39ms 119.61ms Latency Distribution 50% 1.72ms 75% 2.32ms 90% 3.12ms 95% 3.31ms 99% 4.44ms HTTP codes: 1xx - 0, 2xx - 1001, 3xx - 0, 4xx - 0, 5xx - 0 others - 0 Throughput: 21.20KB/s With limit of 1000 requests per second, Latency has gone up 15x Still no error reported and database has additional records. bombardier.exe -m POST -l -r 1000 http://localhost:3000/Addfoo Bombarding http://localhost:3000/Addfoo for 10s using 125 connection(s) [===========================================================================================================================================================] 10s Done! Statistics Avg Stdev Max Reqs/sec 964.73 314.04 2149.94 Latency 30.05ms 80.20ms 1.48s Latency Distribution 50% 4.02ms 75% 16.00ms 90% 74.75ms 95% 155.30ms 99% 434.48ms HTTP codes: 1xx - 0, 2xx - 9670, 3xx - 0, 4xx - 0, 5xx - 0 others - 0 Throughput: 202.72KB/s With limit of 4000 requests per second, it looks like below, bombardier.exe -m POST -l -r 4000 http://localhost:3000/Addfoo Bombarding http://localhost:3000/Addfoo for 10s using 125 connection(s) [===========================================================================================================================================================] 10s Done! Statistics Avg Stdev Max Reqs/sec 1304.49 688.92 2199.91 Latency 95.00ms 174.73ms 2.40s Latency Distribution 50% 35.00ms 75% 95.11ms 90% 228.16ms 95% 382.68ms 99% 1.02s HTTP codes: 1xx - 0, 2xx - 13186, 3xx - 0, 4xx - 0, 5xx - 0 others - 0 Throughput: 275.73KB/s Overall, above shows that,\nSQLite with WAL mode on and busy timeout set to 5 seconds can support high concurrency Above is very simplistic test, in real application it is likely going to be very different (i.e. most likely on the lower side of throughput) since there will be multiple connections reading and writing to not one but many tables concurrently. This is likely to impact throughput and latency. One of the factors on why better throughput is recorded could be because SQLite, being implemented as library, can be easily integrated with application and resides on same node/VM as application. This helps tremendously in avoiding network round trip and helps in much better performance. Refer to Martin Fowler\u0026rsquo;s First law Back to evaluating databases for a given use case and SQLite fits in ,\nCriteria Description SQLite ACID Guarantees Is Database expected to provide Strong Consistency guarantees? (this may not be required for every use case) Yes.Note that, there is no isolation between operations that occur within the same database connection. Data Durability Does Database maintain data in durable ,consistent way? Yes Reliability Does it provide reliable storage ? (Although storage reliability is not limited only to software and often depends on other factors like type of storage, associated hardware.) Yes Availability Is database highly available? SQLite being file based, availability is confined to the Node/VM on which it is running. It can be further enhanced using tools like, Litestream (which implements change data capture and syncs it with remote storage like AWS S3 or SFTP among others). rqlite is clustered database based on SQLite. network partition support Does database support partitioning of data? No. Being a file based data storage system, it is constrained on single node i.e. it can be scaled vertically. However, it can be setup in active + Stand-by backup mode using specific tools. Additionally, other databases (files on same node) can be attached to and accessed by application as one database. Tech. Support In case of FOSS software, is Community active in terms of releases/bug fixes as well as on discussion forums? Are their any providers who provide paid support? SQLite is mature database. Though, it is open source, it does not accept pull requests from anyone out side of core committers. Having said that, one has to check for availability of support in case things go north (corrupted database and so on.) Database features Support for Typical RDBMS features like Data types, User Management \u0026amp; Security, Stored procedures (but not triggers) etc. Refer here for detailed comparison of features across SQLite and populate RDBMSs. Hopefully, above provides good starting point in deciding database for your next application. As always, comments/suggestions are welcome.\nUseful References # Limits in SQLite Consider SQLite SQLite has good support for JSON, read about it here SQLite as a document database, read about it here Interesting lists of extensions sqlite-utils - Collection of utilities including migration from MySQL/PostgreSQL Sqlite and Go by David Crawshaw Server side SQLite SQLite backup using Cron Happy Coding !!\n"},{"id":8,"href":"/posts/profiling_n_benchmarking/","title":"Profiling and benchmarking tools for Applications","section":"Posts","content":" Introduction # We develop a piece of software with aim to fulfil specific business requirements in terms of resource usage, throughput, availability among others. Profiling and benchmarking are approaches that developer has in his/her arsenal to gain continuous feedback on whether a piece of code is behaving optimally and adhering to it\u0026rsquo;s objectives.\nLets look at what they mean,\nProfiling is defined as process aimed at understanding the behavior of a program. A profile result might be a table of time taken per function, as per this and this) Benchmarking measures the time for some whole operation. e.g. I/O operations per second under some workload. So the result is typically a single number, in either seconds or operations per second. Or a data set with results for different parameters, so you can graph it.. Refer this for more information. Also do check Benchmarking correctly is hard by Julia Evans. Typically, Profiling is supported by most of the environments (either via IDEs like Visual Studio or through language itself [Like Go] has buil-in provision for the same while Benchmarking is typically performed on dedicated testing infrastructure.\nIn this article, We will look at couple of tools in this space that can be easily integrated in developer\u0026rsquo;s workflow so as to get early feedback. Lets\u0026rsquo; go.\nProfiling # Pyroscope is Open Source Application for profiling Application. It is a cross-language tool i.e. programs in variety of languages can be profiled using it. It works in client server model where in, - Client - Pyroscope executable runs the intended code (in languages like C#, Ruby) etc. (in case of Go, it is available as dependency) and collects instrumentation details to be sent to server. - Server - Runs as a separate process (on Linux [Works in WSL if using Windows] or Mac), collects the data from client processes and renders them as table and/or flame graph via Web UI.A flamegraph is a way to visualize resources used by a program, like CPU usage or memory allocations, and see which parts of your code were responsible.\nLets see how a function in C# can be instrumented using PyroScope.\nDevelop function to be profiled Lets have ASP.NET Core 5.0 based Web API as below, Simple Web API handler in C# Setup Pyroscope\nInstall Pyroscope Application by following instructions here for Windows. Note that, Pyroscope server component won\u0026rsquo;t run on Windows in which case either Windows Subsystem for Linux (WSL) or Docker can be used. In case of Linux, instructions provided here are sufficient for both client and server components.\nI have setup the application on Windows 10 while using WSL for Pyroscope Server.\nConfigure Pyroscope client and run the Application\nBuild the application using dotnet build\nConfigure below environment variables (below is powershell format or you can use SET ... commands on command prompt),\n``` $env:PYROSCOPE_SPY_NAME=\u0026quot;dotnetspy\u0026quot;; $env:PYROSCOPE_APPLICATION_NAME=\u0026quot;my.dotnet.app\u0026quot;; $env:PYROSCOPE_SERVER_ADDRESS=\u0026quot;http://localhost:4040\u0026quot;; ``` Update path to include pyroscope installation folder using $env:Path += \u0026quot;;C:\\Program Files\\Pyroscope\\Pyroscope Agent\\\u0026quot;\nRun the Application using pyroscope exec dotnet .\\bin\\Debug\\net5.0\\webapi.dll.\nRun Pyroscope Server\nStart Pyroscope server from WSL Linux prompt using, sudo pyroscope server. The output of this command should show Port on which server is running.\nEither use curl or hey tool to invoke the API. Below command shows how to generate load using hey, run .\\hey.exe -m GET -c 10 -q 2 http://localhost:5000/weatherforecast (Note: Modify the URL As appropriate) Observe the flame graph in Pyroscope Web UI.\nObserve the Table and/or flamegraph using Pyroscope Web interface. Below screenshot shows flamegraph for above code. Refer here and here for everything about flame graphs.\nTable and flamegraph for API Table and flamegraph for API Overall, Pyroscope provides easy way to observe Memory/CPU utilization as part of developer workflow on workstation itself. This is especially useful for development environments which do not provide profiling out of the box.\nBenchmarking # Crank is tool used by Microsoft internally to benchmark applications. It is released as Nuget package and currently .NET based code or Docker Containers can be benchmarked using it. Lets see steps to benchmark .NET Application using Crank.\nWrite code, intended to be benchmarked. In this case, its very simple one as below,\nC# Code to be benchmarked Setup Crank\nFollow the instructions provided here to setup crank. Crank expects Configuration in YAML format which contains details like Job to be used. Crank has built-in jobs which are essentially wrappers around CLI load testing tools like bombardier and wrk and so on. Since i am using Windows to run crank, we will go with Bombardier which is cross platform. Below is how a basic configuration looks like,\nCrank YAML Configuration It allows for extensibility in terms of overriding the job configuration in terms of how load should be generated etc.\nRun Crank Agent - Next step is to run crank agent in a command prompt or powershell by simply running crank-agent\nRecord data for benchmarking using Crank CLI.\nNow run Crank from the application folder as crank --config crank.benchmarks.yml --scenario hello --profile local --application.options.displayOutput true\nThis command builds the code and launches job while recording the Utilization and other parameters and shows output like,\nApplication\u0026#39;s CPU Utilization Observations during executing load testing Overall, i found Crank helpful for following,\nit helps quickly test effect of any code changes by means of quickly benchmarking the application. The overall benchmarking might not be similar to end state ie. when the application will be deployed on target infrastructure. However, it still gives insights to developer about impact of code changes Crank can be easily used for local applications as well as for docker containers. It can either be used locally or in distributed manner. Useful References # Pyroscope Crank Performance Anti-patterns Happy Profiling and Benchmarking !!\n"},{"id":9,"href":"/posts/dbre/","title":"Database Reliability Engineering - My Notes","section":"Posts","content":" Introduction # I have been reading excellent Database Reliability Engineering book and below are my notes from it.\nKey Incentive(s) for Automation\nElimination of Toil - Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows. Important System Characteristics\nLatency, also known as response time, is a time-based measurement indicating how long it takes to receive a response from a request. It is best to measure this for end-to-end response from the customer rather than breaking it down component by component. This is customer-centric design and is crucial for any system that has customers, which is any system\nAvailability - This is generally expressed as a percentage of overall time the system is expected to be available. Availability is defined as the ability to return an expected response to the requesting client. Note that time is not considered here, which is why most SLOs include both response time and availability. After a certain latency point, the system can be considered unavailable even if the request is still completing. Availability is often denoted in percentages, such as 99.9% over a certain window. All samples within that window will be aggregated\nHigh availability - For any mission-critical data that you truly care about, you should avoid running with less than three live copies. Thats one primary and two-plus secondaries for leader-follower data stores like MySQL or MongoDB or a replication factor of three for distributed data stores like Cassandra or Hadoop. Because you never, ever want to find yourself in a situation in which you have a single copy of any data you care about, ever. This means that you need to be able to lose one instance while still maintaining redundancy, which is why three is a minimum number of copies, not two.\nInfrastructure engineering # Virtualization * Hypervisor - A hypervisor or virtual machine monitor (VMM) can be software, firmware, or hardware. The hypervisor creates and runs VMs. A computer on which hypervisor runs one or more VMs is called a host machine, and each VM is called a guest machine. The hypervisor presents the guest operating systems with a virtual operating platform and manages the execution of the guest operating systems. Databases running within hypervisors show lower boundaries for concurrency than the same software on bare metal. When designing for these virtualized environments, the focus should be on a horizontally scaled approach, minimizing concurrency within nodes.\nStorage - Storage durability and performance are not what you would expect in the virtualized world. Between the page cache of your VM and the physical controller lies a virtual controller, the hypervisor, and the hosts page cache. This means increased latency for I/O. For writes, hypervisors do not honor calls in order to manage performance. This means that you cannot guarantee that your writes are flushed to disk when there is a crash.\nDatabase Servers\nPhysical servers - Recommended to have dedicated ones for database (and not shared) Why ? Much control with OS and more visibility May find redundant capacity on a dedicated hardware. Linux is not particularly optimized for database loads requiring low latency and high concurrency. The kernel is not predictable when it goes into reclaim mode, one of the best recommendations we can give is to simply ensure that you never fully use your physical memory by reserving it to avoid stalls and significant latency impacts. You can reserve this memory by not allocating it in configuration Storage\nCapacity - Single large disks are single points of failure, unless mirrored (RAID 1). RAID 0 will have its MTBF reduced by a factor of N, where N is the number of disks striped together.\nThroughput - When considering the needs, you must consider IOPS for the peak of a databases workload rather than the average.\nLatency - Latency is the end-to-end client time of an I/O operation; in other words, the time elapsed between sending an I/O to storage and receiving an acknowledgement that the I/O read or write is complete.\nTransactional applications are sensitive to increased I/O latency and are good candidates for SSDs. You can maintain high IOPS while keeping latency down by maintaining a low queue length and a high number of IOPS available to the volume. Consistently driving more IOPS to a volume than it has available can cause increased I/O latency. Throughput-intensive applications like large MapReduce queries are less sensitive to increased I/O latency and are well-suited for HDD volumes. You can maintain high throughput to HDD-backed volumes by maintaining a high queue length when performing large, sequential I/O Availability - Plan for disk failures.\nDurability - When your database goes to commit data to physical disk with guarantees of durability, it issues an operating system call known as rather than relying on page cache flushing. An example of this is when a redo log or write-ahead log is being generated and must be truly written to disk to ensure recoverability of the database. Filesystem operations can also cause corruption and inconsistency during failure events, such as crashes. Journaling filesystems like XFS and EXT4 significantly reduce the possibility of such events, however.\nStorage Area Networks (SAN) vs. SSDs - Data snapshots and movement are some of the nicest features in modern infrastructures, where SSDs provide better IO than traditional SANs.\nRelational Database Internals\nIn Relational databases, data is stored in containers called blocks or pages that correspond to a specific number of bytes on disk. Different databases will use blocks or pages in their terminology. In this book, we use blocks to refer to both. Blocks are the finest level of granularity for storing records. Oracle Database stores data in data blocks. A page is a fixed size called a block, just like blocks on disks. Blocks are the smallest size that can be read or written to access data. This means that if a row is 1 K and the block size is 16 K, you will still incur a 16 K read operation. If a database block size is smaller than the filesystem block size, you will be wasting I/O for operations that require multiple pages. A block require some metadata to be stored, as well, usually in the form of a header and trailer or footer. This will include disk address information, information about the object the block belongs to, and information about the rows and activity that have occurred within that block.\nMost databases structure their data in a binary tree format, also known as B-tree. A B\u0002tree is a data structure that self-balances while keeping data sorted. The B-tree is optimized for the reading and writing of blocks of data, which is why B-trees are commonly found in databases and filesystems\nSummary of the attributes and benefits of B-trees: Excellent performance for range-based queries.\nNot the most ideal model for single-row lookups\nKeys exist in sorted order for efficient key lookups and range scans.\nStructure minimizes page reads for large datasets.\nBy not packing keys into each page, deletes and inserts are efficient, with only occasional splits and merges being needed.\nPerform much better if the entire structure can fit within memory.\nA crucial variable in configuring your databases for underlying storage is the database block size. Weve discussed the importance of aligning database block sizes with the underlying disk block sizes, but that is not enough. If you are using Solid-State Drives (SSDs), for instance, you might find smaller block sizes provide much better performance while traversing B-trees. An SSD can experience a 30% to 40% latency penalty on larger blocks versus performance on Hard Disk Drives (HDDs). Because reads and writes are required in B-tree structures, this must be taken into account.\nNon-Relational Database Internals\nWhat is sorted-string tables (SST) storage engine? - It has a number of files, each with a set of sorted keyvalue pairs inside. Unlike in the block storage discussed earlier, there is no need for the metadata overhead at the block or row level. Keys and their values are opaque to the DBMS and stored as arbitrary binary large objects (BLOBs). Because they are stored in a sorted fashion, they can be read sequentially and treated as an index onthe key by which they are sorted.\nThere is an algorithm that combines in-memory tables, batch flushing, and periodic compaction in SST storage engines. This algorithm is referred to a log-structured merge (LSM) tree architecture\nA bloom filter is a data structure that you can use to evaluate whether a record key is present in a given set.\nDatabase Indexes\nHash indexes - A hash map is a collection of buckets that contain the results of a hash function applied to a key. That hash points to the location where the records can be found. A hash map is only viable for single-key lookups because a range scan would be prohibitively expensive. Bitmap Indexes - A bitmap index stores its data as bit arrays (bitmaps). When you traverse the index, it is done by performing bitwise logical operations on the bitmaps. In B-trees, the index performs the best on values that are not repeated often. This is also known as high cardinality. The bitmap index functions much better when there are a small number of values being indexed Replication\nTypes Synchronous - A transaction that is written to a log on the leader is shipped immediately over the network to the followers. The leader will not commit the transaction until the followers have confirmed that they have recorded the write. This ensures that every node in the cluster is at the same commit point. This means that reads will be consistent regardless of what node they come from, and any node can take over as a leader without risk of data loss if the current leader fails. On the other hand, network latency or degraded nodes can all cause write latency for the transaction on the leader.\nAsynchronous - A transaction is written to a log on the leader and then committed and flushed to disk. A separate process is responsible for shipping those logs to the followers, where they are applied as soon as possible. In asynchronous replication models, there is always some lag between what is committed on the leader and what is committed on the followers. Additionally, there is no guarantee that the commit point on one follower is the same as the others. In practice, the time gap between commit points might be too small to notice.\nSemi-synchronous - In this algorithm, only one node is required to confirm to the leader that they have recorded the write. This reduces the risk of latency impacts when one or more nodes are functioning in degraded states while guaranteeing that at least two nodes on the cluster are at the same commit point. In this mode, there is no longer a guarantee that all nodes in the cluster will return the same data if a read is issued on any reader.\nFormats used during Replication\nStatement based logs - the actual SQL or data write statement used to execute the write is recorded and shipped from the leader to followers. e.g. MySQL Write-ahead logs - A write-ahead log (WAL), also known as a redo log, contains a series of events, each event mapped to a transaction or write. In the log are all of the bytes required to apply a transaction to disk. In systems, such as PostgreSQL, that use this method, the same log is shipped directly to the followers for application to disk. Approaches\nRow based Replication - In row-based replication (also called logical), writes are written to replication logs on the leader as events indicating how individual table rows are changed. Columns with new data are indicated, columns with updated information show before/after images, and deletes of rows are indicated as well. Replicas use this data to directly modify the row rather than needing to execute the original statement.\nBlock level Replication - Block-level replication is synchronous and eliminates significant overhead in the replicated write. However, you cannot have a running database instance on the secondary node. So, when a failover occurs, a database instance must be started. If the former master failed without a clean database shutdown, this instance will need to perform recovery just as if the instance had been restarted on the same node.\nMethods\nSingle Leader - (Simplest of replicated environments) All writes go to single leader and are replicated to other nodes. Advantages are that there will be no consistency conflicts. There are some variations like data getting replicated to only few followers which further replicate to remaining ones. By far the most common implementation of replication due to simplicity.\nMultiple Leaders - There are 2 approaches,\nThere are typically 2 leaders responsible for receiving writes and propagating them to replicas. each leader is located in different data centers/availability zones. Any node can take reads or writes at any time. More complex than Single Leader approach due to need for conflict resolution. Use cases,\nAvailability - In case of failover with Single Leader approach, impact may last from 30 seconds to minutes depending on how the system is designed. This is due to replication consistency checks, crash recovery and more such steps. This impact could be unacceptable. Locality - Application is requirement is such that it needs to cater to users in different regions with separate datacenters. This could be to for data protection purposes or to ensure low latency. Disaster Recovery - Highly critical application with need to have multiple data centers to ensure availability. Conflict resolution approaches,\nSharding - distribute range of primary keys across leaders Affinity - Specific users (by region, unique ID) are always redirected to specific leader Shard by Application layer ie. Application instance is deployed in each datacenter avoid need for active/active cross region replication Write anywhere- Any node can take read or write requests. Attributes typically associated with such systems are,\nEventual consistency - there is no guarantee that data is consistent across all nodes at any time, that data will eventually converge. Read \u0026amp; Write Quorum - It indicates minimum number of readers or writers necessary to guarantee consistency of data. Quorum of 2 in 3 node cluster means one node\u0026rsquo;s failure is tolerated. Formula: N is the number of nodes in a cluster.R is the number of read nodes available, and W is the number of write nodes. If R + W is greater than N, you have an effective quorum to guarantee at least one good read after a write. Sloppy quorums - Indicates situation when nodes are available but unable to meet quorum due to lack of data. Anti Entropy - Mechanism to keep data synchronized across nodes even in case of inactivity (i.e. no reads). Anti-entropy is critical for datastores that store a lot of cold, or infrequently accessed,data. Data governance is the management of the availability, integrity, and security of the data that an organization saves and uses. Introduction of new data attributes is something that should be considered carefully and documented. The use of JSON for data storage allows new data attributes to be introduced too easily and even accidentally.\nImportant aspects in Infrastructure Architecture,\nRelaxed durability means data loss must be considered an inevitability. Instance instability means automation, failover, and recovery must be very reliable. Horizontal scale requires automation to manage significant numbers of servers. Applications must be able to tolerate latency instability. Infrastructure Management # An immutable infrastructure is one that is not allowed to mutate, or change, after it has been deployed. If there are changes that must happen, they are done to the version controlled configuration definition, and the service is redeployed.In the interest of moderation and middle ground, there can be some mutations that are frequent, automated and predictable, and can be allowed in the environment. Manual changes are still prohibited, keeping a significant amount of the value of predictability and recoverability while minimizing operational overhead. , Packer allows you to create multiple images from the same configuration. This includes images for virtual machines on your workstation. Using a tool like Vagrant on your workstation allows you to download the latest images, build the VMs, and even run through a standard test suite to verify that everything works as expected.\nPacker is one such tool from Hashicorp that creates images. The interesting thing about Packer is that it can create images for different environments (such as Amazon EC2 or VMWare images) from the same configuration. Most configuration management utilities can create baked images as well. Service Discovery \u0026amp; Service catalog - Service discovery is an abstraction that maps specific designations and port numbers of your services and load balancers to semantic names. A service catalog can be very simple, storing service data to integrates services, or it can include numerous additional facilities, including health checks to ensure that data in the catalog provides working resources.\nIsolation of Network Traffic - Network traffic can be broken up in,\nInternode communications Application traffic Administrative traffic Backup and recovery traffic Isolation of traffic is one of the first steps to proper networking for your databases. You can do this via physical network interface cards (NICs), or by partitioning one NIC Data Security - Tracking every failed and successful SQL statement sent to database is critical for identifying SQL injection attacks. SQL syntax errors can be a leading indicator\nData Architecture\nFrontline Datastores - Historically, these systems have been referred to as OnLine Transactional Processing (OLTP) systems. They were characterized by a lot of quick transactions, and thus they were designed for very fast queries, data integrity in high concurrency, and scale based on the number of transactions they can handle concurrently. All data is expected to be real time with all of the necessary details to support the services using them. Each user or transaction is seeking a small subset of the data. This means query patterns tend to focus on finding and accessing a small, specific dataset within a large set. Effective indexing, isolation, and concurrency are critical for this, which is why it tends to be fulfilled by relational systems. Typical characteristics are, Low-latency writes and queries\nHigh availability\nLow Mean Time to Recover (MTTR)\nAbility to scale with application traffic\nEasy integration with application and operational services\nDatabase proxies - Sits between application and frontline datastores. It could be,\nLayer 4 (Networking transport layer) - Uses the information available at networking layer like destination IP Addresses to distribute the traffic. This type can not work with factors like load or replication lag while distributing traffic Layer 7 - Operates at higher level of networking transport layer. At this layer, proxy can include functionality like, Health checking and redirection to healthy servers Splitting of reads and writes to send reads to replicas Query rewriting to optimize queries that cannot be tuned in code Caching query results and returning them Redirecting traffic to replicas that are not lagged Generate metrics on queries Perform firewall filtering on query types or hosts Event and Messaging systems - Used for actions to be triggered after a transaction like,\nData must be put into downstream analytics and warehouses Orders must be fulfilled Fraud detection must review a transaction Data must be uploaded to caches or Content Delivery Networks (CDNs) Personalization options must be recalibrated and published Caches and Memory Store - Used to overcome slowness in Disk I/o. Approaches to putting data are,\nPutting data in cache after its been written to persistent data store Writing to cache and datastore at the same time (Fragile due to possibility of one of the store failing) Writing to cache first and then to datstore asynchronously (Write-through approach) Lambda Architecture - The Lambda architecture is designed to handle a significant volume of data that is processed rapidly to serve near-real-time requests, while also supporting long\u0002running computation. Lambda consists of three layers: batch processing, real-time processing, and a query layer.If data is written to a frontend datastore, you can use a distributed log such as Kafka to create a distributed and immutable log for the Lambda processing layers. Some data is written directly to log services rather than going through a datastore. The processing layers ingest this data.\nKappa Architecture - Append only immutable log is used in this Architecture. Kappa architecture eliminates the batch processing system, with the expectation that the streaming system can handle all transformations and computations. One of the biggest values to Kappa is the reduction in complexity and operational expense of Lambda by eliminating the batch processing layer. It also aims to reduce the pain of migrations and reorganizations. When you want to reprocess data, you can start a reprocessing, test it, and switch over to it.\nApplication Architecture Patterns\nEvent sourcing pattern - Changes to entities are saved as sequence of state changes. When state changes, a new event is appended to the log. The datastore is called as event store. it maintains audit of life cycle of entity which helps in recreation or populating the tables in case of data loss. However, evolution of entity over period of time needs to be managed as it may invalidate previous events. It allows giving full historical access via API for auditing, reconstruction, and different transformations can provide significant benefits.\nCQRS - The driver for this is the idea that same data can be represented for consumption using multiple models or views. like Append only log for writes and read optimized data stores for queries.\nMonitoring and Observability # Synthetic Monitoring - The case for synthetic monitoring is to provide coverage that is consistent and thorough. Users might come from different regions and be active at different times. This can cause blind spots if we are not monitoring all possible regions and code paths into our service. With synthetic monitoring, we are able to identify areas where availability or latency is proving to be unstable or degraded, and prepare or mitigate appropriately. Examples of such preparation/mitigation include adding extra capacity, performance tuning queries, or even moving traffic away from unstable region\nLatency SLO - Service Level Objective could be \u0026ldquo;Ninety-nine percent request latency over one minute must be between 25 and 100 ms\u0026rdquo;.\nWHY MTTR (Mean time to recover) Over MTBF (Mean time between failure)? When you create a system that rarely breaks, you create a system that is inherently fragile. Will your team be ready to do repairs when the system does fail? Will it even know what to do? Systems that have frequent failures that are controlled and mitigated such that their impact is negligible have teams that know what to do when things go sideways. Processes are well documented and honed, and automated remediation becomes actually useful rather than hiding in the dark corners of your system\nSome of the Important statistics (Metrics, Events, Logs\u0026hellip;.) to be observed from observability perspective are,\nMetrics\nLatency - How long are client calls to your service ? Availability - How many calls result in errors? Call Rates - How oftern are calls sent to service? Utilization - How critical resources are being utilized to ensure quality of service and capacity. Types of Metrics, Counters - These are cumulative metrics that represent how many times a specific occurrence of something has occurred. Gauges - These are metrics that change in any direction, and indicate a current value, such as temperature, jobs in queue, or active locks. Histograms - A number of events broken up into configured buckets to show distribution. Summaries - This is similar to histogram but focused on proving counts over sliding windows of time Events - An event is a discrete action that has occurred in the environment. A change to a config is an event. A code deployment is an event. A database master failover is an event. Each of these can be signals that are used to correlate symptoms to causes.\nAlerts \u0026amp; Notifications\nAlerts - An alert is an interrupt to a human that instructs him to drop what hes doing and investigate a rules violation that caused the alert to be sent. This is an expensive operation and should be utilized only when SLOs are in imminent danger of violation.\nTickets/tasks - For work that must be done but there is o imminent disaster. The output of monitoring should be tickets/tasks for developers\nNotifications - Events like Code Deployment completed.\nAutomation - One example is Autoscaling basis outcome of monitoring.\nVisualization - GUI tool for visualizing outcome of monitoring.\nMinimum Viable monitoring set # Databases Monitor if your databases are up or down (pull checks). Monitor overall latency/error metrics and end-to-end health checks (push checks).\nInstrument th application layer to measure latency/errors for every database call (push checks).\nGather as many metrics as possible about the system, storage, database, and app layers, regardless of whether you think they will be useful. Most operating systems, services, and databases will have plug-ins that are fairly comprehensive.\nCreate specific checks for known problems. For example, checks based on losing x percent of database nodes or a global lock percent that is too high\nHealth check at the application level that queries all frontend datastores\nQuery run against each partition in each datastore member, for each datastore\nImminent capacity issues\nDisk capacity Database connections Error log scraping\nDB restarts Corruption Database connection layer - A tracing system should be in place be able to break out time talking to a proxy and time from the proxy to the backend as well. You can capture this via tcpdump and Tshark/Wireshark for ad hoc sampling. This can be automated for occasional sampling.\nUtilization Connection upper bound and connection count (Tip: PostgreSQL uses one Unix process per connection. MySQL, Cassandra, and MongoDB use a thread per connection) Connection states (working, sleeping, aborted, and others) Kernel-level Open file utilization Kernel-level max processes utilization Memory utilization Thread pool metrics such as MySQL table cache or MongoDB thread pool utilization Network throughput utilization Measure Saturation using, TCP connection backlog Database-specific connection queuing, such as MySQL back_log Connection timeout errors Waiting on threads in the connection pools Memory swapping Database processes that are locked With utilization and saturation, you can determine whether capacity constraints and bottlenecks are affecting the latency of your database connection layer. Monitor Errors, Database logs will provide error codes when database-level failures occur. Sometimes you have configurations with various degrees of verbosity. Make sure you have logging verbose enough to identify connection errors, but do be careful about overhead, particularly if your logs are sharing storage and IO resources with your database. Application and proxy logs will also provide rich sources of errors. Host errors discussed in the previous section should also be utilized Internal Database Activity\nThroughput and latency metrics Reads Writes Inserts Updates Deletes Other Operations Commits Rollbacks DDL Statements Other admin. tasks Commits, redo, journaling Dirty buffers (MySQL) Checkpoint age (MySQL) Pending and completed compaction tasks (Cassandra) Tracked dirty bytes (MongoDB) (Un)Modified pages evicted (MongoDB) Memory structures A mutex (Mutually Exclusive Lock) is a locking mechanism used to synchronize access to a resource such as a cache entry. Only one task can acquire the mutex. This means that there is ownership associated with mutexes, and only the owner can release the lock (mutex). This protects from corruption. A semaphore restricts the number of simultaneous users of a shared resource up to a maximum number. Threads can request access to the resource (decrementing the semaphore) and can signal that they have finished using the resource (incrementing the semaphore). Application\nMeasuring and logging all requests and responses to pages or API endpoints. also external services, which includes databases, search indexes, 3rd party APIs and caches. Any jobs or independent workflows that should be monitored. Any independent, reusable code like a method or function that interacts with databases, caches, and other datastores should be instrumented. Monitor how many database calls are executed by each endpoint, page, or function/method When doing SQL tuning, a big challenge is mapping SQL running in the database to the specific place in the codebase from which it is being called. In many database engines, you can add comments for information. These comments will show up in the database query logs. This is a great place to insert the codebase location Logging - Logs should include stack traces Setting up an external check for each major product or service, as well as a health check on the monitoring service itself, is a good best practice Server (Metrics)\nCPU Memory Network interfaces Storage I/O Storage capacity Storage controllers Network controllers CPU interconnect Memory interconnect Storage interconnect Server (Logs) - should be sent to processors (e.g. Logstash, Loki)\nkernel, cron, authentication, mail, and general messages logs as well as process- or application-specific log to ingest, such as MySQL, or nginx Overall, this book is highly recommended for understanding the Observability landscape. Though focussed on databases, it covers lot of ground on other aspects involved in infrastructure.\nHappy Coding !!\n"},{"id":10,"href":"/posts/nrtanalysispostgresql/","title":"Near real time API Monitoring with Grafana and PostgreSQL","section":"Posts","content":" Introduction # Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being \u0026ldquo;always on\u0026rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.\nWhat if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?\nIs it even possible to have near real time Monitoring of Application\u0026rsquo;s behaviour using already-in-use technologies (like PostgreSQL) ?\nMonitoring and more generically, \u0026ldquo;Observability\u0026rdquo; has three pillars. They are Logs, Metrics and traces. Many of the existing applications are producing either (mostly logs or traces) but seldom all. Hence, it is necessary to use existing logs/traces as basis for Metrics generation.\nThere are on-going developments With standards like Opentelemetry in this field. Some have even suggested ( here \u0026amp; here) that traces (distributed) will eventually replace logging.\nApproach # The high level architecture looks like below,\nHigh Level Architecture Considering as-is state of Application Architecture and given the constraints (mentioned earlier), this post covers approach that is based upon,\nPostgreSQL - Data store for Analytics and Reporting TimescaleDB - Timescale plugin for PostgreSQL FluentBit - Processing of Web Server logs \u0026amp; forwarding to database Grafana - Data Visualization and Monitoring platform. Lets see how to get this done step by step.\nData Collection and Storage # TimescaleDB # Timescale is a Postgresql Plugin for time-series data management.\nRationale\nThe reports and dashboards expected for near real time API monitoring are time intensive in nature. TimescaleDB is optimized for such time intensive reporting and suits well for this use case as it is a plugin over PostgreSQL, which is already being used for analytics/reporting. Installation of plugin is straightforward. Step by Step tutorial is very helpful.\nNext step is to create a database for the data to be used for Monitoring. Hyper table(s) in this database will contain Metrics data, collected from Application and web server (IIS).\nOne of the required dashboard/report was to monitor API request(s) in terms of success \u0026amp; failure (%), Response times (in buckets like 1-5 secs,5-10 secs and so on).\nFor each API request, application collects specific details and persists it in database. Currently below attributes are stored in database as part of each log entry,\nAttribute Description Time Timestamp of event Service Attribute indicating service name Operation Attribute indicating Operation of the service for which request was received Outcome Outcome of the API Invocation i.e. Success or Failure Timeout Timestamp of completion of API invocation The DDL command will look like,\ncreate table apilog (time timestamptz not null, service text not null, operation text, outcome text not null, timeout timestamptz ); After creating the table, it will have to be converted into Hypertable by using command,\nSELECT create_hypertable('apilog', 'time');\nNote: Timescale transparently manages storage for hyper table and PostgreSQL Developer can continue to use standard SQL/plpgsql with it.\nFor the sake of quick testing, One can add dummy data to this table using below SQL,\ninsert into apilog SELECT (current_timestamp - \u0026#39;0 day\u0026#39;::interval), (case when x = 1 then \u0026#39;finance\u0026#39; else \u0026#39;it\u0026#39; end),(case when x = 1 then \u0026#39;getPrices\u0026#39; else \u0026#39;getUptime\u0026#39; end), (case when x \u0026lt; 2 then \u0026#39;success\u0026#39; else \u0026#39;failure\u0026#39; end), (current_timestamp - \u0026#39;0 day\u0026#39;::interval) + trunc(random() * 20) * \u0026#39;1 second\u0026#39;::interval FROM generate_series(0, 5000, 5) AS t(x); Currently, Application generates log events in OLTP Database and data from this database is replicated to Reporting database. Since we have created new Hyper table to host this data,a simple approach of Trigger can be used to populate it from current table.\nIn real scenario, you may want to consider replicating the data directly to hyper table.\nFluentBit # So far , we have collected Application logs in the database. There is one more source which is of importance in the context of Monitoring and that is infrastructure software. It could be Operating System, Web Servers and so on. They generate lot of logs and metrices that can be ingested and consumed in conjuction with Application log to get better picture. We will look at how Web server logs can be sourced in data source.\nThere are many monitoring tools (refer Useful links below for comparison) available with focus on IT and Network monitoring. Such tools readily include infrastructure software too. For the sake of this article, We can use Log collector tool for this purpose. As such there are many log collector tools available, we will use Fluentbit.At a very High level, It has concepts of,\nInput - Log sources Parsers - Plugins to parse \u0026amp; transform the logs Output - Log Destination like Prometheus, Kafka, PostgreSQL and so on. Some of the advantages of Fluentbit are,\nHigh log delivery performance with efficient resource utilization Robut and Lightweight approach Log enrichment at Node level itself than on the destination Simpler configuration format Setup FluentBit - Fluentbit provides binaries that are bundled with package managers in case of Linux and as installers for Windows.\nAs of writing of this post, Pre-built binaries do not include output plugin for PostgreSQL. So Fluentbit has to be built from source after modifying Cmakelist so,\nClone the github repository\nModify CMakeLists.txt file as below,\noption(FLB_OUT_PGSQL \u0026quot;Enable PostgreSQL output plugin\u0026quot; No)\nto\noption(FLB_OUT_PGSQL \u0026quot;Enable PostgreSQL output plugin\u0026quot; Yes)\nRefer to Compiling from Source for further details.\nConfiguration - Once fluentbit is installed, It needs to be configured to read Web server logs , parse them and push them to PostgreSQL.\nBelow is sample configuration to periodically read Web Server Logs (in w3c log format), parse and push them to PostgreSQL,\n[SERVICE] Flush 5 Daemon Off Log_Level debug Log_File d:\\monitoring\\fluentbit.log Parsers_File parsers.conf Parsers_File generated/parsers_generated.conf HTTP_Server On HTTP_Listen 0.0.0.0 HTTP_Port 2020 [INPUT] Name tail Tag format.iis Parser dips-w3c path d:\\temp\\iis.log DB d:\\temp\\raw.iis.db [OUTPUT] Name pgsql Match * Host 172.0.0.1 Port 5432 User fluentbit Password fluentbit Database timescalepoc Table iislogs Timestamp_Key time Configuration for Parser is as below,\n[PARSER] Name dips-w3c Format regex Regex ^(?\u0026lt;time\u0026gt;\\d{4}-\\d{2}-\\d{2} \\d{2}[\\:\\.]\\d{2}[\\:\\.]\\d{2}) (?\u0026lt;serverip\u0026gt;\\S+) (?\u0026lt;method\u0026gt;\\S+) (?\u0026lt;uristem\u0026gt;\\S+) (?\u0026lt;uriquery\u0026gt;\\S+) (?\u0026lt;serverport\u0026gt;\\S+) (?\u0026lt;username\u0026gt;\\S+) (?\u0026lt;clientip\u0026gt;\\S+) (?\u0026lt;userAgent\u0026gt;\\S+) (?\u0026lt;referrer\u0026gt;\\S+) (?\u0026lt;status\u0026gt;\\S+) (?\u0026lt;substatus\u0026gt;\\S+) (?\u0026lt;win32status\u0026gt;\\S+) (?\u0026lt;timetaken\u0026gt;\\S+) (?\u0026lt;useragent1\u0026gt;\\S+) (?\u0026lt;auth\u0026gt;\\S+) (?\u0026lt;contenttype\u0026gt;\\S+) Time_Key time Time_Format %F %T Time_Keep True types serverPort:integer httpStatus:integer httpSubStatus:integer win32Status:integer timetaken:integer This parser basically uses Regular Expression to parse each line in log file into key - value pairs with data points of interest.\nIn terms of output, Fluentbit\u0026rsquo;s Postgresql plugin provisions the table itself with a structure that stores entire JSON in field as part of row. Either this table can be used as is or use \u0026ldquo;Before insert\u0026rdquo; trigger as suggested by Fluentbit\u0026rsquo;s manual to parse the Json and populate separate table.\nFluentbit can be easily configured to run as daemon (on Linux) or Service (on windows).\nVisualization # With data getting added to timescaledb Hyper table,Lets see how it can be visualized.\nTypically, there are 2 approaches to be considered for Visualization,\nCustom-built Web UI - This only makes sense if,\nThere is already a Reporting/Visualization Web UI in place and adding new dashboards/reports is not much pain Not much customization and/or slicing-dicing is expected. Limited Efforts available. Off the shelf Tools - This approach makes sense if,\nIt is expected that Monitoring dashboards should be flexible and provide ease of customization by business or power users.\nAdditional dashboards are expected or can be provisioned with minimal or no coding.\nThere are many paid and open source tools available. Notable OSS options are,\nGrafana - Tailor made for Monitoring and extensive analysis of Time series data. Apache Superset - open-source application for data exploration and data visualization able to handle data at petabyte scale. Lets see how Grafana can be used for visualization (Probably, i may evaluate superset some time and update this post.)\nGrafana # Grafana has multiple offerings and one of them being Open source, Self-hosted Application. It has Go backend and is very easy to install. For Windows, Just follow the steps at Installation.\nOnce grafana is setup, one can quickly start it by running grafana-server. By default, it starts Web server at port 3000. With Grafana Web-based GUI up and running, lets perform below steps to get dashboard in place.\nConnectivity to PostgreSQL - One needs to add Data Source in Grafana which in this case is PostgreSQL Database. It can be added from sidebar on Grafana UI, by hovering over \u0026ldquo;Configuration\u0026rdquo; option. In below screenshot, it shows configuration. Grafana: Connect to PostgreSQL Add Dashboard - Once the Data source is setup, next step is to add a dashboard. Dashboard essentially is a visualization or a report. It has Query (in this case SQL Query) to fetch the data. Below screenshot shows configuration of simple query for Dashboard, Grafana: Query for Dashboard Grafana requires certain functions to be included (like $__time(..) and $__timeFilter(..)) in query so as to facilitate filtering/ordering by user through UI, like shown below,\nGrafana: View data and apply Filter Grafana provides extensive ways to transform on the data fetched by SQL Query. This feature is more aimed at business and power user who may want to perform additional analysis on it. Alternative is to provide desired SQL and get the visualization like Time series or Graph as shown below,\nGrafana: Complex SQL Query with minimal transformation Grafana: Time Series Visualization Note that there are many more features provided by Grafana (in terms of transformations, Visualization options, Access Control to UI itself and so on.\nKey points with this approach are,\nLeveraging tools/products currently in use. Greater Flexibility in Visualization over custom built tool containing canned reports/graphs Lesser learning curve than inducting new tools. This post barely touches surface of what each of the individual tools mentioned have on offer, one would do well to go through their documentation to derive most value out of it.\nUseful links (#usefullinks) # Comparison of IT Monitoring tools Nice Introduction to Modern Observability Happy Coding !!\n"},{"id":11,"href":"/posts/apiupgrade/","title":"Upgrading API: Learnings","section":"Posts","content":" Introduction # One of the design considerations stressed upon by Jeffrey richter about APIs (Read more here) is that \u0026ldquo;API is expected to be stable over long period of time\u0026rdquo;. Recently,for a .NET based project, we decided to upgrade some of the ASMX (legacy SOAP based approach) based APIs and were immediately reminded by Customer(s) to avoid any kind of impact on existing users.\nThis means that upgrade must be done keeping in mind,\nNo changes to API Contract (SOAP remains SOAP and so on) No changes to URLs Testing to ensure no impact Initial plan was to move away from SOAP to adopt REST based approach. This thinking was aided by fact that .NET core may not support WCF (framework that supports SOAP apart from ASMX Web Services) in addition to other aspects like simplicity and wide adoption of REST. However, even microsoft has now decided to support WCF in .NET Core via CoreWCF.\nWith this constraints, below alternatives were considered to upgrade ASMX based services to WCF (the only other framework that supports SOAP based services),\nApproach Description Have existing ASMX Service call new WCF Service using Async/Await This involves hosting additional WCF Service and making HTTP requests to it. It also means maintaining both ASMX \u0026amp; WCF endpoints. Also to be mindful of latency introduced due to HTTP communication between the two. New WCF Service and URL Rewrite rules to handle requests to ASMX This involves developing new WCF Service, compatible to current service contract, and configuration to route/re-write incoming requests to new service. Existing ASMX end point can be sunset New WCF Service and mapping .asmx handler to WCF handler This involves developing new WCF Service,compatible to current service contract, and configuration so that requests to .asmx url will be served by WCF handler. Existing ASMX end point can be sunset. Lets go through above approaches in detail.\nWCF service invoked from ASMX asynchronously # This involves developing new WCF Service. Existing ASMX based web service will be modified to invoke new WCF Service. Asynchronously invocation should help in this case since whole operation is I/O bound (Asynchrony is a way to get concurrency without multithreading. E.g., freeing up the calling thread instead of blocking it while an I/O operation is in progress Stephen Cleary). Since ASMX is legacy framework and only support Event-based asynchronous pattern (EAP), it is necessary to combine EAP with Task based asynchronous pattern (TAP) which is what async/await uses. Below is sample snippet,\nprivate async Task\u0026lt;string\u0026gt; FooAsync(int arg) { using (var resp = await client.GetAsync(string.Format(\u0026#34;https://jsonplaceholder.typicode.com/todos/{0}\u0026#34;, arg)).ConfigureAwait(false)) { resp.EnsureSuccessStatusCode(); using (var contentStream = await resp.Content.ReadAsStreamAsync().ConfigureAwait(false)) { APIResponse obj = await JsonSerializer.DeserializeAsync\u0026lt;APIResponse\u0026gt;(contentStream).ConfigureAwait(false); string output = string.Format(\u0026#34;{0} at {1}\u0026#34;, obj.Title, DateTime.Now.Ticks); System.Diagnostics.Debug.WriteLine(output); return output; } } } [WebMethod] public IAsyncResult BeginFoo(int arg, AsyncCallback callback, object state) { return FooAsync(arg).ToApm(callback, state); } [WebMethod] public string EndFoo(IAsyncResult result) { try { return ((Task\u0026lt;string\u0026gt;)result).Result; } catch (AggregateException ae) { throw ae.InnerException; } } Where ToApm is extension function from Stephen Toub\u0026rsquo;s excellent blog (link in code as comment),\npublic static Task\u0026lt;TResult\u0026gt; ToApm\u0026lt;TResult\u0026gt;(this Task\u0026lt;TResult\u0026gt; task, AsyncCallback callback, object state) { if (task.AsyncState == state) { if (callback != null) { task.ContinueWith(delegate { callback(task); }, CancellationToken.None, TaskContinuationOptions.None, TaskScheduler.Default); } return task; } var tcs = new TaskCompletionSource\u0026lt;TResult\u0026gt;(state); task.ContinueWith(delegate { if (task.IsFaulted) tcs.TrySetException(task.Exception.InnerExceptions); else if (task.IsCanceled) tcs.TrySetCanceled(); else tcs.TrySetResult(task.Result); if (callback != null) callback(tcs.Task); }, CancellationToken.None, TaskContinuationOptions.None, TaskScheduler.Default); return tcs.Task; } This approach involves,\nhosting and maintaining both (current and new) API end-points. We also came across issues where async/await was not working properly in case code blocks. Measuring and mitigating any latency induced due to this additional hop Additional Monitoring and logging to track WCF end-point We decided to explore alternative approaches instead of this.\nWCF service with URL re-write # This requires hosting WCF Service which is backward compatible with ASMX based SOAP implementation.\nTypically this involves,\nsupporting basicHttpBinding Adding namespaces and support for XML Serialization to Service contract like, [ServiceContract(Name = \u0026#34;RequestReplyService\u0026#34;, Namespace = \u0026#34;http://tempuri.org/\u0026#34;),XmlSerializerFormat] Adding Action to OperationContract attribute like, [OperationContract(IsOneWay = false, Action = \u0026#34;http://tempuri.org/DoWork\u0026#34;)] Additional configuration to re-write incoming requests to .asmx to new service in web.config as below,\n\u0026lt;system.webServer\u0026gt; \u0026lt;validation validateIntegratedModeConfiguration=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;rewrite\u0026gt; \u0026lt;rules\u0026gt; \u0026lt;rule name=\u0026#34;asmxtosvc\u0026#34; stopProcessing=\u0026#34;true\u0026#34;\u0026gt; \u0026lt;match url=\u0026#34;^service.asmx(.*)$\u0026#34; /\u0026gt; \u0026lt;action type=\u0026#34;Rewrite\u0026#34; url=\u0026#34;Service.svc{R:1}\u0026#34;/\u0026gt; \u0026lt;/rule\u0026gt; \u0026lt;/rules\u0026gt; \u0026lt;/rewrite\u0026gt; \u0026lt;/system.webServer\u0026gt; One may want to test above re-write settings in IIS as older versions of it require installation of URL Rewrite module.\nThis is followed by testing new WCF service from existing client(s), be it .NET based clients or other ones with no changes. .NET based clients typically invoke service through generated proxy class. For other clients, we basically simulated it via Postman.\nThis approach provides cleaner implementation vis-a-vis earlier approach such that it is still new WCF based implementation with no ASMX in use.\nWCF service with .asmx extension mapped to WCF handler # This approach is very similar to last one with only change being instead of using URL re-write module, we will map .asmx extension to WCF Handler. So the changes are only in web.config as below,\n\u0026lt;system.web\u0026gt; \u0026lt;httpHandlers\u0026gt; \u0026lt;remove path=\u0026#34;.asmx\u0026#34; verb=\u0026#34;*\u0026#34; /\u0026gt; \u0026lt;add path=\u0026#34;*.asmx\u0026#34; verb=\u0026#34;*\u0026#34; type=\u0026#34;System.ServiceModel.Activation.HttpHandler, System.ServiceModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\u0026#34; validate=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;/httpHandlers\u0026gt; \u0026lt;compilation debug=\u0026#34;true\u0026#34; targetFramework=\u0026#34;4.8\u0026#34;\u0026gt; \u0026lt;buildProviders\u0026gt; \u0026lt;remove extension=\u0026#34;.asmx\u0026#34;/\u0026gt; \u0026lt;add extension=\u0026#34;.asmx\u0026#34; type=\u0026#34;System.ServiceModel.Activation.ServiceBuildProvider, System.ServiceModel, Version=4.0.0.0, Culture=neutral, PublicKeyToken=b77a5c561934e089\u0026#34;/\u0026gt; \u0026lt;/buildProviders\u0026gt; \u0026lt;/compilation\u0026gt; \u0026lt;httpRuntime targetFramework=\u0026#34;4.8\u0026#34;/\u0026gt; \u0026lt;/system.web\u0026gt; .... \u0026lt;system.webServer\u0026gt; \u0026lt;handlers\u0026gt; \u0026lt;remove name=\u0026#34;WebServiceHandlerFactory-Integrated\u0026#34;/\u0026gt; \u0026lt;add name=\u0026#34;MyNewAsmxHandler\u0026#34; path=\u0026#34;*.asmx\u0026#34; verb=\u0026#34;*\u0026#34; type=\u0026#34;System.ServiceModel.Activation.HttpHandler, System.ServiceModel.Activation, Version=4.0.0.0, Culture=neutral, PublicKeyToken=31bf3856ad364e35\u0026#34; /\u0026gt; \u0026lt;/handlers\u0026gt; \u0026lt;validation validateIntegratedModeConfiguration=\u0026#34;false\u0026#34; /\u0026gt; \u0026lt;/system.webServer\u0026gt; This was tested in same way as earlier with existing .NET and other clients.\nThis feels like even more cleaner approach than using URL re-write as it doesn\u0026rsquo;t involve using any additional module/library.\nFinally, we went ahead with this approach.\nHopefully,this article will be helpful to anyone involved in legacy modernization initiatives.\n[Update on 21-May-2021]\nASMX supports both SOAP as well as Form POST (i.e. content type application/x-www-form-urlencoded). This implies that there would be consumers of this API who are using either of the two formats to interact with API. Hence, it is necessary that new WCF based API supports both the formats. One way (If you are aware of any other approach, do let me know via comments) is to, Expose both SOAP and HTTP End-points like below,\n\u0026lt;service name=\u0026#34;wcf.Myservice\u0026#34;\u0026gt; \u0026lt;endpoint address=\u0026#34;\u0026#34; binding=\u0026#34;basicHttpBinding\u0026#34; contract=\u0026#34;wcf.IMyserviceSoap\u0026#34; /\u0026gt; \u0026lt;endpoint address=\u0026#34;http\u0026#34; kind=\u0026#34;webHttpEndpoint\u0026#34; endpointConfiguration=\u0026#34;webEndpointWithHelp\u0026#34; contract=\u0026#34;wcf.IMyservice\u0026#34; /\u0026gt; \u0026lt;endpoint address=\u0026#34;mex\u0026#34; binding=\u0026#34;mexHttpBinding\u0026#34; contract=\u0026#34;IMetadataExchange\u0026#34; /\u0026gt; \u0026lt;/service\u0026gt; This exposes SOAP end point at root (/) and HTTP end-point at (/http).\nSince clients are not aware of this new http end point, additional steps are needed to handle non soap requests seamlessly. This can be done in Global.asax as below,\nprotected void Application_BeginRequest(object sender, EventArgs e) { const string httpAddress = \u0026#34;http/\u0026#34;; if (Request.HttpMethod.ToLowerInvariant() == \u0026#34;post\u0026#34;) { if (!Request.ContentType.ToLowerInvariant().Contains(\u0026#34;xml\u0026#34;) \u0026amp;\u0026amp; !Request.Url.AbsolutePath.ToLowerInvariant().Contains(httpAddress)) { List\u0026lt;string\u0026gt; segments = Request.Url.Segments.ToList(); segments.Insert(segments.Count() - 1, httpAddress); var redirPath = String.Join(\u0026#34;\u0026#34;,segments.ToArray()); Context.RewritePath(redirPath); } } } Above function, injects http in path based on Content-type of incoming request and then re-writes it.\nIdeally, i would have liked to do it via URL Rewrite module in web.config. However, i faced issues while setting up the rule that uses Content-type header.\nHowever, this approach still had issues wherein WCF run-time raised errors when ?singlewsdl url was accessed. It seems problem was due to multiple interfaces (one for SOAP and other for REST) and WCF not being able to generate WSDL for it. Additionally, REST handler is also deserves a look as it simply parses payload as Query String and populating properties of request DTO/class has to be done manually,\nResponseDTO IMyservice.Process(Stream input) { string body = new StreamReader(input).ReadToEnd(); NameValueCollection nvc = HttpUtility.ParseQueryString(body); return new ResponseDTO() { cnField = string.Format(\u0026#34;NVCol --\u0026gt; {0}|{1}\u0026#34;, nvc[\u0026#34;prop1\u0026#34;], nvc[\u0026#34;prop2\u0026#34;]) }; } Overall, WCF does not have great support for handling FORM POST requests. Hence, other alternative is to have ASP.NET MVC Web API handle the post requests. This approach is detailed here, check it out. Additionally, it takes changes to BeginRequest in global.asax to re-write incoming request so that Web API controller can process it, like below,\nprotected void Application_BeginRequest(object sender, EventArgs e) { if (Request.HttpMethod.ToLowerInvariant() == \u0026#34;post\u0026#34;) { if (!Request.ContentType.ToLowerInvariant().Contains(\u0026#34;xml\u0026#34;)) { List\u0026lt;string\u0026gt; segments = Request.Url.Segments.ToList(); Context.RewritePath(string.Format(\u0026#34;/controllers/{0}\u0026#34;,segments[segments.Count()-1])); } } } ASMX and SOAP 1.1 - It was noticed that though ASMX supports SOAP 1.1, it doesn\u0026rsquo;t enforces it when it comes to \u0026ldquo;SOAPAction\u0026rdquo; Header. As per the SOAP 1.1 specification, \u0026ldquo;SOAPAction\u0026rdquo; Http Header is mandatory and is used to determineWebmethod to be invoked. Since WCF is compliant with SOAP 1.1 specification, it required additional step to infer Webmethod by means of parsing the body. Luckily, Microsoft has sample for Dispatch by Body Element and same can be readily used. Overall, WCF Samples is fantastic set of samples that covers wide variety of such scenarios. Do Check it out.\nUseful References # Comparing ASMX web services to WCF APM Pattern using Tasks Async in WCF Comparing ASMX with WCF Discussion on ASMX to WCF Happy Coding !!\n"},{"id":12,"href":"/posts/presto/","title":"Presto - A distributed SQL Engine for variety of data stores","section":"Posts","content":" Introduction # In a company/enterprise, typically there are multiple sources of data. This could be result of M\u0026amp;A (where each of those add in a new data store) or result of multi year process of using data stores that are in vogue at that time. Result is combination of various types of relational databases, flat file systems, queues and so on. This results in Data Silos. This scenario is typically observed in companies who are running workloads On-prem (i.e. Pre-cloud, Companies who started on Cloud or have moved to it, typically tend to organize data platform better. This could be because of ease of migrating data on cloud. Typically, they centralize it around cheaper object storage (say AWS S3)).\nCompany will want to utilize this data, accumulated over the years, for business intelligence, machine learning purposes. Usually, it would require querying efficiently across these data sources or first collecting all the data in central location (say Data Lake or Operational Data store) and then querying on it.\nOverall, below are the widely adopted approaches,\nData warehouse with ETL Approach - This involves extracting data from Transactional systems (OLTP), ERP, Events store and so on. Transforming the same and then loading it into Data warehouse which is typically a store used for Analytics. Whole process is orchestrated as workflow using ETL Tools.\nLakehouse - Many companies have two different types of storate: Data Lake and Data warehouse. The data warehouse handles offline analytics, such as BI dashboards and reports, that describe what has or is happening in a business. The data lake is store for raw data (including unstructured). Instead of ETL (Extract - Transform - Load), ELT (Extract - Load - Transform) approach is followed where data from transactional system is loaded into Data Lake. Later, it is transformed/processed for analytics purposes and loaded in data warehouse. Alternatively, there is a trend where data lake itself is used for trend and/or predictive analytics. Data lake is usually based on cheaper, object storage with data stored using open formats (like Parquet , ORC etc.) favouring columnar approach. Columnar store is typically favoured for analytics over relational one.\nAs explained in Emerging Architecture for Data Infrastructure,\nData warehouse is used for analytics use cases i.e. help business make better decisions. Data lake is used for operational use cases. All the above approaches typically assume rather large volume of data being handled. Then what can be approach for companies who are having moderate amount of data (few terabytes) and still want to derive actionable insights from it. Such companies are unlikely to have big data systems like HDFC in place.\nFor these cases, One may consider Presto a.k.a. Trino. At it\u0026rsquo;s core, Presto translates SQL queries (it supports ANSI SQL) into whatever query language is necessary to access the underlying storage medium. Storage medium could be a Elasticsearch cluster, or a set of Parquet files on HDFS, or a relational database.\nPresto uses MPP (Massively parallel processing) architecture in which it has,\nCoordinator node - responsible for creating query plan and distributing the work among workers. Worker node(s) - they push down predicates to those data sources. Only the data necessary to compute join is retrieved. All workers operate in parallel mode. Presto provides many connectors like below (but not limited to),\nRelational Databases MySQL PostGres SQL Server Non-relational Databases Mongodb Redis Cassandra Columnar file formats like ORC, Parquet and Avro  stored on: Amazon S3 Google Cloud Store Azure Blog Store HDFS Clustered file systems It\u0026rsquo;s important to note that Presto does not write intermidiate results to disk, Hence worker nodes are expected to be optimized for processing and memory over storage. A Single Presto query can combine data from multiple sources. Most importantly, Presto can work without Hadoop. Presto has cost-based optimizer which means query plan takes into account the time cost associated with executing that plan and can therefore do various types of optimizations around join ordering and the sequence with which you execute that query plan to deliver the fastest level of performance.\nBelow is apt representation of how Presto works (Ref: prestodb.io)\nWhere Presto fits Typical Presto Deployment Typical use cases for Presto are,\nAd-hoc, Interactive Analytics Batch ETL processing. Centralized Data Access with Query Federation From the consumption perspective, Presto Offers CLI as well as JDBC Driver. However, there are many language specific clients available from Community.\nKey points to note while considering Presto,\nNo need for complex ETL/ELT processes and related Monitoring. No need to provision for specialized data store for Data Lake and/or Data warehouse. However, this may not hold true if Query results from Presto are required to persisted. Although, overall efforts and cost will much lower. This would also mean that existing data stores need to maintain historical data too Any specific use cases not suitable for Presto will have to be alternatively approached. Some of the points to explore further would be ,\nGiven that Presto does not use storage on its own, how can one perform Capacity planning given the expected workflow ? How are failures handled? Useful References/Interesting Links, # Trino PostgreSQL protocol gateway for Presto distributed SQL query engine Happy Coding !!\n"},{"id":13,"href":"/posts/elt/","title":"ELT approach for Data Pipelines","section":"Posts","content":" Introduction # While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,\nExtract - typically involves retrieving data from source. This could also be via streaming Transform - Apply transformation to the extracted data. Load - Loading the data in Operation Data store (ODS) or data warehouse Refer here for more details on ETL. ETL has been made easy by tools like Talend, SSIS and so on. However, there has been shift from above approach due to,\nNeed to handle different kinds of data (Structured and Unstructured) hugh volumes of data (IOT, Customer data management) Availability of cheaper storage and compute along with availability of internet scale cloud based data warehouses has recently caused wide adoption of ELT (Extract-transform-load) over ETL.\nELT offers an alternative to ETL in which data is loaded into the warehouse (sometimes in storage area called as data lake) before transforming it. It allows focussing on extraction and loading with heavy transformation offloaded to later stage. Since the transformation happens in the warehouse, it can potentially be defined using SQL (thus using same language across the pipeline). This allows more roles (say Data Analysts) to contribute to (or entirely own) the transformation logic. Data warehouse becomes single source of truth for data. Ref: ETL vs ELT\nTypically, Data flow pipeline consists of below phases (it also lists available tools for each phase),\nIngestion - Airbyte, Fivetran, Stitch Warehousing - Snowflake, BigQuery, Redshift, PostgreSQL Transformation - dbt Orchestration - Airflow, Prefect, Dagster BI - Superset, Metabase, Redash, Looker etc. I think the best way to understand the landscape is to use above tools. So i decided to implement below problem statement. The requirement is to run a weekly process that,\nDownloads list of CNX 500 companies from Exchange\u0026rsquo;s web site For each of the company , get Last traded price(ltp) and 52 week high price (yearlyhigh) Exclude companies having ltp \u0026lt; 20 or ltp \u0026gt; 50000 Rank companies by closeness of ltp to yearlyhigh Prepare buy list of up to 20 such companies. Earlier short listed stocks, which are not in top 20 this week or further than 5% from their yearlyhigh, should be marked for sell. Above is hypothetical example and using full fledged data stack may be overkill but should suffice the purpose of this article.\nE \u0026amp; L in ELT - Get the list of CNX 500 Companies and also get stock price for each of them # Below are some of the options available for this task under extract and load category,\nUse Python to download list of stocks and then use yfinance to get the price and yearly high. Use tool like Airbyte which provides declarative way of importing the data via HTTP. I am planning to explore this option later. Use Go to perform the task. I decided to go with this one and code is available at here. It downloads CSV file from Exchange\u0026rsquo;s website (containing list of stocks in Index) and loads them to database. Since Yahoo finance no longer provides Free tier for API, It uses htmlquery library to parse HTML and retrieve stock price and yearly high value. T in ELT - Transform the company-wise data to arrive at weekly list of momentum stocks # This is implemented using dbt. dbt (Data Build Tool) is a framework to facilitate transformations using SQL along with version control, automates tests, support for incremental load, snapshots and so on. It has notion of project or workspace that many developers are familiar with. It is offered as Command line interface (CLI) as well as on cloud which also provides web based UI. I have used CLI for this exercise. For a quick recap of dbt folder structure, refer [here]https://towardsdatascience.com/data-stacks-for-fun-nonprofit-part-ii-d375d824abf3).\nSource code of dbt project here. We will go through key part of this project which are Models that carry out the transformation. After the initial setup of dbt like configuring target (i.e. data source which in this case is a PostgreSQL database), below are Models used,\nSince Loading of company-wise data is already done in earlier step, next step is to rank the companies w.r.t. closeness to their yearly high. Below is dbt SQL which does it (At run time, dbt converts below SQL to the one understood by the Target database),\n``` {{ config( materialized='incremental', ) }} with cnxcompanies as ( select symbol, company, ltp, yearlyhigh, updatedat, rank() over (order by yearlyhigh-ltp) as diff_rank from {{ source('datastore', 'cnx500companies') }} where yearlyhigh::money::numeric::float8 - ltp::money::numeric::float8 \u0026gt; 0 and ltp::money::numeric::float8 \u0026gt; 20 and ltp::money::numeric::float8 \u0026lt; 50000 ), cnxtopstocks as ( select symbol, company, ltp, yearlyhigh, updatedat, diff_rank from cnxcompanies order by updatedat desc,diff_rank ) select * from cnxtopstocks ``` Above model creates corresponding table in database (as such dbt abstracts changes to database from developer and manages it on its own). Note that model is marked incremental so that it doesn\u0026rsquo;t overwrite the table on every run but rather incrementally applies changes.\nNext step is to arrive at Weekly list of stocks to buy and even sell those which are lacking momentum.\n``` {{ config( materialized='incremental', unique_key='concat(symbol,updatedat)' ) }} with currentlist as ( select distinct symbol, company, ltp, yearlyhigh, updatedat,diff_rank,'buy' as buyorsell from {{ref('rankstocks')}} where (yearlyhigh-ltp)/ltp*100 \u0026lt;= 5 order by updatedat desc, diff_rank limit 20 ), finallist as ( {% if is_incremental() %} select symbol, company, ltp, yearlyhigh, updatedat,diff_rank,'sell' as buyorsell from {{this}} as oldlist where not exists (select symbol from currentlist where symbol=oldlist.symbol and (yearlyhigh-ltp)/ltp*100 \u0026lt;= 5 ) union select symbol, company, ltp, yearlyhigh, updatedat,diff_rank,'buy' as buyorsell from currentlist where not exists (select symbol from {{this}} where symbol=currentlist.symbol and buyorsell='buy') {% else %} select * from currentlist {% endif %} ) select * from finallist ``` This model refers to earlier one using {{..}} jinja directive. It also refers to itself using {{this}} directive.\nAmong others, below are key feature of DBT that were observed,\nConcept of Project/Workspace which programmers are typically familiar with Using SQL for Data Transformation Support for Version control Support for testing Support for incremental load Support for snapshots Automatic schema updates Out of the box Documentation browser covering traceability across sources and models. Orchestration # After completing ELT aspects, now it\u0026rsquo;s time to orchestrate this pipeline wherein the whole process will run every week. Typically, one can use task scheduler like Airflow or Prefect to do this. But for the purpose of this article, lets use at on windows (or cron if you are using Linux).\nso a simplest possible batch file (as below),\nset http_proxy= set https_proxy= .\\gover\\go run . .\\.venv\\scripts\\activate \u0026amp; .\\dbt\\dbt run will run the whole process and generate weekly list in weeklylist table in database. This batch file can be scheduled to run on weekly basis using command at 23:00 /every:F runscript.bat.\nThis is very basic approach to scheduling (with no error handling/retries or monitoring). Hopefully, i will be able to work on these part (something like this). Till then\u0026hellip;\nUseful References # Reverse ETL Data stacks for Fun and Profit What warehouse to use Build Data Lake in PostgreSQL using FDW, Singer, Metabase Happy Coding !!\n"},{"id":14,"href":"/posts/restapiversioning/","title":"Learnings from Jeff Richter's Designing and Versioning HTTP REST APIs Video Course","section":"Posts","content":" Background # Recently, i went through excellent video series on Designing \u0026amp; Versioning HTTP_REST APIs presented by Jeffrey Richter. It is available here. In the past, i had read Jeff\u0026rsquo;s books on CLR and found his writing to be very clear and understandable. So is my experience with this Video Series. Below is summary of learnings from this Video Series. I do not claim that every aspect is covered here so please do check out the videos.\nI have been developing REST APIs for many years but the video series opened up many new aspects that were previously unknown. Jeff starts with Need to Good API Design and related considerations, REST Fundamentals and then dives deeper into aspects like idempotent behavior, versioning, ETags and so on.\nJeff covers REST fundamentals, need for thoughtful API design as it might be difficult to amend it later followed by practices covering Naming,Need for Idempotency, Error Handling and so on. Below is an attempt to summarize aspects from these videos.\nREST Fundamentals # REST is an architectural style with emphasis on,\nScalability Reduced latency via Caching Independent Deployment Encapsulating legacy Systems A REST service has resources where they represent state but not behavior. These behaviors are CRUD (Created, Read, Update and Delete). All operations of service must be idempotent.\nURL of the REST Service is expected to be stable over long period of time. URL (apart from HTTP scheme and host name:port), consists of\nDocument (eg. song-management) - sometimes omitted in which case the host determines document. Collection resource (users or playlists) - Hold items; use plural lowercase noun; avoid more than 2 collections. Item resource - Request/Response body holds the item\u0026rsquo;s state Method - Prefer \u0026lsquo;PATCH\u0026rsquo;, with JSON Merge Patch request body, over \u0026lsquo;PUT\u0026rsquo;. \u0026lsquo;PUT\u0026rsquo; for whole creation or replacement but may introduce issues between different versions of service. Avoid \u0026lsquo;POST\u0026rsquo; as it involves challenges in ensuring Idempotent behavior. The argument against \u0026lsquo;POST\u0026rsquo; is that it always creates resource and returns identifier which may get lost and client may retry. . Error Handling # Videos contain tables explaining HTTP status code to be returned along with body in different conditions. below is quick summary,\nIf something unexpected happens return Status 500\nIf service is booting, too busy or shutting down then return Status 303\nIf HTTP Version not 1.1 then return status 505\nIf Authorization fails then return status 401\nIf Client makes too many requests/second then return status 429\nIf URL too long then return status 414\nIf HTTP Method not supported at all then return status 501\nIf resource not accessible to client then return status 403\nIf No support for HTTP Method not 1.1 then return status 405\nIf request not in acceptable format then return status 406\nIn case service returns non-success/error response for a request,\nIt is recommended to add header in response that indicates the same (that way client can inspect it before deserializing/inspecting the response).\nAlso if error is recoverable @ runtime then, string is specific else string is list of similar errors. Response body could be in JSON format as,\n{ \u0026#34;error\u0026#34;: { \u0026#34;code\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;message\u0026#34;:\u0026#34;\u0026#34;, \u0026#34;target\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;innererror\u0026#34;: { \u0026#34;code\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;minLength\u0026#34;: 6 } } } If server is overloaded then return 503\nIf tenant exceeds quota then return 429\nVersioning # New version must be backward Compatible\nExamples of versioning in API,\nhttp://api.contoso.com/v1.0/products http://api.contoso.com/products?api-version=1.0 http://api.contoso.com/products?api-version=2021-01-01 Add new API when changing mandatory parameters, payload formats, error codes or behavior\nApproach to API Versioning should not be afterthought\nConsider embedding version number in the data structure\nChecklist for REST APIs # Focus on great and consistent naming - This is very important because once in production, this is unlikely to change.\nEnsure that resource path make sense\nAlways try to simplify call to Service (by having fewer query parameters \u0026amp; JSON fields)\nUse\nPUT to Create/Replace whole resources. Last write wins. It should return 200-Ok, 201-Created.\nPATCH to Create/Modify resource with JSON Merge Patch. It should return 200-Ok, 201-Created.\nGET to Get the resource.It should return 200-Ok.\nDELETE to remove resource. It should return 200-Ok, 204-No content but 404-not found should be avoided.\nJeff recommends avoiding usage of POST unless request is idempotent (HTTP does not require it to be idempotent).For API having POST operation, below Idempotency Pattern can be considered,\nClient: Creates ID Client: sends request to server with generated ID (this can be retried) Server: If ID is not in log then, do operation \u0026amp; log ID (This should be part of transaction); respond with OK (Server periodically deletes old log to avoid unbounded growth) A URL should be stable/immutable\nUse proper response codes to enable customers to self-fix\nHave clear contracts for string values\nShare samples (Code) that really use your API\nUse Etag (Entity Tag) to identify the version of the resource,\nEtag is usually computed as checksum or as sequence value (which is incremented on every change) for single item response, it is set in header and for collections, it is added as field in body. Caching - Clients can use it for resource caching (send GET Request with ETag and server responds with 304-Not modified if resource hasn\u0026rsquo;t changed) Concurrent/Conditional Updates -Etag along with \u0026lsquo;if-none-match\u0026rsquo;/\u0026lsquo;if-match\u0026rsquo; headers allows conditional update/delete Services must fail fast if requests are greater than quota (requests/time)\nEvery request must be assigned unique ID for logging /tracing purposes. this ID can be returned in header of response.\nGenerate Telemetry for the Service. It should include User Agent information for diagnostic purposes. Also consider adding Distributed tracing (OpenTelemetry is standardization initiative in this regard).\nIn case of client retries, services must be idempotent (Idempotency indicates retrying a request has same intended effect, even if the original request succeeded, though response might differ)\nService must remain fault-tolerant in case of failures.\nTypically REST is meant for State transfer/CRUD Operations but many times the purpose of end point is to offer action. In such cases specify the action being performed at the end , i.e. after establishing the exact resource, of URL like, /user-management/users/{userid}/:send-sms. In this,\n\u0026lsquo;user-management\u0026rsquo; indicates host \u0026lsquo;users\u0026rsquo; indicates users collection \u0026lsquo;{userid}\u0026rsquo; is to identify user by ID \u0026lsquo;:send-sms\u0026rsquo; indicates action (prefixed with \u0026lsquo;:\u0026rsquo;) to be performed. Use tools like Swagger for API documentation and to create language-specific client libraries\nReviewing REST APIs # While reviewing HTTP REST APis, below aspects should be evaluated,\nDoes the API Match Customer\u0026rsquo;s Expectation? Aspects to check are,\nURLs idempotency atomicity json casing status codes paging long running operations Consistency with other Services\nIs the Service/API sustainable over time i.e. API must be able to grow/version over time without breaking customer apps\nIn no way, the above covers everything available in the Video series. So do check it out here.\nUseful References # Making retries safe with Idempotent APIs Happy API designing !!\n"},{"id":15,"href":"/posts/resiliencytoxiproxy/","title":"Resiliency Testing with Toxiproxy","section":"Posts","content":" Background # In a typical workflow of software development, Developer implements a Unit/component, tests it and pushes the changes to source control repository. It then goes through Continuous integration, automated testing, provisioning and deployment. Given High availability requirements expected (or should i say assumed) nowadays, As much as functional correctness of the Unit, it is also important to test how a Unit/Component handles failures, delays etc. in distributed environment. Often, such behavior is observed in production itself, unless project team is following practices of Chaos engineering.\nWouldn\u0026rsquo;t it be great if it is possible to start testing the resiliency features as part of development and during CI/CD pipeline execution itself ? Enter Toxiproxy\nToxiproxy is a TCP Proxy to simulate network and system conditions for chaos and resiliency Testing.\nToxiproxy essentially acts as middleman between your application and remote service/system being tested, allowing injection of delays, simulate Bandwidth restriction or turn interface off (down) etc.\nIt provides CLI as well as http API for applications to inject these behaviors or toxics. Refer here for various toxics supported.\nLets use Toxiproxy # Lets see how one can use it in typical use case where Application uses PostgreSQL database and requirement is to benchmark it against database hosted remotely. Toxiproxy can help simulate production like behavior by means of introducing network delay.\nThe full source code of this application is available here. One can refer to Numbers (only as reference cause live conditions may widely vary) here while deciding on how much toxicity to introduce.\nApplication itself is a Web server in Go using excellent HTTPRouter, It does,\nprovision a table in Postgresql and load dummy data in it.\nExposes REST API that reads data from database and returns JSON\nSetup proxy between application and database either through Toxiproxy CLI (it can also be done programmatically using Toxiproxy-Go client),\n[ { \u0026#34;name\u0026#34;: \u0026#34;pgsql\u0026#34;, \u0026#34;listen\u0026#34; : \u0026#34;[::]:6000\u0026#34;, \u0026#34;upstream\u0026#34; : \u0026#34;127.0.0.1:5432\u0026#34;, \u0026#34;enabled\u0026#34;: true } ] or through Code like,\n// InjectLatency helper func InjectLatency(name string, delay int) *toxiproxy.Proxy { proxy, err := toxiClient.Proxy(name) if err != nil { panic(err) } proxy.AddToxic(\u0026#34;\u0026#34;, \u0026#34;latency\u0026#34;, \u0026#34;\u0026#34;, 1, toxiproxy.Attributes{ \u0026#34;latency\u0026#34;: delay, }) return proxy } Use hey or any other HTTP Benchmarking tool to test end points with and without toxicity\nor\nGo benchmark tests tests that are executed against HTTP end points. In my opinion, Toxiproxy allows us to embed aspect(s) of resiliency verification in the code itself so developer can test it before committing the code and it can be embedded in DevOps pipeline to get early feedback before facing the music in production environment.\nLike latency, one can easily introduce other Toxics like Bandwidth, Down and Timeout to check Application\u0026rsquo;s behavior when faced with such occurrences.\nUseful References, # ToxiProxy - for all details on the tool like Clients available in various languages, server releases and so on. Happy Coding !!\n"},{"id":16,"href":"/posts/temporalworkflow/","title":"Using Temporal.io to build Long running Workflows","section":"Posts","content":" Background # In a typical business Application, there are often requirements for,\nBatch processing - Often long running Tasks like data import/export, End of day processing etc. These tasks are often scheduled to be executed at pre-defined interval or on occurance of an Event. Asychronous processing - Tasks, often part of business process / workflow, that can be performed asychronously or offloaded. Such requirements are often fulfilled with custom approaches like batch processing frameworks, ETL Tools or using Queues or specific database features.\nI had been following how Uber fulfils these requirements using their Cadence platform. Cadence (now Temporal) is a distributed, scalable, durable, and highly available orchestration engine to execute asynchronous long-running business logic in a scalable and resilient way.\nTemporal defines workflow as any program which,\ngoes beyond single request-reply has multiple steps tied together with inherent state can be short or long lived. performs Event processing involves infrastructure automation This is interesting perspective that accommodates various use cases irrespective of architecture style (i.e. Monolith, Microservices) in use. In Temporal, one has to create workflow which in turn consists of one or more activities. Activities are functions containing actions to be taken on each step of the workflow. Temporal transparently preserves all the state associated with workflow and its activities.\nBelow is System architecture of Temporal, more details here,\nTemporal Architecture Overall, Temporal offers following features,\nWorkflow implemented as Application code - Basically it allows to implement Workflow as code, just like rest of the codebase of the application. Thus allowing one to concentrate on business logic and reduces complexity about authoring workflow as DSL, JSON etc. Retries and Timeouts - Nowadays, quite a few steps in workflow involve remote service invocation and whenever one crosses boundary of the application, it is important to have retries and timeouts in place. Reliability - Robustness against failure Scalability - Horizontally Scalable Support for SAGAs - If a Workflow calls multiple external/remote services and if one of them fails then, compensation call to other services will have to be made to complete rollback. Distributed Cron - Scheduled processing of workflow or steps in workflow. Persistent Storage in MySQL, Cassandra among others Frontend for tracking and diagnostics Monitoring using Prometheus or other backends. It is very easy to get basic \u0026ldquo;Helloworld\u0026rdquo; workflow up and running using detailed instructions on setup provided here provided docker desktop or such environment is easily available. Temporal documentation does a great job on this.\nTo evaluate Temporal further, we will orchestrate below,\nList of users are imported/received (say from a file or provided as input) These users are verified/validated by Admin through some Frontend (to simulate a maker/checker process). This may not resemble real world scenario but it will help evaluate features of Temporal like Signals - Waiting on Events (such as human intervention).\nWe will have below activities in our workflow,\nImport users - This activity will import list of users from file/stream. For the sake of simplicity, we will just pass it as string. func ImportUsers(ctx context.Context, userdata string, DbConnectionString string) (int, error) { logger := activity.GetLogger(ctx) logger.Info(\u0026#34;ImportUsers activity started.\u0026#34;, zap.String(\u0026#34;Dbconn\u0026#34;, DbConnectionString)) // Open connection to database db, close, err := GetSQLXConnection(context.Background(), DbConnectionString) if err != nil { logger.Error(\u0026#34;Cant open connection to database\u0026#34;, zap.Error(err)) return 0, err } defer close() if _, err := db.Exec(DBSchema); err != nil { logger.Error(\u0026#34;Error while executing Schema\u0026#34;, zap.Error(err)) return 0, err } logger.Info(\u0026#34;Database connection opened, now parsing user data\u0026#34;) sqlStmt := \u0026#34;insert into users(name,dob,city) values(?,?,?)\u0026#34; tx := db.MustBegin() defer func() { if err != nil { tx.Rollback() } tx.Commit() }() r := csv.NewReader(strings.NewReader(string(userdata))) r.Comma = \u0026#39;,\u0026#39; r.Comment = \u0026#39;#\u0026#39; records, err := r.ReadAll() if err != nil { logger.Error(\u0026#34;Error while reading\u0026#34;, zap.Error(err)) return 0, err } i := 0 for i, record := range records { if i == 0 { continue } logger.Info(\u0026#34;Record read is -\u0026gt;\u0026#34;, record[0]) if _, err := tx.Exec(sqlStmt, record[0], record[1], record[2]); err != nil { logger.Error(\u0026#34;Error while writing user record\u0026#34;, zap.Error(err)) return 0, err } } return i, nil } Approve users - This activity will mark all those users, Approved by Admininstrator via Service, as approved. func ApproveUsers(ctx context.Context, DbConnectionString string, Users string) (int, error) { logger := activity.GetLogger(ctx) logger.Info(\u0026#34;ApprovedUsers called\u0026#34;, zap.String(\u0026#34;Dbconn\u0026#34;, DbConnectionString), zap.String(\u0026#34;Userlist\u0026#34;, Users)) db, close, err := GetSQLXConnection(context.Background(), DbConnectionString) if err != nil { logger.Error(\u0026#34;Cant open connection to database\u0026#34;, zap.Error(err)) return 0, err } defer close() if _, err := db.Exec(DBSchema); err != nil { logger.Error(\u0026#34;Error while executing Schema\u0026#34;, zap.Error(err)) return 0, err } r := csv.NewReader(strings.NewReader(Users)) tx := db.MustBegin() defer func() { if err != nil { tx.Rollback() } tx.Commit() }() sqlStmt := \u0026#34;update users set isapproved =1 where id =:1\u0026#34; i := 0 for { record, err := r.Read() if err == io.EOF { break } if err != nil { logger.Error(\u0026#34;Error while reading from file\u0026#34;, zap.Error(err)) return 0, err } if i == 0 { continue } i++ if _, err := tx.Exec(sqlStmt, record[0]); err != nil { logger.Error(\u0026#34;Error while writing user record\u0026#34;, zap.Error(err)) return 0, err } } return i, nil } HTTP Service - This service will receive list of approved users and send it over to workflow via Signal, func (s *server) UpdateUsers(w http.ResponseWriter, r *http.Request, ps httprouter.Params) { creader := csv.NewReader(r.Body) records, err := creader.ReadAll() if err != nil { log.Fatal(err.Error()) http.Error(w, err.Error(), http.StatusBadRequest) return } // Create the client object just once per process c, err := client.NewClient(client.Options{}) if err != nil { log.Fatalln(\u0026#34;unable to create Temporal client\u0026#34;, err) http.Error(w, \u0026#34;Internal Error :Temporal\u0026#34;, http.StatusInternalServerError) return } defer c.Close() workflowOptions := client.StartWorkflowOptions{ ID: app.UserApprovalWorkflow, TaskQueue: app.UserApprovalTaskQueue, RetryPolicy: \u0026amp;temporal.RetryPolicy{ InitialInterval: time.Second, BackoffCoefficient: 2.0, MaximumInterval: time.Minute, MaximumAttempts: 5, }, } _, err = c.SignalWithStartWorkflow(r.Context(), app.UserApprovalWorkflow, app.ApprovalSignalName, records, workflowOptions, app.OnboardUsers, app.Userdata, s.DBConnection) if err != nil { log.Fatal(err.Error()) http.Error(w, \u0026#34;Internal Error: Workflow\u0026#34;, http.StatusInternalServerError) return } fmt.Fprint(w, \u0026#34;Success\u0026#34;) } HTTP service uses workflow.SignalWithStartWorkflow function. This function sends the signal to running instance of workflow or starts new if none is in progress. Full source code is available here\nTemporal documentation has reference to Helm charts for deploying temporal in clustered configuration, for organization who is managing own data center it would be interesting to know if it also supports bare metal based deployment in addition to Kubernetes. Will update this post as and when details are available on this.\nOverall, Temporal provides a different approach to workflow orchestration. Its been battle tested at Uber and host of other companies. Temporal Community is a very active one with founders actively participating in discussions.\nCollection of Temporal related stuff Happy Coding !!\n"},{"id":17,"href":"/posts/opentelemetry/","title":"Getting Started with OpenTelemetry","section":"Posts","content":" Background # How many times have we landed up in a meeting staring at random slowness or such production issues in a distributed Application ? only to experience helplessness with limited (or often times no) visibility available about the runtime behavior of the Application. It often ends up in manually correlating whatever diagnostic data available from Application and combining it with trace/logs that are available from O/S, databases etc. and trying to figure out \u0026ldquo;Root cause\u0026rdquo; of the issue.\nTodays mission critical, distributed applications and systems make it even more important to observe them, be it serving web requests, processing stream of data or handling events. The scale at which these applications/systems operate at, often hundreds or thousands of requests, requires watching how well system is working, instead of waiting for failure or doing analysis post failure.\nIn distributed systems, telemetry can be divided into three overarching flavors:\n(Distributed) Traces: detailed records of the paths that distinct requests take as they propagate across an entire system (including across service boundaries) Metrics: aggregate statistics (mostly counters and gauges) about processes and infrastructure, typically with key:value tags attached to disaggregate patterns Logs: timestamped messages  sometimes structured  emitted by services or other components (though, unlike traces, not necessarily associated with any particular user request or transaction) To this effect, Cloud Native Computing Foundation (CNCF) has been working on Opentelemetry.\nWhat is OpenTelemetry? # OpenTelemetry is a vendor-neutral standard for collecting telemetry data for applications, their supporting infrastructures, and services.\nFor deep dive, history etc., refer to Overview here.\nSo is this standard already available? As of this writing, it is about to go GA soon. This makes it more important to be aware of its scope (subjected to change). Let\u0026rsquo;s see how it is proposing to address/implement Observability.\nBelow diagram depicts what OpenTelemetry does in Nutshell (Source: Opentelemetry.io),\nOpenTelemetry in Nutshell The general process of using OpenTelemetry is,\nInstrumentation of Application Code (including libraries) Validate Instrumentation by sending it to Collector like Jaeger (For simplicity, we will only be using Console exporter) Learn how Instrumentation helps in correlating, watching runtime behavior While vendors, having back-end systems, are providing or working on integrations with OpenTelemetry. The OpenTelemetry team has provided client libraries for Instrumentation in Go, .NET, Java,JavaScript, Python (and more coming). So lets us see what these libraries offer as of today by implementing .NET library.\nIn this post, We will look at how Opentelemetry helps us with \u0026ldquo;Distributed tracing\u0026rdquo;.\nOpenTelemetry for .NET # .NET client of OpenTelemetry supports both .NET Framework as well as .NET Core.\nFor list of available instrumentation libraries and exporters, refer here\nIn the below sections, We will try to simulate a scenario, which is typical in Microservices style of Architecture, where service invokes another service using HTTP. Now, aim is to verify how using OpenTelemetry will help in watching traffic between these two services.\nSample Scenario Lets start,\nCreate Web Service (I am using .NET Core SDK 3.1.300 on Windows)\nUse dotnet new webapi to scaffold a REST API\nAdd references to below packages using Nuget,\nOpenTelemetry.Exporter.Console - Exporter package to output telemetry to Console OpenTelemetry.Instrumentation.AspNetCore - Package that transparently instruments ASP.NET Core request processing pipeline OpenTelemetry.Instrumentation.Http - Package that transparently instruments HTTP Communication. Startup.cs - It configures OpenTelemetry instrumentation with Console Exporter and instrumentation for HTTP requests. Below is ConfigServices function of Startup class.\npublic void ConfigureServices(IServiceCollection services) { services.AddOpenTelemetryTracing( (builder) =\u0026gt; builder.AddAspNetCoreInstrumentation(opt =\u0026gt; opt.Enrich = (activity, eventName, rawObject) =\u0026gt; { if (eventName.Equals(\u0026quot;OnStartActivity\u0026quot;)) { if (rawObject is HttpRequest httpRequest) { activity.SetTag(\u0026quot;requestProtocol\u0026quot;, httpRequest.Protocol); } } else if (eventName.Equals(\u0026quot;OnStopActivity\u0026quot;)) { if (rawObject is HttpResponse httpResponse) { activity.SetTag(\u0026quot;responseLength\u0026quot;, httpResponse.ContentLength); } } }) .AddHttpClientInstrumentation() .AddConsoleExporter() //opt =\u0026gt; opt.DisplayAsJson = true) ); } WeatherForecastController.cs - This is default controller added by dotnet new webapi command. We will add GET endpoint to simulate a dummy HTTP Request. This is to verify telemetry produced for the same.\n[HttpGet(\u0026#34;{key}\u0026#34;)] public async Task\u0026lt;IEnumerable\u0026lt;WeatherForecast\u0026gt;\u0026gt; Get(string key) { // Making an http call here to serve as an example of // how dependency calls will be captured and treated // automatically as child of incoming request. var res = await httpClient.GetStringAsync(string.Format(\u0026#34;https://www.google.com/search?q={0}\u0026#34;, key)); var rng = new Random(); return Enumerable.Range(1, 5).Select(index =\u0026gt; new WeatherForecast { Date = DateTime.Now.AddDays(index), TemperatureC = rng.Next(-20, 55), Summary = Summaries[rng.Next(Summaries.Length)], }) .ToArray(); } Lets create a Service 1 For the sake of simplicity, we will have \u0026ldquo;Service 1\u0026rdquo; implemented as Console Application,\nUse dotnet new console to create new App.\nAdd reference to \u0026ldquo;OpenTelemetry.Exporter.Console\u0026rdquo; using dotnet add OpenTelemetry.Exporter.Console -version 0.7.0-beta.1. This package is specifically meant for exporting telemetry to Console. There are other exporters available to export to Jaegar, Zipkin etc. but this is simplest one to setup.\nAdd reference to \u0026ldquo;OpenTelemetry.Instrumentation.Http\u0026rdquo; using dotnet add OpenTelemetry.Instrumentation.Http -version 0.7.0-beta.1. This package helps in instrumenting HTTP requests.\nAdd below code to program.cs,\nstatic async Task Main(string[] args) { // Configure OpenTelemetry Tracer with Console exported and initiate it Sdk.CreateTracerProviderBuilder() .AddHttpClientInstrumentation() .AddConsoleExporter() .Build();\ntry { // Simulate HTTP Request to our service string responseBody = await client.GetStringAsync(\u0026quot;https://localhost:5001/weatherforecast/abc\u0026quot;); Console.WriteLine(responseBody); } catch (HttpRequestException e) { Console.WriteLine(\u0026quot;\\nException Caught!\u0026quot;); Console.WriteLine(\u0026quot;Message :{0} \u0026quot;, e.Message); } Console.WriteLine(\u0026quot;Done!\u0026quot;); } In this class, we have configured OpenTelemetry tracer with Console Exported and intialized it. Further, HTTP requests are automatically instrumented since we have added OpenTelemetry.Instrumentation.Http package.\nObserve the Telemetry,\nStart the Web Service. Check that it is listening on port 8080 by visiting https://localhost:8080. Note: you may have to install Client certificate to enable SSL. Start the Console Application. This application will send HTTP request to the service. Observe the telemetry produced by, Console Application,\nDefault Telemetry generated Activity Id (GUID) is generated for a Span (Refer here for details on what span means) It also records start and end time Web Service, Default Telemetry Observations\nCheck Activity ID being shown is same as one reported by Console Application. So correlation has been established across process boundaries. This is important when tracing end to end across processes. This is achieved by means of passing Activity ID as HTTP Header. In a Visualization tool, this correlation is used to depict end to end flow with time at each step. By default, it logs start and end time. For any HTTP request, it generates additional telemetry covering URL to which request was sent and start and end time. In Summary, this default telemetry can obviously be enhanced by adding Tags. When coupled with additional telemetry in the form of metering (to statistically observe behavior of high traffic, large scale system) and telemetry from Infrastructure (i.e. OS) and other Systems (e.g. Databases), it truly provides complete view of proceedings end to end.\nHope this provides overview of instrumentation as provided by OpenTelemetry. Let me know if you have any questions or suggestions in Comments section below.\nInstrumenting .NET framework based Apps for same scenario is similar to above, refer folder Opentelemetry in repository here\nUseful References, # OpenTelemetry in 2023 OpenTelemetry in .NET Short course on OpenTelemetry) Happy Coding !!\n"},{"id":18,"href":"/posts/ninjabuildsystem/","title":"Ninja - Using lightweight build system for Go projects ","section":"Posts","content":" Background # I primarily work on Windows for development purposes. Whenever its about writing code in Golang, invariably one comes across usage of Make. A quick check on popular Go projects on Github will show Makefile being used to automate tasks like linting, build, testing and deployment.\nBeing on Windows, i have been looking for alternative build tool that is easy to setup (i.e. doesn\u0026rsquo;t require mingw and such environments) and use compared to Make (which is primarily targetted at Unix and Unix like Operating Systems).\nFollowing a wonderful post by Julia Evans (read here) on Ninja. I decided to give it a try for a Golang Application.\nJulia, in her post, has covered important aspects of Ninja but to summarize, Ninja is,\nA build automation tool Lightweight, with focus on speed Easy to configure Cross platform (Easy to setup across Windows and Linux) With this, lets give it a try,\nTo start with, lets create a simple go \u0026lsquo;Hello World\u0026rsquo; project,\nInitiate Go Module (in a Empty folder), go mod init github.com/sachinsu/ninjabuild\nCreate a \u0026lsquo;main.go\u0026rsquo; that prints \u0026lsquo;Hello World\u0026rsquo;,\npackage main import \u0026#34;fmt\u0026#34; func main() { fmt.Println(\u0026#34;hello world\u0026#34;) } Now setup Ninja, It is as easy as downloading binary for your Platform. It is also possible to build it locally, if you prefer it that way. For details, refer here\nOnce ninja is setup, lets create build configuration file (i.e. build.ninja),\nGOARCH = amd64 GOOS = linux rule lint command = go vet -mod=vendor ./... build lintoutput: lint rule unit command = go test -mod=vendor -cover -v -short ./... build utest: unit rule compile command = cmd /c go mod tidy \u0026amp;\u0026amp; go mod vendor \u0026amp;\u0026amp; go build -o $out $in \u0026amp;\u0026amp; echo \u0026#34;build done.\u0026#34; description = compile $in build ninjabuild.exe: compile . lets go through the contents of this file,\nOne can define variables GOARCH = amd64 and refer them as $GOARCH\nNinja configuration is combination of build step and associated rule, for e.g.\nrule compile command = cmd /c go mod tidy \u0026amp;\u0026amp; go mod vendor \u0026amp;\u0026amp; go build -o $out $in \u0026amp;\u0026amp; echo \u0026#34;build done.\u0026#34; description = compile $in build ninjabuild.exe: compile . Above snippet, defines rule compile with associated command that builds the code. Being on Windows, i have used cmd /c to start a new shell and concatenate multiple commands as part of compile rule using \u0026amp;\u0026amp; which chains the commands and executes next one only if current one succeeds. As demonstrated in above file, Ninja can be used to automate wide variety of tasks like build, tests, deployment and so on.\nMany of you using Make will find the approach similar to it. In contrast to Make, Ninja lacks features such as string manipulation, as Ninja build files are not meant to be written by hand. Instead, a \u0026ldquo;build generator\u0026rdquo; should be used to generate Ninja build files.\nI found simplicity (of installation and configuration) and easy of use to be key aspects of this tool.\nHappy Coding !!\n"},{"id":19,"href":"/posts/urlhealthchecks/","title":"Validating urls from 'Useful Links' section using bash / command line tools","section":"Posts","content":" Background # I started this blog, https://sachinsu.github.io few months back .\nIn this relatively short period of time, Blog has sizeable number of useful links across various categories in addition to the detailed blog post like this one.\nAs an ongoing activity, I think that it is necessary to verify links mentioned on this blog.\nSo how can it be done ? obviously one way is to do it manually by visiting each link and updating/removing those that are no longer available. but there is always of better way of doing things.\nThe requirement is to,\nParse all the files to links (being in Markdown links will be enclosed in brackets) Send request to each link and verify if its active using HTTP Status (say 200 or 302) Approach # Enter Automation !!\nIt is possible to write a utility/tool (or it might be already available) or can good old command line utlities be used for this task?\nI decided to go for dos / shell script way and surprisingly all the necessary tools are already available.\nBelow is single command line that fulfils the requirement,\ngrep -E -i -w \u0026quot;http|https\u0026quot; *.md | sed 's/](http/\\nhttp/g' | sed 's/)/\\n/g' | grep ^http | xargs curl -s -I -w 'URL:%{url_effective} - %{http_code}\\n' | grep ^URL:\nIn above chain,\nI am using excellent Cmder console emulator, which also makes above nice tools (grep, sed etc.) available on Windows.\ngrep -E -i -w \u0026ldquo;http|https\u0026rdquo; *.md - this command extracts all the lines containing http(s) from all the markdown (.md) files\nPipe | - Pipe command streams output of command to the next one.\nsed \u0026rsquo;s/](http/\\nhttp/g\u0026rsquo; - this sed (stream editor) command adds line break before http for better extraction.\nsed \u0026rsquo;s/)/\\n/g\u0026rsquo; - this sed (stream editor) command removes trailing ) bracket.\ngrep ^http - this command removes all lines not containing http.\nxargs - xargs is a command on Unix and most Unix-like operating systems used to build and execute commands from standard input.\ncurl -s -I -w \u0026lsquo;URL:%{url_effective} \u0026mdash;\u0026gt; %{http_code}\u0026rsquo;\u0026rsquo; - previously used xargs command feeds each line (url) to this command as last argument. This command sends tcp request to the URL and prints out http status code along with URL.\ngrep ^URL: - For some reason, CURL outputs content even if -s (silent) parameter is passed. Hence, this grep command is used to ignore all lines not containing URL and HTTP Status.\nThe output is as below,\nList of URLs with HTTP Status code So, It is possible to quickly come up with this using built-in tools if writing a program is not an option or cumbersome for task at hand.\nAs a next step, Plan is to automatically run this script as part of Github Build and notify in case of any URL is failing so that appropriate action can be taken.\nHat Tip # Suppose the requirement is to extract a particular text by recursively searching through files(for e.g. extract Target .NET Framework version across each of the project in a folder) then grep can be used as below,\ngrep -r --include \u0026quot;*.csproj\u0026quot; -oP \u0026quot;\u0026lt;TargetFrameworkVersion(?:\\s[^\u0026gt;]*)?\u0026gt;\\K.*?(?=\u0026lt;/TargetFrameworkVersion\u0026gt;)\u0026quot; .\nThis command will recursively search through all folders and print names of all those .csproj files containg \u0026lt;TargetFrameworkVersion\u0026gt; tag.\nLet me know (in comments) if you are aware of any alternate better way of achieving this.\nHappy Coding !!\n"},{"id":20,"href":"/posts/connectiontimeouts/","title":"Trobleshooting TCP Connection request time outs","section":"Posts","content":" Background # I recently had opportunity to support team who has been battling with Intermittent (scary i know :)) issues with TCP connectivity in Production.\nSimplified deployment Architecture is as below,\nHigh Level Architecture Technology Stack used is Microsoft .NET Framework 4.8 using ODP.NET for Oracle Connectivity (Oracle Server is 8 CPU box). Each of Web Servers in cluster have IIS hosted on it with multiple Applications (Application domains) serving HTTP(s) based traffic. These applications connect to Oracle Database.\nTeam is experienced in developing and running .NET Apps, but they needed help to diagnose and fix \u0026ldquo;Connection request timed out\u0026rdquo; exceptions being thrown while connecting to backend database.\nProblem Statement # Host of .NET Applications (Web Applications, Web APIs) connect to Oracle Database. Each of them use ODP.NET. ODP.NET maintains connection pool per Application domain (Database resident Connection pool is not used). Some of these applications receive high number of requests per second compared to others.\nOracle.ManagedDataAccess.Client.OracleException (0x80004005): Connection request timed out.... has been reported randomly which results in failure of business transactions. ODP.NET provides extensive trace and along with above trace also contains OracleInternal.Network.NetworkException (0x80004005): Network Transport: TCP transport address connect failure ---\u0026gt; System.Net.Sockets.SocketException (0x80004005): A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond\nSpecifically, Applications, receiving less traffic, were reporting it often compared to those with high traffic.\nApproach # Simulate the Exception in Test Environment - We decided to try and simulate this exception in a Test Environment. Test environment is scaled down (50%) compared to production. Random Exceptions in production could not be simulated due to lack of Test Automation. for example, In Production, each server receives traffic for multiple HTTP End points whereas in Test environment, load testing was being done only against Single Application. This was like a end of the road since simulation would have greatly helped in diagnosing the issue. However, the show must go on so we decided to,\nCheck online documentation regarding this exception,\n\u0026ldquo;Pooled\u0026rdquo; or \u0026ldquo;Non-pooled\u0026rdquo; Connection - Whenever, ODP.NET raises \u0026ldquo;..timed out\u0026rdquo; error, it diffrentiates the same to indicate whether error is due to delay in retrieving connection from pool or if it is due to delay from the database server (Ref: Here). From this, it was clear that issue is clearly due to delay in obtaining response from database server.While this was happening , Database server (8 CPU Core box) was reporting less than 50% CPU Usage but it still had large number of inactive sessions originated from IIS Servers.\nSince the exception was reported frequently in low traffic applications, it was decided to track and verify the same on firewall and database server,\nFirewall - Firewall had TCP Timeout of 30 minutes and maintains detailed log of sessions. Quick analysis of it revealed that,\nProduction Environment - Unusually high number of sessions were being destroyed due to \u0026ldquo;Age out\u0026rdquo; (i.e. time out) Test Environment - No abnormal activity was reported. Most probably because of differences in traffic. Database Server - Listener Log on Oracle Database server did not had any log entry for request at the precise time when Application(s) had reported Exception.\nNext is to check settings in Application for connectivity with Oracle. Though ODP.NET does not have any direct \u0026ldquo;Time out\u0026rdquo; or \u0026ldquo;Time to live\u0026rdquo; settings, it does provide few parameters that can influence it,\n\u0026ldquo;Connection Lifetime\u0026rdquo; - ODP.NET uses this whenever Connection is closed and disposed by the Application. It will be destroyed (after maintaining number of connections as per \u0026ldquo;Min Pool Size\u0026rdquo;) if it has exceeded life time. For whatever reasons, this was set to unusually high duration (i.e. 90000 seconds). \u0026ldquo;Connection Timeout\u0026rdquo; - Period for which ODP.NET waits for the connection to be made available. This was set to 60 Seconds. Oracle has articles titled \u0026ldquo;ODP-1000 \u0026ldquo;Connection Request Timed Out\u0026rdquo; Explained\u0026rdquo; and \u0026ldquo;Resolving Problems with Connection Idle Timeout With Firewall (Doc ID 257650.1)\u0026rdquo; where it primarily recommends measures for tuning Application as well as database.\nOn the basis of above, it was decided to modify the code for below,\nThorough code review to verify that every ODP.NET Project is closed/disposed. Upgrade ODP.NET to latest version (v19.8.0 as of this writing) Turn \u0026ldquo;KeepAlive\u0026rdquo; while connecting to database Leverage ODP.NET tracing in case of exception Modify the connection lifetime to be less than time out at firewall and increase the \u0026ldquo;Time out\u0026rdquo; period. Introduce Retry functionality using Excellent Polly library with exponential back-off. These changes have been deployed to production and so far % of \u0026ldquo;Connection Request Timed out\u0026rdquo; errors have gone down significantly.\nWrap up # Some key areas of focus are,\nFor a distributed system, Always validate assumptions by dignosing end to end. Plan to have test automation readyness to simulate production like load. Monitoring the behavior end to end using logs. Currently, Pool settings across applications is not optimal going by Oracle Real world Guidelines, also be mindful of Connection Storms Happy Troubleshooting !!\n"},{"id":21,"href":"/posts/massdmgolang/","title":"Tool to mass DM followers on Twitter in Go","section":"Posts","content":" Background # I recently came across bounty by Balaji Srinivasan to send Direct Message to all twitter followers. Currently, i do not intend to participate in bounty and this is mere exercise.\nThis is an attempt to write CLI tool in Golang in response to it.\nFor detailed requirements, refer here\nApproach # In Brief,\nCLI should,\naccept arguments like Twitter API Key,Auth token, DM Message Download all followers (with profile details) Rank them by Criteria (e.g. Location) Send each follower a DM with provided message (upto daily DM Limit) be easy to use and maintain Notes,\nDue to Daily DM Limit, Follower details will have to be persisted alongside flag indicating if DM has been sent. SQLIte is used from simplicity perspective. There should be a scheduled job that will send the DM upto daily DM Limit. At the same time, it needs to refetch any new followers and push them in the flow (reconcile). Potentially, this could be extended to other social media providers other than twitter. Milestones, Create code structure Plan is to have separation between CLI \u0026amp; have twitter as go package Accept Arguments and Connect to Twitter Study and complete follower retrieval Ranking of followers Persisting followers Sending DM upto Daily limit Rules, Use golang\u0026rsquo;s in-built packages as much as possible Every milestone to have associated Unit test cases Current Status # The code is ready and functionality to retrieve followers and saving in local DB is tested. Code to send DM is not yet tested as it will require setting up dummy twitter account.\nRoadmap # In addition to CLI, Expose the utility as responsive Web Application Possibly extend this to social media platforms other than Twitter Have a look at code on Github and let me know what you think.\nHappy Coding !!\n"},{"id":22,"href":"/links/onlearn/","title":"Resources for Online Learning","section":"Links","content":"Section covering resources for Online learning etc.\nExploring basics of Computer Science, bit by bit Exploring basics of Distributed Systems Awesome List of Free Learning Resources Collection of online learning resources Complete intro to Linux and CLI Linux System Administration - Skill challenge "},{"id":23,"href":"/links/python/","title":"Programming Languages - Python","section":"Links","content":"Python has become pervasive all throught Data Science be it Machine learning, Deep learning, Data Processing and general purpose tasks like Web Development.\nCourses, Trainings # Python Tutorials Python for Beginners from MSDN Nice Collection of trainings per level of complexity Python Programming And Numerical Methods: A Guide For Engineers And Scientists Practical Python Projects Articles # Getting machine learning to production A quick-and-dirty guide on how to install packages for Python What to do when data doesn\u0026amp;amp;rsquo;t fit in memory DataSette - Architecture Notes Packages # EasyOCR - supports 40\u0026#43; languages Simplified Static file serving for Python Static Site generator (with Markdown support) JupyterLab Desktop App Python helper library for ETL between databases Podcasts # Talkpython "},{"id":24,"href":"/posts/websecurity/","title":"Web Security Measures in ASP.NET Applications","section":"Posts","content":"At my current workplace, All Applications are expected to adhere to PCI DSS standards meant for Data protection, Access Regulation and so on. Dedicated SOC Team,consisting of Security analyst who are continously on the prawl to identify breach, conduct periodic auditing of Applications, hardening of Servers.\nWhile all our .NET applications adhere to below guidelines,\nASP.NET Security Overview Secure Coding Guidelines Security Guidelines by OWASP We also use tools like Snyk to perform code vulnerability analysis as part of Jenkins driven CI/CD pipeline. In spite of above, we do come across vulnerabilities identified by SOC Team which we needs to be addressed quickly. SOC team uses tools such as Burp Suite.\nThis post is going to summarize such incidents reported so far (and will keep updating it). In most of these cases, These issues required additional efforts over and above those provided by library or framework. Hopefully, it will be helpful to anyone trying address such vulnerabilities.\nCookie Path # Every cookie being sent by the Web application has attributes like,\nHTTPOnly - Indicates whether a cookie is accessible by client-side script Domain - Indicates the domain to associate the cookie with Path - the virtual path to transmit with the current cookie Secure - Indicates whether cookie is sent only on Secure (HTTPS) Connections. OWASP has nice primer on Cookie Security.\nOf the above, Path attribute limits the scope of a cookie to a specific path on the server and can therefore be used to prevent unauthorized access to it from other applications on the same host. Accordingly, SOC Team had recommended that all cookies issued by application must have path attribute set.\nIn case of typical ASP.NET Application, there are cookies generated by .NET framework (like for Session, Anti XSRF Token and son on) and custom ones which are issued and used by Application itself.\nWhile it is fairly easy to set path for custom ones, we had to make code changes for cookies issued by .NET framework libraries. Lets take case of Session ID cookie, by default, this cookie always has root / as path. So how can this be changed as per the application\u0026rsquo;s deployment settings (i.e. specific virtual directory in IIS)?\nWe tried below,\nStep 1, try using httpcookies section in web.config like, \u0026lt;httpCookies requireSSL=\u0026#34;false\u0026#34; httpOnlyCookies=\u0026#34;true\u0026#34; domain=\u0026#34;site.com/myapp\u0026#34;/\u0026gt; First of all, this configuration element does not allow setting Path property and even during runtime, only the Domain property is populated while issuing the cookie. So this definitely does not help address the issue.\nSo other way is to programmatically set the path for Session Cookie. This can be done by providing custom implementation of SessionIDManager class like below,\npublic class MySessionIDManager : SessionIDManager, ISessionIDManager { void ISessionIDManager.SaveSessionID(HttpContext context, string id, out bool redirected, out bool cookieAdded) { base.SaveSessionID(context, id, out redirected, out cookieAdded); if (cookieAdded) { var name = \u0026#34;ASP.NET_SessionId\u0026#34;; var cookie = context.Response.Cookies[name]; // this will be possibly read from configuration cookie.Path = \u0026#34;/myapp\u0026#34;; } } } Thanks to this Stackoverflow thread for listing this approach. Application under consideration only had this particular cookie however, for all other .NET framework issued cookies, similar technique will have to be used.\nSameSite Cookie # This is already detailed here\nMasking of Sensitive Data # This typically involves masking the sensitive data like\nE-mail id Phone Number Credit/Debit Card Number It could well be used in Web Application or be received or sent as part of HTTP API.\nThe best bet in this case is to mask it on the server side itself before sending/rendering the data in browser. Note that, in some cases, above fields are used for data binding purposes. The approach we followed in such scenario was to use Hash value of it instead of merely masking the data. We have always used SHA256 or above for hashing.\nSummary # Addressing security vulnerabilities is continuous process as hackers keep on inventing new ways for breaching and exploiting weak spots. As Application Architect/Developers, we need to brace ourselves for the same.\nHappy Coding !!\n"},{"id":25,"href":"/posts/webassembly/","title":"Is WebAssembly future of Web Development","section":"Posts","content":"Over the last many years, de-facto language of the Web (specifically front-end) has been Javascript (and variants like Typescript, ECMAScript versions and so on). The Web development has been revolving around HTML+CSS+Javascript trio. It all started with support for Javascript in browsers, followed by addition of XMLHTTP API, Rich DOM Manipulation Support in Javascript. To induce order and apply patterns to Javascript\u0026rsquo;s usage in browsers, numerous frameworks and libraries were introduced like React and Vue among others. To begin with, The target used to be browsers on Large Devices like Desktop \u0026amp; Laptops. However, soon all sorts of devices were targetted with advent of Responsive and Progressive CSS+Javascript libraries eg. Bootstrap. Offline Support soon came in ref: Electron and Progressive Web Applications.\nAs a result, Javascript has become lingua franca of Web Development and is being used on server side development (Nodejs).\nThe reason for this whole rant on history (which you are most likely to be aware of) is that latest kid on the Block could possibly challenge Monopoly of Javascript (and its ilk) at far as browsers are concerned.\nEnter WebAssembly (A.K.A. WASM)\nWebAssembly # As per Wikipedia,\nWebAssembly (often shortened to Wasm) is an open standard that defines a portable binary-code format for executable programs, and a corresponding textual assembly language, as well as interfaces for facilitating interactions between such programs and their host environment.\nWebAssembly or wasm is a low-level bytecode format for in-browser client-side scripting, evolved from JavaScript. It is intermidiate representation(IR) where IR is transformed into machine instructions for the client architecture by browser.\nWebAssembly executables are precompiled, hence it is possible to use a variety of programming languages to make them. This essentially means that one can use same language for Server Side as well as Client side (i.e. in browser) development like (C# or Golang).\nWebAssembly was announced in 2015 and has since being supported by prominent browser(s) like Chrome and Firefox.\nAlong side browsers, many Vendors and open source communities/contributors have released libraries to make development of WebAssembly easy. We will look at how a WebAssembly can be developed in C# and Golang.\nNote: All major languages now support WebAssembly.\nC# # During Microsoft Build 2020 1 event, Steve Sanderson had very good session on building WebAssembly using Blazor framework in .NET. Highly recommended to watch it.\nBlazor scaffolding provided with .NET core allows,\nBlazor Server App - A Template that runs server-side inside an ASP.NET Core app and handles user interactions over a SignalR connection.\nBlazor WebAssembly App - A Template for creating a Blazor app that runs on WebAssembly.\nChoosing Blazor WebAssembly App project type generates a project that has sample WebAssembly running. Overall, it makes development easy for any .NET developer easy since, it usesRazor syntax to add C# code along with HTML. During Build, it generates assembly for C# Code. When Accessed via browser, it downloads .NET runtime for WebAssembly (~ 621 KB) and the project assembly itself apart from static content (i.e. HTML files, images etc). The default scafolding includes Bootstrap CSS and prepares the UI to be responsive.\nThe repository referenced by Steve during presentation is available here.\nGolang # Go has got clean, fast tooling. it produces static binaries and has superb concurrency primitives.\nVugu is an open source library that allows building a Web front-end in Go using WebAssembly. Generally static binaries are bulky and Vugu has addressed it using TinyGo compiler. Vugu is still work in progress but does work great in its current form. Check out their getting started page.\nInteresting take on Journey of JavaScript and what lies ahead for it, read it here.\nSummary # In nutshell, Concept of WebAssembly provides compelling way to have full stack development in a language of your choice. It remains to be seen how and whether it provides viable alternative to current Javascript driven ecosystem.\nUseful References, # Happy Coding !!\nModern Web UI with Blazor\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n"},{"id":26,"href":"/posts/usinggithubactions/","title":"Using Github Actions for Automated Testing and Deployment","section":"Posts","content":" Background # The source code of tracfee.com is hosted on Github Private.\nAt a High level, Tracfee\u0026rsquo;s Architecture involves,\nSingle Page Application using VueJS, deployed on Netlify API in Go, deployed on Oracle Cloud So far, API testing has been automated and we were looking at ways to automate deployment of both UI and API. Steps required to deploy API are less since we are using Docker to run it on VM. However, in case of Netlify, it is required to build and then upload the output folder on Netlify.\nAccordingly, it was decided to explore Github actions to automate deployment.\nUsing Github actions # As per Github,\nGitHub Actions makes it easy to automate all your software workflows, now with world-class CI/CD. Build, test, and deploy your code right from GitHub. Make code reviews, branch management, and issue triaging work the way you want. GitHub actions work by provisioning Virtual machine to run an Event based workflow. It provides option to provision Linux/MacOS/Windows based Virtual machines. Steps in Workflow will have to be configured in YAML file. Trigger for Workflow can be (but not limited to) wide variety of events like on Push or commit on branch and so on.\nPost trigger, set of action(s) can be configured like,\nCheckout the branch Setup environment (Install Node.JS) Perform build Deployment Github has Marketplace which has many pre-built actions available. My requirement was to,\nProvision Linux (i.e. ubuntu-latest) Virtual Machine Checkout the code (using actions/checkout@v2) Setup Node.js (using actions/setup-node) perform Build and test using NPM Deploy to Netlify using netlify/actions/cli@master Any secrets required as part of Workflow can be maintained using Github secrets Above workflow needs to be maintained in .github\\workflows folder in the repository.\nbuild.yml for tracfee.com looks like,\nRefer Gist here.\nTesting the Build Workflow # After configuring the workflow steps, next question is to check whether it is possible to test it locally? Luckily, there is tool available to do this. Enter Act , which is a tool to Run your GitHub Actions locally . Local testing is useful for Faster feedback. In Nutshell, Act uses local docker setup to provision container and then runs workflow steps in it. Give it a try !!\nAs a next step, Plan is to automate deployment of API on Oracle Cloud using OCI CLI interface.\nUseful References, # Build with GitHub Actions, host on Netlify Adventures in CI/CD [#4]: Deploying A Microservice To The Oracle Cloud With GitHub Actions [OCI CLI Edition] Happy Coding !!\n"},{"id":27,"href":"/links/aiml/","title":"Artificial Intellgence, Machine Learning","section":"Links","content":"Useful links related from AI, ML space\nCollections # AI Guide by Mozilla Collection of resources related to Applied ML List of for MLOps Free courses # Fast AI by Jeremy Howard AI Canon - List of resources around GPT Free Deep learning course Articles # How to build your own perplexity for any dataset How a Machine Learns Machine learning is still too hard - Year 2022 Neural Networks from Scratch History of AI Machine Learning Algorithms: What is a Neural Network? What is Benfords Law and why is it important for data science? Benfords Law and Financial Statements Data Scientists Should Be More End-to-End Team Data science process (Microsoft) Traits of Good Data Scientist The First Rule of Machine Learning: Start without Machine Learning Deep learning is hitting wall Real world Recommendation System Videos # Neural Networks Demystified Deep Learning: A Crash course Vector Embeddings, Vector Databases # Storing OpenAI embeddings in Postgres with pgvector ChatGPT, LLMs # A practical guide to building successful LLM products. Emerging Architecture for LLM Applications LocalGPT - Chat with your documents on your local device using GPT models Run LLMs from command line Resources on LLMs AI based Translation # Lokalize - AI based translation of file Vibery - Semantic Search using embeddings and KNN Tools # Aider - AI pair programming in your terminal An open platform for training, serving, and evaluating large language models. Release repo for Vicuna and Chatbot Arena Open source LLM engineering platform: LLM Observability, metrics, evals, prompt management, playground, datasets. Integrates with LlamaIndex, Langchain, OpenAI SDK, LiteLLM, and more. Vespa is an open-source search engine and big data processing platform. Its particularly well[1]suited for applications that require low latency and high throughput. Our teams like Vespas ability to implement hybrid search using multiple retrieval techniques, to efficiently filter and sort many types of metadata, to implement multi-phased ranking, to index multiple vectors (e.g., for each chunk) per document without duplicating all the metadata into separately indexed documents and to retrieve data from multiple indexed fields at once. Kotaemon - An open-source RAG-based tool for chatting with your documents. "},{"id":28,"href":"/links/cloud/","title":"Cloud Tech","section":"Links","content":"Useful links for deep diving in various Cloud Ecosystems\nArticles # Cost of Cloud - Paradox Cloud vs. On-premise Computing Zero dollar Infrastructure stack Cloud Server Performance, Prices, Specs and Features The Cost of Cloud, a Trillion Dollar Paradox Why we are leaving cloud General Guidelines when working as Cloud Engineer Articles (GCP) # 13 sample architectures to kickstart your Google Cloud journey Using Google Cloud Spanner locally \u0026amp;amp;hellip;using Emulator Articles (AWS) # Preparation Guidelines and courses for AWS Certification Saving egress costs on AWS using S3 Reducing AWS Costs Choosing between EC2 and RDS What a typical 100% Serverless Architecture looks like in AWS! Automating safe, hands-off deployments Containerizing legacy ASP.NET applications using AWS App2Container (A2C) Replacing web server functionality with serverless services AWS Lambda vs Cloudflare Workers Unbound One line Explaination for each of AWS Services Building a Multiplayer Game with API Gateway\u0026#43;Websockets, Go and DynamoDB Best Practices To Handle Lambda Timeout Errors Save 99.93% for Lambda with Init time Architecture of SAAS on Cloud managed by One man Team AWS Costs that every programmer should know You should not be probably using AWS Videos, Talks (AWS) # DynamoDB - Advanced Design Patterns DynamoDB - Deep Dive Migration from Postgres to DynamoDB Tools # Checkov-Prevent cloud misconfigurations during build-time Infracost - Open Source tool that shows Cloud cost estimates for Terraform in pull requests About Infrastructure as a Code Cloud Native Wiki - Cloud native Architectures, DevSecOps etc. Mock AWS Infrastructure "},{"id":29,"href":"/links/uiux/","title":"UI/UX","section":"Links","content":" User Interface / User Experience # Design Approach # Modern Web - Guides, tools and libraries for modern web development. How Stripe Designs Beautiful Websites Tools for non artistic developers Principles of Design Micro frontends - Techniques, strategies and recipes for building a modern web app with multiple teams that can ship features independently. Thoughts on SPAs 33 JavaScript concepts every Developer should know Server side Events for Real-time streaming Updates Four ways to build Web Apps Parallel Data Fetching in SPA\u0026amp;amp;hellip;has Good Primer on React Articles # guideline on implementing auth in web applications Examples to manipulate HTML-DOM Comparing Polling vs WebSockets vs SSE A simple, choice-driven chatbot framework with Vanilla Javascript Centering in CSS: A Complete Guide Centering in CSS Full-bleed layout using CSS Grid Blog on HTML,fonts, Asynchronous JavaScript How to pick beautiful colors You dont need Javascript Sign in form Best practices How HTTP Range Requests work (for large file downloads etc.) Beginner\u0026amp;amp;rsquo;s guide to Next.js Data Model behind Notion\u0026amp;amp;rsquo;s flexibility How TCP Communication works between Client \u0026amp;amp; Server Practical Frontend Architecture using React,GraphQL, Next.JS and Typescript The baseline for Web development in 2022 Web UI Patterns by Addy Osmani The Web\u0026amp;amp;rsquo;s Next Transition Everything about HTMX Testing # OSS Load and Functional testing tool Puppeteer - Testing using Headless Chrome Nodejs API Playwright - Nodejs library to automate Chromium, WebKit and Firefox Platforms # Medusa - Flexible ECommerce Platform Libraries and Tools # RsPack - Fast web bundler like webpack Dash - Python based framework for Visualization with no javascript T3 - full-stack, typesafe Next.js app Perspective.js - A data visualization and analytics component, especially well-suited for large and/or streaming datasets. Observable - A static site generator for data apps, dashboards, reports Nginx Unit - Web Server with Native support for Languages GoatCounter - Open source Web site Analytics RedwoodJS - App framework Gatsby - React based fast framework Remix - Modern SPA framework Blitz - Modern SPA framework based on React Polaris design system by shopify Qiankun - Complete solution for Micro front-ends Astro - a website build tool Single SPA - Router for Micro front-ends HTMX -access AJAX, CSS Transitions, WebSockets and Server Sent Events directly in HTML, using attributes Bulletproof React - Opinionated React starter kit Javascript based Query/filter creator React based Sci-fi style UI Library with Animation and Sound Javascript libraries for Date and Time (Alternative to Moment.js) Zod - Schema validation in Typescript One line CSS Layouts by Google G9 - Interactive Graphs Interactive CSS Grid generator Msw - Mock Service Worker for REST \u0026amp;amp; GraphQL API Mocking Modern JavaScript Tutorial Web Vitals- Essential metrics for a healthy site. Clerk - User Management as Service Go based Fast Javascript bundler and minifier Finite State Machine in JS/Typescript Observable Plot - Data Visualization Library Shared data types for building collaborative software NoSQL-database for JavaScript Applications like Websites, hybrid Apps, Electron-Apps, Progressive Web Apps and NodeJs RemixIcon - Open Source Icons Repository Mermaid - Generate Diagrams from Markdown AutoMerge - Network agnostic library for JSON-like data structure (a CRDT) that can be modified concurrently by different users, and merged again automatically. Text (DSL) to diagrams Interactive guide to Flexbox End to end encryption in browser Collection of SVG logos Desktop App frameworks # Tauri - smaller, faster, and more secure desktop applications with a web frontend (Native Webviews and no chromium) le\nPWA # PWABuilder - Publish Progressive Web App as Mobile App What a PWA can do today Step by Step using PWABuilder Mobile App Development # Expo - universal native apps with React "},{"id":30,"href":"/posts/windowsservicecancellabletask/","title":"Windows Service with Cancelable Task","section":"Posts","content":" Background # Recently, we had requirement wherein a process should,\nPeriodically (Poll) or Asynchronously (Pub-sub) listen on incoming requests/messages. The whole process is expected to be long running. Should also implement clean disposal of in-flight requests and subsequent cleanup using something similar to Cancelble Context in Go The first of the objective is somewhat dependent on mechanism (Pub/sub, Listener), protocol (TCP, HTTP etc.). For the second one, .NET framework (and .NET Core) offers CancellationToken. It is maint for co-operative cancellation between threads and Task Objects. So Armed with this, is it possible to come up with a template that allows cancellation of long running task while also being deployed as Windows Service (or using systemd in Linux) ?\nLets get Started,\nApproach # We can use below to construct service,\nTopshelf - Allows Hosting services in-process as console apps or Windows services. NLog - For Logging Accordingly, we will have below Components,\nListener.cs - It wraps the long running process in a C# Task. It exposes Start and Stop functions which are essentially event handlers awaiting for Signal from the service. Refer Gist here\nProgram.cs - It configures the startup parameters for the service and initializes it. Using Topshelf, one can easily debug it as Console Application before deploying it as Service. Refer Gist here\nAbove Code was targetted at .NET Framework but the same can potentially be used on .NET Core thus targetting both Windows and Linux.\nHappy Coding !!\n"},{"id":31,"href":"/posts/usingbenchmarkdotnet/","title":"Optimizing  .NET Code using Benchmarks","section":"Posts","content":" Background # Oftentimes, we come across situation where code does not perform as per expectation. What is typically approch to address it,\nPerformance Testing - Visual Studio Load Tests or Third party tools like Locust, Vegeta, Gatling etc. Visual Studio Diagnostics Tools Or Use tools like Perfview/dotTrace/dotMemory to diagnose bottlenecks What if it is possible to Benchmark code for,\nSet of varying parameter(s) Different runtimes (.NET Framework versions, .NET core, Mono etc.) with option to Benchmark it Observe Memory Allocations for diagnostics Get Detailed report on execution timeline Have it as part of test suite so that it can be easily executed with every iteration involving optimized code to get immediate feedback Enter BenchmarkDotNet, a Powerful .NET library for benchmarking. It is used by DotNET Team, Roslyn, ASP.NET Core and many other projects.\nThough Benchmarkdotnet.org has nice documentation with detailed examples, Below we will look at how to benchmark a code which is aimed at dumping in-memory list of objects to a delimited file. In real-world scenario, the list of objects could be retrieved from external data store.\nSo Lets Start.\nApproach # We will have below before we proceed with using BenchmarkDotNet\nDummy class that represents Data Structure to be dumped to a file, Refer Gist here\nClass CardWriter.cs that generates file using,\nUsing StreamWriter with Buffer Using Stringbuilder and StreamWriter Using Open source CSVHelper library Refer Gist here\nNow, let us write code to benchmark above functions with Memory Diagnostics, Refer Gist here\nAbove code,\nClass FileGeneratorBenchmark - This class uses BenchmarkDotNET attributes to decorate set of functions which in turn call functions from CardWriter.cs class. Class Program - General purpose class with static main function that invokes BenchmarkRunner to execute benchmarks. It is required to run these benchmarks in Release mode or else BenchmarkDotNet will alert about the same. After running the benchmark, It will generate detailed report like below,\nReport shows Memory Allocation as well as Execution time lines across Platform (.NET Framework Vesions) and parameters.\nReferences:\nBenchmarkDotNet Introduction to Benchmarking C# Code with Benchmark .NET Happy Coding !!\n"},{"id":32,"href":"/posts/samesitecookies/","title":"ASP.NET Core - Mind the SameSite HTTP Cookie settings","section":"Posts","content":" Background # A Web Application, developed in ASP.NET Core (Runtime Version 3.1.100) using Razor Pages and Web API, is expected to be launched from within third-party Web Application in iframe, with complete HTML being rendered.\nDuring the Development, a mock HTML Page was developed to simulate launching of ASP.NET core based Web Application in iframe. Note that this page as well as Application was hosted on same IIS Server and it worked fine. Subsequently, Web Application was deployed on Test Server and URL was shared for integration with third party Application and then it happened Boom\u0026hellip;. i.e. Application when launched in iframe rendered HTML but none of the post request would work (returning HTTP Error 400). Careful inspection showed that,\nBrowser\u0026rsquo;s Dev tools showed HTTP 400\nThere were no entries in Application\u0026rsquo;s Log File which indicates that Request was rejected either by IIS or ASP.NET Core\u0026rsquo;s chain of filters i.e. even before it reaches handler.\nIIS Log depicted that Request was rejected but had no additional details. May be some of the log settings were missing.\nNext up is to carefully look at Request sent by browser in \u0026lsquo;Network\u0026rsquo; tab of Dev tools. It showed that none of the cookies required by Application (i.e. for Session, CSRF token etc.) were present.\nEnter SameSite\nSameSite # SameSite is a standard designed to provide some protection against cross-site request forgery (CSRF) attacks. Support for Samesite was added from .NET Core 2.2 and onwards. It is expected that developer will control the value of SameSite attribute using HttpCookie.SameSite property.Setting the SameSite property to Strict, Lax, or None results in those values being written on the network with the cookie.\nCookies without SameSite header are treated as SameSite=Lax by default. SameSite=None must be used to allow cross-site cookie use. Cookies that assert SameSite=None must also be marked as Secure. Applications that use \u0026lt;iframe\u0026gt; may experience issues with sameSite=Lax or sameSite=Strict cookies because \u0026lt;iframe\u0026gt; is treated as cross-site scenarios. The value SameSite=None is not allowed by the 2016 standard and causes some implementations to treat such cookies as SameSite=Strict. The SameSite=Lax setting works for most application cookies.\nAccordingly, below settings were made in startup.cs of the ASP.NET Core Application.\nservices.AddSession(options =\u0026gt; { options.IdleTimeout = TimeSpan.FromMinutes(30); options.Cookie.HttpOnly = true; // Samesite Settings. options.Cookie.SameSite = SameSiteMode.Lax; options.Cookie.IsEssential = true; }); services.AddAntiforgery(options =\u0026gt; { options.Cookie.SameSite = SameSiteMode.Lax; }); References # SameSite cookie updates in ASP.net, or how the .Net Framework from December changed my cookie usage. Changes in SameSite Cookie in ASP.NET/Core and How it Impacts the Browser (Specifically Chrome) HTTP 203 Podcast covering CORS,CORB, Samesite Happy Coding !!\n"},{"id":33,"href":"/links/perspectives/","title":"Perspectives","section":"Links","content":" Perspectives # Section covering Business, project/programming perspectives\nLaws of Frugal Architecture Stick to boring Architecture Your tech stack is not the product Architecture anti-patterns Don\u0026amp;amp;rsquo;t call yourself a programmer Grasp Responsibility Patterns Things every programmer should know Guiding principles after 20 years of programming Programmers: Before you turn 40, get a plan B The New Business of AI (and How Its Different From Traditional Software) Hype driven Development Momentum vs Urgency in Software Project Management Data Science: Reality Doesn\u0026amp;amp;rsquo;t Meet Expectations Quantum computing for the very curious How to Speak (MIT) How Software Groups Rot: Legacy of the Expert Beginner What questions should systems architects ask before creating anything Basecamp for Personal Project Management Marketing for Engineers - Resources Approach to Exception Handling PRESALES (SE) LEADER? 10 THINGS YOU MUST BE DOING The Tail at Scale Long tail (99th percentile) latency Models for integrating data science teams within organizations Techniques and numbers for estimating system\u0026amp;amp;rsquo;s performance from first-principles The Amazon Builder\u0026amp;amp;rsquo;s library System Design Primer Telemetry Collection - Corelation in Latency Analysis Advice to Young kids by Stephen O\u0026amp;amp;rsquo;Grady Distributed Systems Reading List Awesome cold showers Behaviors to avoid in Software Architecture Role App Maintenance Cost Can Be Three Times Higher than Development Cost Foundational papers on distributed systems Dont end week with nothing Awesome Scalability - Collection of Articles around Performance, Scalability etc. Ego is the Enemy How to remember what you read? First Principles You are not Google 42 Lessons Learned in building production database Data structures implemented in JavaScript - I Data driven enterprises of 2025 Some Benefits of Simple Software Architecture Determining how Architectural decisions impact business via Value Use just one big Server When are Microservices a bad idea? The best engineers think like Investors not Builders CUPID principles Links for Aspiring CTO First principles thinking How Computer CPUs work A Distributed Systems Reading List 97 things, Pearls of wisdom for programmers collected from leading practitioners Legacy Modernization # Patterns of Legacy Modernization Documenting the Architecture # Arc42 - Open source Template for documenting the Software Architecture Arc42 \u0026#43; C4 - Example Structurizr - C4 Diagrams as Code Strategic Approach # How to build an effective technical strategy Writing an Engineering Strategy A curated and opinionated list of resources for Chief Technology Officers, with the emphasis on startups Best Websites for Programmers Fintech # Accounting for Computer Geeks Mifos X - Open source Financial Inclusion platform Moov.io - Tools/Libraries to integrate bank processing into their own software products like ISO8583 Awesome Fintech Resources Scheduling # Evidence based scheduling Capacity planning, Database scalability # Capacity planning for Web Application Scaling MySQL Web Hosting # How i run my Servers? Career # Checklist for Senior Engineer Power of Negative Thinking Curated Lists # Awesome Software Architecture Learning resources for curious programmer "},{"id":34,"href":"/links/tools/","title":"Tools","section":"Links","content":" General Purpose tools # Section covering useful tools for every day activities, Online learning etc.\nPlane - Open source alternative to JIRA ShareX - Screen capture, file sharing and productivity tools (Windows only) Dark Lang - Declarative platform to build serverless backend OBS Studio - Free and open source software for video recording and live streaming. Open source Wiki platform Open source 3D parametric modeler Backstage - an open platform for building developer portals Zoomit - screen zoom and annotation tool for technical presentations that include application demonstrations revealjs - HTML Presentation framework List of Self hosted software Open source Alternative to Heroku/Netlify for Self hosting A book of Secret knowledge - Collection of Useful tools Parsr - Transform PDF,Image into Structured data SOPS - Tool to secure secrets (JSON,YAML, INI etc.) via Command line and as GO library Windows Powertools for greater productivity Ex-googler\u0026amp;amp;rsquo;s list similar tools/techniques Recoll - Desktop full search tool Take potentially dangerous PDFs, office documents, or images and convert them to safe PDFs Briar - Secure peer to peer messaging on Android Open source alternative to Jira, slack,notion Useful Command line tools # Mise - version manager for multiple languages Guide to Linux Bash script Devbox - Quick shell with runtime environment without polluting laptop/desktop Shellcheck- a static analysis tool for shell scripts Useful online playgrounds by Julia Evans New list of useful Command line tools dsq- run sql queries against CSV,JSON,TSV, Web server logs exa - colorful alternative to ls duf - better disk usage/free utility Zmap - collection of open source tools for performing large-scale studies of the hosts and services that compose the public Internet. ripgrep - Recursively search directories for regex ripgrep-all - rigrep \u0026#43; PDFs, E-books, Office documents gron - Make JSON greppable xsv - fast command line CSV toolkit App that corrects previous Console command hstr - view bash shell history Lightening fast Code searching made easy Rewritten in Rust: Modern Alternatives of Command-Line Tools Broot - A better way to navigate directories fd - Alternative to Find bat - cat clone with wings Handy Linux networking tools rclone - manage files on cloud storage, Rsync for Cloud CPU-Z is a freeware system profiling and monitoring application for Microsoft Windows and Android Fselect - Find files with SQL-like queries HTTPie - Command line HTTP Client Visidata - A terminal spreadsheet multitool for discovering and arranging data Nginx - Tips for Sys Admins Avoiding the Top 10 NGINX Configuration Mistakes Listmonk - Open source newsletter and mailing list manager ATOP - Performance monitor for Linux (Better than htop) Below - Analyze Historical performance data for Linux Hyperfine - Generic Benchmarking tool Gmail backup tool gmvault - gmail backup tool Age - Simple File Encryption tool (Go) Encryption with Pass but Age as backend Yark - Archive youtube channels Linux related References # Linux Network level performance Parameters Understand grep, awk and sed Awk in 20 minutes Understandin Awk Visual guide to SSH Tunnels Web based interface for Servers Structured data tools # Structured data tools Hardware # 10 Best Lightweight Operating System for old Laptop How and why I stopped buying new laptops Useful spreadsheet formulas # Formulas for Personal finance Search tools # Grep app- Search across Git Repos Blogging platforms, RSS etc # Writefreely Yarr - Yet another feed aggregator Book of secret knowledge # Book of Secret Knowledge Guidance and Templates for Resume Building # Harward Uni. guidance on Resume building OpenResume - Professional, Free resume builder "},{"id":35,"href":"/posts/channelsforproducerconsumer/","title":"Using Channels for High performance Producer consumer implementation","section":"Posts","content":" Background # Recently, i got involved in assignment where in an application was facing issues with throughput. Expectation is to support more than 500 transactions per second while load testing results were indicating system was experiencing high latency beyond 100+ transactions per second.\nThis application is developed in .NET Framework + .NET Core and primarily uses Relational Database for persistence and has point to point integration (mainly over HTTP) with internal \u0026amp; external application(s).\nApproach # The high level approach decided to perform diagnostics and subsequent corrective action(s) were,\nBenchmark code that involves Database and take corrective action Identify tasks in hot code path that could potentially be decoupled or done in fire-n-forget mode. For point 2 from above, some of the tasks identified were,\nSending Email/SMS on myriad of events Integration with External Applications over HTTP Next task was to arrive at approach on how to perform them effectively outside of hot code path without incurring need of any additional resources (hardware or software)as far as possible. Accordingly, we had two options,\nPolling - Periodically polling database to check for occurance of event and then performing the action. Event Driven - Using Event notification feature of database (e.g. Listen/Notify in PostgreSQL or Change Notification/Advanced Queuing in Oracle). We decided to go with Event driven as,\nCleaner approach that doesn\u0026rsquo;t require perodically checking for events thus consuming a database connection and more code. We may have to have more than one such daemons to cater to different events in application. Post finalizing on event driven approach for gathering events, next task was to determine how to effectively send email/sms or any other HTTP requests considering that rate of arrival of events will not be matching rate of processing them. Also these\nSo what are the options available in .NET Ecosystem, Below are the ones i am aware of,\nChannels - High performance implementation of In-memory producer/consumer pattern. TPL Dataflow - Super set of Channels Library. Aimed at use cases where blocks of logic are to be linked together to same or different consumers and so on. Also all these features come with additional overheads. For the task at hand, functionality offered by Channels is sufficient to implement in-memory producer consumer pattern.\nSo we wrapped above event processing in a Windows service implemented as .NET Core WorkerService\nGeneric Implementation is as follows,\nEvent Generator - In practice, this class will be responsible for wiring up to receive events from database\nEvent Consumer which uses channels to process events in parallel\nRefer Gist here\nAdditionally, one may want to process requests out of order or asynchronously without using message queues. One such use case could be service to send Notifications where this service is exposed as Web API and it uses external service to dispatch notifications. For such scenarios, one can use back ground job in conjunction with Channels to process requests.\nBelow code shows a Web API that handles HTTP Requests and delegates actual task to background worker which is deployed as hosted service.\nRefer Gist here\nHowever, note that there are trade-offs vis-a-vis message queues with this approach. Notably, in case of Web server crash, the pending jobs in queue will be lost.\nSummary # Other languages (notably Channels in Go) have been providing out of the box implementation for in-memory producer with concurrent, parallel consumers. With Channels, .NET Ecosystem finally has construct that can be effectively put to use for high performance, concurrent use cases.\nUseful References, # Event Pattern in C# Gist on using Channels Happy Coding !!\n"},{"id":36,"href":"/links/oracle/","title":"Oracle","section":"Links","content":" Oracle Database # Performance, Best Practices # Connection Strategies for Database Applications Using High-Speed Data Loading and Rolling Window Operations with Partitioning Designing Applications for Oracle Real-World Performance Best Practices for Extreme Performance with Oracle Data Warehousing Blog on Oracle Performance troubleshooting Using PL/SQL Bulk processing features Auditing tables using Oracle Flashback data archive instead of triggers Flashback Data Archive to record changes to Table Bulk processing with PL/SQL Bulk Processing with BULK COLLECT and FORALL Primer on Oracle Partitioning Database Core performance principles - Deck Database insert \u0026amp;amp; referential integrity - Performance On Connection Pools, Cursor Differentiation, and Optimal Ordering Analytical Functions Overview About Materialized Views How to find Slow SQL Using External Tables and Table Clusters in Oracle Oracle DBA - Application Tuning Replacing Kafka use cases with Oracle Advanced queues in modern applications SQL Tips you can\u0026amp;amp;rsquo;t do without Change Data Capture # Nice writeup on options to do CDC in Oracle Database Integrating Oracle and Kafka Videos # Real world performance video series Oracle LiveLabs How to:Analyze AWR Report 5 Minutes Demo: Using Liquibase in SQLcl to version Oracle Database Analytic SQL for Developers - Free course Connection Pooling and SmartDB Oracle Database for Developers - Training How to Create an Execution plan? Machine learning in Autonomous Database Utilities, Tools # OraTOTP, Free tool to enable 2 factor authentication Audit table Generator for Oracle Tables Swingbench, free load generator (and benchmarks) designed to stress test an Oracle database (12c, 18c, 19c). Create Excel file PL/SQL "},{"id":37,"href":"/links/nosql/","title":"NOSQL","section":"Links","content":" NOSQL Databases # MongoDB # FerretDB - MongoDB Interface with underlying PostgreSQL as database engine Amazon DynamoDB # Data Modelling in DynamoDB Must follow Twitter handle of Rick Houlihan Best Practices for Secondary Indexes with DynamoDB Apache Cassandra # 7 mistakes when using Apache Cassandra Apache Geode # How Mastercard fights fraud with Apache Geode Apache Pinot # Pinot- Enabling Real-time Analytics @ linkedin Redis # About Redis DragonflyDB - Alternative to Redis Redis High Availability Redis Cluster KeyDB is a high performance fork of Redis with a focus on multithreading, memory efficiency, and high throughput. In addition to multithreading RediSQL, fastest, simplest, in-memory SQL Redisearch - Redis powered Search Engine JuiceFS - POSIX File System with Redis or S3 as backend SSDB - A fast NoSQL database, an alternative to Redis Comparing REDIS and Memcached Oracle Coherence # Oracle Coherence Community Edition Full text Search Engines # Deep Dive into Querying Elasticsearch. Filter vs Query. Full-text search Engine for Low-latency Computation over large data sets Open source Full text Search Engine Sonic - Fast, lightweight \u0026amp;amp; schema-less search backend "},{"id":38,"href":"/links/mysql/","title":"MySQL","section":"Links","content":" MySQL # Links # Query analytics for the day-to-day developer with MySQL 8.0 Schema Change Management for MySQL Temporal Data tables in MariaDB MySQL 8.0 Indexes, Histograms, and Other Ways to Speed Up Your Queries Maxwell - MySQL to Kafka change data capture LetsEncrypt setup for MariaDB How LetsEncrypt has built Next Gen Database Servers 18 things you can do to remove mysql Bottlenecks due to High traffic MySQL from Developer\u0026amp;amp;rsquo;s perspective Replication in MySQL Interesting libraries, extensions # Distributed job-queue built specifically for queuing and executing heavy SQL read jobs asynchronously. Supports MySQL and Postgres Orchestrator - Replication topology and high availability Vitess,a Distributed MySQL # Massively scaling MySQL database How Slack uses Vitess "},{"id":39,"href":"/links/databases/","title":"Databases","section":"Links","content":" Database # Knowledge base around general database related topics.\nGeneral Links # Prisma\u0026amp;amp;rsquo;s Data Guide - A growing library of articles focused on making databases more approachable. Query optimization guide Database performance for Developers Heimdall data -Database scale-out without Application changes Database of databases Modern SQL in databases Eventual consistency by Werner Vogels Amazon Aurora ascendant: How we designed a cloud-native relational database - All Things Distributed Options for scaling from 1 to 100,000 tenants Amazon Aurora: design considerations for high throughput cloud-native relational databases | the morning paper NOSQL - Key Points Criteria for Choosing Data store Building Real Time Analytics APIs at Scale Streaming Database Changes with Debezium Why you should pick strong consistency, whenever possible Change Data Capture, Outbox and Event Sourcing Debezium Engine - setup without Apache Kafka Debezium without kafka connect Using Streamsets for CDC From Oracle to Other destinations Transactions in Google Spanner Things I Wished More Developers Knew About Databases Interactive Book about SQL SQL Interview Questions Hadoop or Laptop The lightweight, distributed relational database built on SQLite Optimizing SQL Queries, Regardless of Platform How to do Data Modelling the right way Primer on Database Replication Connection pool sizing for databases Some SQL tricks from Application DBA Best Practices while writing SQL Using checksums to verify syncing 100M database records How to populate a table with 1 million records using single query How databases optimize Sub-queries Approaches to database migration Tigetbeetle - Fast financial accounting database Opinionated thoughts on SQL Databases Tools Collection # DBMS Tools OctoSQL - Query, Join CSV with Postgresql/mysql from Command line TSBS - tool to benchmark bulk load performance and query execution performance. Goose - Database schema migrations HammerDB - Benchmarking Suite for databases Sysbench - Scriptable database and system performance benchmark Soda core - Data schema checks, for Quality, as code Readyset - MySQL and Postgres wire-compatible caching layer that sits in front of existing databases to speed up queries and horizontally scale read throughput. Data Analytics # Understanding avro, parquet and ORC Guidance on Data Visualizations Simple data pipeline Powertools Cube.dev - Open source Headless BI platform Evidence.dev - BI as Code - SQL \u0026#43; Markdown to generate Reports Apache spark defined Getting started with Spark in Python About Data Mesh Architecture Data mesh vs. Data Fabric Emerging Architectures for Modern Data Infrastructure Data Visualization/Exploration platforms Comparion Matrix Supercharging Apache Superset Snowplow - Cloud Native Behavioral data engine (e.g. User Analytics) Redash - Collaboration, dashboards Why data culture matters Designing a data transformation that delivers value right from the beginning List of Computational Data Analysis Workflow Systems Data Visualization framework for Python Analytics Academy by Segment Analytics Whitepapers by Sisense SQL Analytics Training A Beginner\u0026amp;amp;rsquo;s Guide to Data Engineering - 3-part series Chart types and its usage Rudder - Open source Customer Data Infrastructure Catalog of Widgets for Data Visualization Open source OLAP Database Modern Data stack guide by Castor Data Stack of 1mg A Unified Data Infrastructure Architecture Data and AI Product Landscape Transformations for DWH using DBT Awesome list of Business Intelligence Tools Article Series on Open source Data Analytics Stack (Postgres,Meltano, Airflow, dbt and Superset) Posthog - open source product analytics platform Typical Analytics Stack Flat Data - Scheduled Data Download on GitHub Actions in Repository and visualization Nocodb - Turn *MySQL/PostgreSQL data in smart Spreadsheet Real time data analysis with Apache Pinot and kafka UUIds are bad for performance Noria - Caching and updating Relational query results Differential Datalog - Language for incremental computation Using NanoIDs (not longer UUID) for public APis Duck DB # DuckDB - Embeddable OLAP DBMS SQL Workbench - run Duckdb on WASM DuckDB - Connect and join on external databases ETL,ELT, Database-as-a-queue, Evolutionary Practices # All about ETL Airbyte-Open source ELT Database CI/CD practices using Redshift Awesome Apache Airflow A Python library for building data applications: ETL, ML, Data Pipelines, and more. A modern data workflow platform Databus - Change Data capture System from Linkedin Dolt - Git for Data GridDB - next generation database for IoT \u0026amp;amp; big data with both NoSQL interface \u0026amp;amp; SQL Interface. Compressing data with Parquet Lance - alternate columnar, compressed format for ML Mara pipelines - Opinionated ETL framework Enso - Interactive Data Workflow builder with no coding Database for Event Sourcing What are Data Contracts Centrifuge - Database as a Queue Database scaling # Scaling TIDB to 1 million QPS Sharding a database MySQL Sharding at Quora CUID-Collision-resistant ids optimized for horizontal scaling and performance. Data Discovery # OpenMetadata - Data Discovery, Lineage, Data Quality Evaluation of Data Discovery Platforms Data Discovery at Shopify Great Expectations - Data Documentation and Profiling tool Database Migration Practices # Zero downtime database migrations Stripe - Database Online migration at scale using dual writes How big companies migrate from one database to another without losing data i.e database independent? Efficiently diff rows across two different databases. Metadata Management # Growing importance of Metadata Management Systems SQLite # Query against multiple SQLite databases using ATTACH Command Online SQLite Fiddle Why you should be using SQLITE(2023) Performance tuning settings Pocketbase - SQlite database with Go-based Wrapper to expose API Scaling SQLITE to 4M QPS on Single Server Streaming S3 Replication for SQLite lightweight, distributed relational database built on SQLite Interesting use cases for SQLITE Hosting SQLite databases on Github Pages Joining CSV and JSON data with an in-memory SQLite database Baked Data Architecture Pattern -DB side by side Web App Cron based backups for SQLITE Data Security, GDPR # Tool for Sensitive Data Detection from Capital one Data bunker - Secure storage for personal records built to comply with GDPR Search # Google Code Search using Inverted Index Open source Google Code Search tool in Go Manticore Search - easy to use open source fast database for search ZincSearch - lightweight alternative to ElasticSearch Why OpenSearch, fork of ElasticSearch Peer to peer web search and Intranet Search Appliance Get Started with Opensearch Capacity Planning # About Oracle Capacity Planning Guidelines for SQL Server Capacity Planning Database Documentation # [Schema spy - ER Diagram, Metadata Reports][https://github.com/schemaspy/schemaspy] Data Engineering # Concepts Choosing a Data Catalog Awesome Data Catalog Create a Serverless Data Lake on AWS and Migrate your On-Prem Data to it Data Engineering How tos- List of Curated Articles/Videos Guide to Data lake, Data lake house Data Lake - Solution Patterns What is delta lake house? Poor man\u0026amp;amp;rsquo;s Data lake with Duckdb Data Model for Managing Collaborative Editing of Data Dictionary of databases # Database of Databases "},{"id":40,"href":"/projects/","title":"About","section":"","content":"Below are some of my project(s),\nTracfee, One stop for Tutors to manage students, track fees. Developed as SPA in VueJS + Quasar using API in Golang, Oracle Database and hosted on Netlify.\nRSS APP RSS Reader app, to be used in lieu of Google Reader. Developed in Python with MongoDB as database.\n"},{"id":41,"href":"/about/","title":"About","section":"","content":"I am a software developer, currently working at @worldlineglobal. This is my personal site where i share helpful content (gathered or authored) on Technology (and other topics).\nI appreciate any ideas/suggestions you have on how I can improve this site.\n"},{"id":42,"href":"/links/dotnet/","title":"Programming Languages - .NET","section":"Links","content":" Microsoft .NET # Platform where i have spent most time till now.\nGeneral Links # What is .NET? by Scott Hanselman Async in Depth Using Async/Await in WCF or ASMX with AsyncEx Comparing Async/Await with GoRoutines .NET Presentations - Events in a Box Building Microservices in .NET Materialized View Pattern for Cross Service Queries Oracle DB and .NET - Optimizing Real-World Performance with Static Connection Pools Clean Code concepts and tools adapted for .NET Multiple ways how to limit parallel tasks processing Parallel programming in .NET Clean Architecture in .NET Youre (probably still) using HttpClient wrong and it is destabilizing your software Async/Await - Guidance \u0026amp;amp; Best Practices in Asynchronous Programming Async/Await - Deep dive for Windows based Async I/O One more look at why Async/Await, what happens underneath Implement a producer-consumer dataflow pattern Use Arrays of Blocking Collections in a Pipeline Performance related # Web forms, Asynchronous operations and its performance impact List of Awesome Resources Using System.Diagnostics.StopWatch.GetTimeStamp for accurate duration C# Job Queues with TPL Dataflow and Failure Handling Know about Threadpool, types of Threads in CLR and changing them to improve performance Work flow of diagnosing memory performance issues ***Contention, poor performance, and deadlocks when you make calls to Web services from an ASP.NET application .NET GC - Memory fundamentals Debug high CPU usage in .NET Core Measure performance of High frequency events in .NET Core App .NET Core debug memory leak, High CPU Usage, Deadlock TCP Connection Pool and how it works in .NET Framework/.NET Core Using max number of worker threads using Semaphore Performance tuning for .NET Core API # A light-weight REST API development framework for ASP.NET 6 and newer. Starter kit # .NET Core Starter kit ASP.NET Web forms # What not to do in ASP.NET, and what to do instead Use Task.Run at the invocation, not in the implementation Take Advantage of ASP.NET Built-in Features to Fend Off Web Attacks Blazor for Web Form Developers Windows Forms # Task.run vs. BackgroundWorker Tools, Libraries # Coravel - In-memory Task Scheduling , Queueing Library Generate PDF using Scriban and Playwright .NET Playground RestSharp - REST HTTP Client Ocelot - API Gateway AsyncAwaitBestPractices Flurl Distributed transaction solution in micro-service base on eventually consistency, also an eventbus with Outbox pattern Simple Swiss Army knife for http/https troubleshooting and profiling Event sourcing using variety of stores like AMQP, database Feature Management library for ASP.NET Core General Checklist for Projects Open Source ing tool for .NET Core/.NET Framework that helps your application generate document-like reports Open source database, Optimized for Event sourcing bflat - No-frills, standalone compiler for .net Hashids.NET - Generate Youtube-like hashes (short codes) from one or more numbers Rate Limiting Library from Microsoft Task Queue/Scheduling tools # Hangfire Tempus Background tasks with hosted services in ASP.NET Core Rebus - Smart end-points, dumb pipes service bus for .net .NET Core # Approach for Incremental Migration from ASP.NET to ASP.NET Core .NET Portability Analyzer ASP.NET Core Architecture Overview ASP.NET Core Performance Best Practices Diagnosing Issues Under Load Of WebAPI App Migrated To ASP.NET Core On Linux Model binding in ASP.NET core HttpClient Connection Pooling in .NET Core An Introduction to System.Threading.Channels Working with Channels With Stephen Toub BackgroundService Gotcha: Application Lifetime AWS Porting Assistant for .NET Sample of Micro services in .NET Core CoreWCF (SOAP,TCP, WS-HTTP support) on .NET Core ASP.NET Web API Versioning Samples of ASP.NET Core you can use Step by Step OpenTelemetry in .NET Core Techempower performance benchmarks Security # OWASP - Top Ten Vulnerabilities Microsoft RESTler-Security testing using Automated Fuzzing Security Code Scan in .NET Networking # .NET 5 Networking Improvements Understanding WebRequest Problems and Exceptions Twitter Handles # Scott Hanselman General # .NET Conf 2021 Videos, Slides etc. Nuke - Alternate (to MSBUILD) Build system for .NET "},{"id":43,"href":"/links/go/","title":"Programming Languages - Go","section":"Links","content":" Go Language # My current Favorite Language\nArticles, E-books # About Go - Compiler, packaging etc. When (and when not to) to use Generics in Go Ver. 1.8 High performance GO Workshop Learnings from Production usage of Go Thoughts on Go performance optimization Effective Go Handling 1M websockets connections in Go Notes on Go language Standard Go Project Layout 10 things you (probably) don\u0026amp;amp;rsquo;t know about Go Useful patterns in Go Interesting ways of using Go channels How i writer web services in Go Embed static file(s) in Go Executable and expose over HTTP Using go:embed in Go 1.16 Go Useful patterns by Roberto Clapis Strategies for Working with Message Queues Continuous build \u0026amp;amp; Testing using Go Convey Why American Express chose Go Thoughts on Performance Optimizations in Go by Damian Gryski Quick list of performance improvement targets in Go 10 things you probably don\u0026amp;amp;rsquo;t know about Go Learn Go with test-driven development Cancellable Pipelines in Go Running Go binary in Docker Go for Cloud - Tips and Techniques Why and what to instrument in Go Web Apps Continuous Profiling of Go programs How I write HTTP services in 2024 Go Concurrency - Singleflight, Bounded concurrency, Weighted bounded concurrency Why you should be using errgroup withcontext in Golang WebAssembly in Go gRPC in Go Go: Discovery of the Trace Package Tracing in production for Latency Rust for Go Developers Rust vs Go - When to use which Example of how to let only one Goroutine do the task while letting others wait for it useful in case of reading data from DB to be cached How to leverage AWS Lambda timeouts with Go context cancellation Design philosophy TLS and Go Effectively using Systemd for setting up HTTP Server Useful Code patterns Libraries, Tools # HTMX \u0026#43; Go in single binary Staticcheck - The advaned Go linter Scripting with Go Right way to check weather Generate Go Code for Database / SQL for Mysql and PostgreSQL Why SQLc is better approach than ORM Golang style guide by Uber ORM to Model and Traversal of Data as a Graph structure Gops-A tool to list and diagnose Go processes currently running on your system Pocketbase - SQlite database with Go-based Wrapper to expose API Wails - Electron like environment in Go Visualize call graph of a Go program using dot (Graphviz) Semgrep - Lightweight static code analysis focussed on Security Draw Application diagrams using Go A Go metrics interface with fast buffered metrics and third party reporters Hey - HTTP load generator, ApacheBench (ab) replacement Go-metrics - library for exporting performance and runtime metrics to external metrics systems (i.e. statsite, statsd) Progressive Web App (PWA) with WebAssembly in Go GoPlus - The Go\u0026#43; language for data science Notes on Profiling in Go Go-Micro - Web and RPC Framework for Microservices in Go Approach on project Structure in Go Zero Allocation JSON logger Use Makefile with Go Review of HTTP Routers Library over Financial Markets i.e. Yahoo Finance etc. Excelsize - pure Go library providing a set of functions that allow you to write to and read from XLSX / XLSM / XLTM / XLTX files Benthos - Simplified stream processing with built-in connectors Service weaver - Write Modular Monolith Apps Xo - Tool to Generate DB Specific Go Code Learning # Learn go with tests Task queues # Queueing with Update..skip locked Machinery - Asynchronous task queue/job queue Bleve - Full text Search Engine Event Sourcing, pub/sub using AMQP/SQL/Channels Hydra - OAuth 2.0 Server Temporal - Scalable orchestration platform Distributed job-queue built specifically for queuing and executing heavy SQL read jobs asynchronously. Supports MySQL and Postgres. Tunny - Library to manage pool of goroutines to limit incoming work Scheduler library for Go Web scraping, downloader # Elegant Scraper and Crawler Framework for Golang Fast, simple and clean video downloader Videos, Talks # Best practices for Industrial Programming - by Peter Bourgon Profiling \u0026amp;amp; Optimizing in Go Rethinking classical Concurrency patterns Justforfunc: Programming in Go A Channel Compendium Visualize Concurrency in Go Real-world systems in Go Host Free Go Web app on Netlify Zen of Go - Ten engineering values for writing simple, readable, maintainable Go code [Ultimate Go Study Guide](https://githu b.com/hoanhan101/ultimate-go) Code snippets # Web Service in Go - Code with Best practices Remote service with Retries Curated list of design patterns implemented in Go Gophercises - Exercises for Go Developers Practical concurrency guide in Go, communication by channels, patterns Sample DDD Project with Code Podcasts # Go Time Performance Analysis # Example of Performance analysis of Go Program using benchmarks "},{"id":44,"href":"/links/testing/","title":"Programming - Testing","section":"Links","content":" Testing # Links # Testing in 2021 How different software companies do testing HTTP(S) benchmark tools, testing/debugging, \u0026amp;amp; restAPI (RESTful) Toxiproxy - A TCP proxy to simulate network and system conditions for chaos and resiliency testing Papercut SMTP - Test Email delivery during development Malabi -Trace based testing in JavaScript AB Testing 101 API Test Client # Bruno Load Testing # K6 - Load testing tool Vegeta - HTTP load testing tool and library. Bombardier - Fast cross-platform HTTP benchmarking tool written in Go Plow - A high-performance HTTP benchmarking tool with real-time web UI Hey - HTTP load generator, ApacheBench (ab) replacement Collection of HTTP(S) benchmark tools, testing/debugging, \u0026amp;amp; restAPI (RESTful) Light weight cross-platform test automation "},{"id":45,"href":"/links/planguages/","title":"Programming Languages","section":"Links","content":" Programming Languages # Links # Hello world in every Programming Language General # You are not Google Production Launch Checklist Things I Learnt The Hard Way in 30 Years of Software Development A collection of (mostly) technical things every software developer should know Startup idea Checklist System Design Primer Developer Roadmaps Why our team cancelled our move to microservices How Does HTTPS Work? RSA Encryption Explained How do you cut a monolith in half? Containers # Awesome Collection of Docker Compose Recipes Podman Desktop - Alternative to Docker Desktop "},{"id":46,"href":"/links/programming/","title":"Programming","section":"Links","content":" System Design, Architecture # Links covering concepts and approaches around Distributed Systems, DevOps, Observability etc.\nArchitectural Case studies # Temenos Serverless banking at Scale @ AWS using CQRS leveraging RDS and DynamoDB Temenos @ AWS Architecture Diagrams Architecture for Generations Ubers Domain-Oriented Microservice Architecture Books on Architecture,Design # Software Architecture Patterns by Mark Richards A comprehensive list of books on Software Architecture. Introduction to architecting systems for scale. Strategies/Approaches # Serving a billion web requests with boring code Professional programming resources Rob Pikes 5 Rules of Programming Modules, monoliths, and microservices The macro problem with Microservices Break Monolith into Microservices Steps to migrate from Monolith to Microservices Distributed architecture concepts I learned while building a large payments system Video: Developing Asynchronous Microservices  Chris Richardson Collection of Software development Videos Slides Managing Data Consistency in Microservices Architecture Reliable Microservices Data Exchange With the Outbox Pattern Scaling to 100k Users Monolith - Modular Approach You dont need Microservices Event Modelling - Approach Ready for changes with Hexagonal Architecture How to fix Overloaded Web server How gov.uk reliably sends SMS messages using multiple providers Rule of thumbs for Architecture Scalability About Structure of Design document Important Aspects about Circuit breaker from Shopify CRDTs for Synchronization Asynchronous transaction processing @ Facebook Evolutionary Database Design Scaling with Common Sense by Zerodha Guidelines for Command line interface Azure Well-Architected Framework Change Data Capture, Strangler fig and Saga Patterns Gateway pattern to encapsulate integration with external systems Why refactoring? OpenFeature - Standardizing Feature Flagging for Everyone API Development, Security, Cryptography # Mockoon - Run Mock APIs locally Bruno - Opensource IDE For Exploring and Testing Api\u0026amp;amp;rsquo;s REST API Guidelines from Microsoft Open Source Vulnerability Management Equinors API Strategy and Guidelines Checklist of Web APIs Guidelines for designing better APIs API Security Checklist Googles API Design Guidelines API Design for Serverless Apps OWASP - Top Ten Vulnerabilities API Security Checklist libsodium - Easy to use cryptography Schannel in Windows for Strong Ciphers/Cryptography Training # Teach yourself Computer Science Collection of Video Courses on Computer Science Learn by doing - You dont need another MOOC Distributed Systems # Build your own (insert technology here) Kubernetes for Everyone The Service Mesh: What Every Software Engineer Needs to Know about the Worlds Most Over-Hyped Technology Very Brief intro to Container Orchestrators E-book kubernetes Up \u0026amp;amp; Running Class materials for a distributed systems lecture series Containers - Training resources Distributed Systems Cheat Sheet Microservices  architecture nihilism in minimalisms clothes Microservices, pl. dont Disasters from Microservices world Automation # Microsoft Power Automate Desktop - Free Windows 10 Desktop Automation Automate the Boring Stuff with Python Four bad ways to use RPA Data Science at the Command line RobotFramework - Open source Test Automation and RPA WASP - Windows Automation Snapin for PowerShell Web API rate limiting Tools, Libraries # (\nCollection of TILs (Today I learned) Miller - awk, sed, cut, join, and sort for name-indexed data such as CSV, TSV, and tabular JSON Digital services offerings from within European union Six things I wish we had known about scaling Awesome Design tools Regex Repository SpiderFoot, the most complete OSINT collection and reconnaissance tool Analyze TCP Connections by proxy TCP/IP -Why your websites should be 14KB in size Cloud Architecture Diagrams Free Online Cloud Architecture Diagram Tool Online Flowcharts, UML diagrams Embeddable charts using DataWrapper Figma - Design and prototype builder Open source Voice chat Zulip - Open source alternative to Slack Open source Video Conferencing Open network for secure, decentralized communication Alternatives for Local Kubernetes development Jami - tool for Encrypted Audio/Video calls Keycloak - Open source Identity and Access Management Ory - Next Gen Identity Management Diagram as Code (Python) Virtual whiteboard for sketching hand-drawn like diagrams with Collaboration*** Syncthing - Free, OSS, File synchronization across devices Library for Code Scanning Across GO, C#, Java etc. Open source Project Management Software Blueboat - Serverless infrastructure for On-premise deployment Text to Timeline (Gantt) chart Open source Pipelining, workflows # Pipelines/workflow frameworks Workflow Engines List of ETL frameworks Streaming frameworks Extendable Workflow Automation tool in NodeJS Nodered - Low code event driven pipelines Security # Definitive guide to key management7 A deep dive in CyberSecurity OWASP ZAP- Free Security Testing for Web Application Web Application Security Testing Understanding OAuth and OpenID Connect OWASP Cheat Sheet Series Microsoft App Inspector Open Policy Agent - General purpose Policy Engine Teler - Tool for Real time HTTP Intrusion detection Joern - platform for analyzing source code, bytecode, and binary executables Syft - Software Bill of Materials (SBOM) generator for vulnerability scanning Devops/Monitoring # Open source Alerts Management Trivy - Scanner for vulnerabilities in container images, file systems, and Git repositories, as well as for configuration issues and hard-coded secrets Impact of Architecture on DevOps Run CI/CD pipeline locally with Dagger List of how organizations do SRE (Publicly available) Open source API Designer with CI/CD Workflow Microsoft Azure - DevOps Checklist Hashicorp Waypoint - easy way to build, deploy and release applications Zabbix, Time Series Data and TimescaleDB  Zabbix Blog PromScale - Observability backend powered by Timescaledb \u0026amp;amp; PostgreSQL How to Create and Manage CRON Jobs lazydocker - Docker mgmt tool for linux Whats in a CI pipeline Repository of DevOps Questions n Answers Google Incident Response Framework Dockerfile Best Practices Github Workflow - Test them locally using Act Code coverage best practices from Google A terminal UI for tshark, inspired by Wireshark Developer playbook approach by Hackney council Nice content on Ansible Performance profiling using Open source Pyroscope Server Act - Run Github Actions locally Approach to Uptime Guarantees Observability # OpenObserve - Elasticsearch/Splunk/Datadog alternative for logs, metrics, traces Ntfy - OSS Server \u0026#43; Android App to send \u0026amp;amp; receive notification on Desktop/Android App Tips for Analyzing logs Decision guide on Tooling Prometheus and Cardinality of Metric All about Log Aggregation Observability in 2022 Observability @ Cloudflare What to Monitor and Metrics to collect for Web App with Background Jobs What was observability again? Dashboard design best practices Infrastructure Monitoring with Postgres Tracing, Fast and Slow  roguelynn OpenTelemetry in 2023 Course on using Opentelemetry Opentelemetry Overview Techniques for Monitoring Web services OpenTelemetry, Distributed Tracing, W3c Trace Context Tracing at Slack using Kafka Distributed tracing covering Client (Mobile App) tracing at slack Metrics, tracing, and logging Open source infrastructure and application Monitoring Opstrace - OSS alternative to Datadog,SignalFX Monitoring your own infrastructure using Grafana, InfluxDB, and CollectD Tool to extract whitebox monitoring data from application logs for collection in a timeseries database Percona Monitoring \u0026amp;amp; Mgmt - Open Source Software for MySQL/MongoDB/PostgreSQL Monitoring Monitoring and Observability With USE and RED Horizontally scalable storage for Prometheus Thanos - Highly available Prometheus setup with long term storage capabilities. Monitoring with VictoriaMetrics Healthchecks.io -Simple and Effective Cron Job Monitoring Decks on Prometheus deep dive, OpenMetrics Improving Observability with AWS App Mesh Metrics to track for your API Details about Cortex vs Thanos, Grafana Loki and Tempo Get started with Prometheus, Grafana and loki How to build a scalable prometheus architecture Introduction to FluentBit - Logs n Metrics Processor Distributed tracing vs. Logging Identifying disk i/o bottlenecks in Linux Signoz - Open Source Opentelemetry based Observability platform Centralized logging with Signoz What is eBPF \u0026amp;amp; its application in Observability Database Reliability Engineering(ebook) Distributed tracing using Tempo, OpenTelemetry and Grafana Cloud Skywalking - Open source Application Performance Monitoring tool HTTP Toolkit - Freemium HTTP Interceptor toolkit Comparing Open source log collectors Fluentd, Logstash, Fluentbit Observing Network Traffic with Open source tools Distributed messaging, Streams # Iggy-persistent message streaming platform written in Rust, supporting QUIC, TCP and HTTP transport protocols, capable of processing millions of messages per second. SmoothMQ - Drop-in replacement for SQS With SQlite backend Comparison - BlazingMQ, RabbitMQ and Kafka Redpanda - Alternative to Kafka, Streaming Platform VerneMQ - A distributed MQTT message broker based on Erlang/OTP Kafka - Capacity Planning Why Kafka Is so Fast RabbitMQ vs Kafka - architec tural perspsective RabbitMQ vs Kafka A comparison between RabbitMQ and Apache Kafka Comparing RabbitMQ and Kafka Strategies for Working with Message Queues All about Queues Benefits of Message Queues Reasons to use Message Queues NSQ - a realtime distributed messaging platform designed to operate at scale Kafka Without Zookeeper - A Sneak peak Oracle Advanced Queues Instrumenting distributed systems for operational visibility Microservices Antipattern - Queue Explosion Trying out durable, replicated quorum queues in RabbitMQ ZeroMQ - Universal Messaging Library Comparing Techniques for Communicating Between HTTP Services Rust # Getting started with Rust A half-hour to learn Rust Tour of Rust PHP # PHP: The Right way Email Server # Setup Email server using Docker mailserver Networking # All about Load Balancers Nerdy Videos # Contalks Real world Architectures # Deployment @ Wikimedia Interview Questions # System Design 101 through Diagrams 10 API Product Manager Interview Questions API # REST API Best practices Comparing API Architectural Styles Eventcatalog - Open source tool to document event driven Architectures Interesting free services # Grist spreadsheets - alternative to Airtable Penpot - prototyping tool C, C++ # Learnings from C Lang "},{"id":47,"href":"/links/home/","title":"Useful Links","section":"Links","content":" Below is list of curated links for various technical topics, # Awesome - de-facto repository covering wide range of technical topics. Awesome list of self hosted software List of Open source Alternatives to SASS Online Learning Perspectives Distributed Systems Design, Architecture Testing UI/UX Languages Go .NET/C# Python Databases MySQL Oracle PostgreSQL NoSQL Cloud Tech AI/Machine Learning Machine Learning General Purpose tools Must follow Community Sites # Hacker news Lobsters A list of SaaS, PaaS and IaaS offerings that have free tiers of interest to devops and infradev Miscellaneous # OSS alternatives to Popular tools/Systems Open Source alternative tools Attention is all Manager need - Techniques and processes Gokey - Derived random passwords based on Master password Useful tools for Windows by Scott Hanselman Library of Free music Ergonomic Home office setup Consider upgrading a few PC/laptop Components like SSD How to use Google like a pro Pick Parts.Build Your PC.Compare And Share Privacy - Nice Overview and content Privacy tools for everyday user Hackers Diet Beam - Blog for a Project or Organization List of Greatest Novels of all time Cryptonomics by Tyler Cowen All About Public key Infrastructure(PKI) Most data work seems fundamentally Worthless 100 tips for Better Life "},{"id":48,"href":"/posts/dotnetstandard/","title":"Using .NET standard Assembly in .NET core and .NET Framework","section":"Posts","content":" Background # One of the key project(s) at my current organization is developed on .NET 4.6.1. It is developed as Modular Monolith. As part of it\u0026rsquo;s functionality, it supports different channels like Mobiles, Terminals and Web. For the Web channel, there was need to develop a Web application with,\nHigh availability Lightweight, High throughput (Need to support few thousand(s) active users) Accordingly, we have been exploring developing this Web Application in .NET core 3.1. However, it also means that we will have to use class libraries, targeted at .NET framework 4.6.1, in .NET core and vice-versa. How can this be done?\n.NET Standard to the rescue !!\n.Net Standard is a standard that enabled development of portable libraries usable across .NET versions.\nBelow is approach adopted to create usable libraries across .NET framework \u0026amp; .NET Core.\n.NET \u0026amp; IDE versions used are,\n.Net Framework 4.6.1 .Net core 3.1 Visual Studio 2015 - for .NET Framework 4.6.1 development Visual Studio Code - For .NET core development Step 1 -\nCreate a library that targets .NET Standard. Refer to Table on Implementation Support to decide on version that can be targetted at. In my case, it was 2.0 (Remember that higher the version, more APIs will be available to use). Do check .NET API browser, which lists API available with each version.\nUsing .NET core, use below command, dotnet new classlib \u0026lt;name\u0026gt;\nNote that, by default csproj file generated targets .NET Standard, but do confirm by checking in \u0026lt;name\u0026gt;.csproj file, It should have entry like,\n\u0026lt;PropertyGroup\u0026gt; \u0026lt;TargetFramework\u0026gt;netstandard2.0\u0026lt;/TargetFramework\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Change the version of .NET Standard if required.\nAdd necessary code to the library and build it using, dotnet build\nCreate a Nuget Package using, dotnet pack This will generate \u0026lt;name\u0026gt;1.0.0.nupkg package in bin\\debug folder (assuming that you are using Debug mode)\nStep 2 -\nLets consume this library from console Application, using .NET Framework 4.6.1, in Visual Studio 2015.\nCreate New Console Application and ensure that it is targeted at .NET Framework 4.6.1 or Higher.\nBefore consuming .NET standard library, few steps are needed since VS 2015 only has legacy support for consuming .NET core artifacts also it does not have latest version of Nuget, so lets do below,\nInstall NuGet 3.6.0 or higher for VS 2015 from NuGet\u0026amp;amp;rsquo;s download site Install the \u0026ldquo;.NET Standard Support for Visual Studio 2015\u0026rdquo; from here Open the csproj file in Text Editor and add \u0026lt;ImplicitlyExpandDesignTimeFacades\u0026gt; tag as shown in below example, \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;utf-8\u0026#34;?\u0026gt; \u0026lt;Project ToolsVersion=\u0026#34;12.0\u0026#34; DefaultTargets=\u0026#34;Build\u0026#34; xmlns=\u0026#34;http://schemas.microsoft.com/developer/msbuild/2003\u0026#34;\u0026gt; \u0026lt;PropertyGroup\u0026gt; \u0026lt;Configuration Condition=\u0026#34; \u0026#39;$(Configuration)\u0026#39; == \u0026#39;\u0026#39; \u0026#34;\u0026gt;Debug\u0026lt;/Configuration\u0026gt; \u0026lt;Platform Condition=\u0026#34; \u0026#39;$(Platform)\u0026#39; == \u0026#39;\u0026#39; \u0026#34;\u0026gt;AnyCPU\u0026lt;/Platform\u0026gt; \u0026lt;ProjectGuid\u0026gt;{75678902-8224-4222-BB33-756784B2FA29}\u0026lt;/ProjectGuid\u0026gt; \u0026lt;OutputType\u0026gt;Library\u0026lt;/OutputType\u0026gt; \u0026lt;RootNamespace\u0026gt;FooBar\u0026lt;/RootNamespace\u0026gt; \u0026lt;AssemblyName\u0026gt;FooBar\u0026lt;/AssemblyName\u0026gt; \u0026lt;TargetFrameworkVersion\u0026gt;v4.6.1\u0026lt;/TargetFrameworkVersion\u0026gt; ... \u0026lt;ImplicitlyExpandDesignTimeFacades\u0026gt;false\u0026lt;/ImplicitlyExpandDesignTimeFacades\u0026gt; \u0026lt;/PropertyGroup\u0026gt; Post update to file, VS 2015 will prompt to reload the project.\nNow we are set to consume .NET standard library, authored in .NET Core, in this project.\nStep 3 -\nWithin VS 2015, Goto Nuget Console and install the package created earlier. This link has steps to consume local nuget package(s). Happy Coding !!\n"},{"id":49,"href":"/posts/dddnotes/","title":"Dddnotes","section":"Posts","content":"Even Eric Evans explicitly states that DDD isn\u0026rsquo;t suitable for problems when there\u0026rsquo;s substantial technical complexity, but little business domain complexity. Using DDD is most beneficial when the complexity of the domain makes it challenging for the domain experts to communicate their needs to the software developers. By investing your time and effort into modeling the domain and coming up with a set of terminology that\u0026rsquo;s understood for each subdomain, the process of understanding and solving the problem becomes much simpler and smoother\nModeling is an intense examination of the problem space. Key to this is working together with the subject matter experts to identify the core domain and other subdomains that you\u0026rsquo;ll be tackling. Another important aspect of modeling is identifying what\u0026rsquo;s called bounded contexts. And within each of these bounded contexts, you focus on modeling a particular subdomain. As a result of modeling a bounded context, you\u0026rsquo;ll identify entities, value objects, aggregates, domain events, repositories, and more and how they interact with each other.\nthe ubiquitous language. A simple definition of a ubiquitous language is to come up with terms that\u0026rsquo;ll be commonly used when discussing a particular subdomain. And they will most likely be terms that come from the problem space, not the software world, but they have to be agreed upon so that as discussions move forward, there is no confusion or misunderstanding created by the terminology used by various members of the team\nProjects,\nFrontdesk.core - contains domain model Frontdesk.infrastructure - integration with database and rabbitmq Frontdesk.API - API Endpoints bounded contexts maintain their separation by giving each context its own team, codebase, and database schema.\nsubdomain is a view on the problem space, how you\u0026rsquo;ve chosen to break down the business or domain activity, whereas a bounded context represents the solution space, how the software and the development of that software has been organized. Quite often, these will match up perfectly, but not always.\nSame Entity can appear in more than one bounded context\nThe Domain Layer is responsible for representing concepts of the business, information about the business situation, and business rules. State that reflects the business situation is controlled and used here, even though the technical details of storing it are delegated to the infrastructure. This layer of the domain is the heart of business software.\nValue object is an object that is used to measure, quantify, or describe something in your domain. Rather than having an identity key, its identity is based on the composition of the values of all of its properties. Because the property values define a value object, it should be immutable. In other words, you shouldn\u0026rsquo;t be able to change any of the properties once you\u0026rsquo;ve created one of these objects. Instead, you would simply create another instance with the new values. If you need to compare two value objects to determine if they are equal, you should do so by comparing all of the values. Value objects may have methods and behavior, but they should never have side effects. Any methods on the value objects should only compute things; they shouldn\u0026rsquo;t change the state of the value object, since it\u0026rsquo;s immutable, or the system. If a new value is needed, a new value object should be returned. In DDD, both entities and value objects are typically defined as classes. Classes have advantages over structs when it comes to encapsulation and support for inheritancebased extension and reuse.Value objects typically don\u0026rsquo;t exist alone, they\u0026rsquo;re usually applied to an entity to describe something about it.\ndomain services give you a place to put logic and behavior that you can\u0026rsquo;t find a home for in the entities and value objects in your domain.domain services should generally only be used if you don\u0026rsquo;t have an entity or value object where the behavior makes sense.domain services should be stateless, though they may have side effects. What this means is we should always be able to simply create a new instance of a service to perform an operation, rather than having to rely on any previous history that might have occurred within a particular service instance. But of course, the result of calling a method on a service might result in changes to the state of the system itself. These rules apply specifically to domain services which belong in the core of our application.\nSide effects are changes that occur in your application or any kind of interaction with the outside world.\nAggregates consist of one or more entities and value objects that change together. We need to treat them as a unit for data changes, and we need to consider the entire aggregate\u0026rsquo;s consistency before we apply changes.an aggregate is a cluster of associated objects that we treat as a unit for the purpose of data changes.\nA bidirectional association means that both objects can be understood only together. When application requirements do not call for traversal in both directions, adding a traversal direction reduces interdependence and simplifies the design.\nAn aggregate is a group of related objects that work together in a transaction. The root becomes the entry point through which you do any work with the aggregate, and the root also is what\u0026rsquo;s in charge of making sure that all of the rules that apply to that graph of objects are met. Each of the rules that describes the state that the system must be in in order to be valid is called an invariant. Within our aggregates, we have objects that are related to one another. In DDD, we refer to these relationships as associations. If you use an ORM, you may hear the term navigation properties, which refers to those properties that reference the related objects in the model. And we talked about the importance of defaulting to oneway relationships, which we also refer to as unidirectional relationships. In addition to these important terms, Steve and I shared a lot of guidance around creating aggregates and roots in your domain models. Nobody wants to work with a big ball of mud. We use aggregates to organize our model. An aggregate is a set of related objects that live in a single transaction while encapsulating the rules and enforcing invariance of that transaction, making sure that the system is in a consistent state. When designing how related objects work together, your job will be easier with oneway relationships. Use those as a default, and only introduce bidirectional navigation if you really need to. And most importantly, don\u0026rsquo;t resist updating your model as you and your team of domain experts learn more about the domain. Hopefully, most of this will happen early on, and then just once in a while you might have a big breakthrough, like we did when we realized that the schedule made more sense as an aggregate root than trying to have each appointment be its own aggregate.\nbe sure to provide repositories only for aggregate roots that require direct access. And next, keep the clients focused on the model, while delegating all of the object storage and access concerns to the repositories.\neach domain event should be its own class\nDomain events are a type of object that actually represents something that occurred within the domain that other parts of the system may find interesting and want to tie their behavior to.\nanticorruption layers, which use a variety of design patterns to insulate our model from the design choices of other applications or bounded contexts.\n"}]