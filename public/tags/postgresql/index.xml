<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Postgresql on Learnings in IT</title><link>http://localhost:1313/tags/postgresql/</link><description>Recent content in Postgresql on Learnings in IT</description><generator>Hugo</generator><language>en</language><lastBuildDate>Thu, 15 Jul 2021 10:25:04 +0530</lastBuildDate><atom:link href="http://localhost:1313/tags/postgresql/index.xml" rel="self" type="application/rss+xml"/><item><title>Near real time API Monitoring with Grafana and PostgreSQL</title><link>http://localhost:1313/posts/nrtanalysispostgresql/</link><pubDate>Thu, 15 Jul 2021 10:25:04 +0530</pubDate><guid>http://localhost:1313/posts/nrtanalysispostgresql/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &amp;ldquo;always on&amp;rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.&lt;/p>
&lt;p>What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?&lt;/p></description></item><item><title>ELT approach for Data Pipelines</title><link>http://localhost:1313/posts/elt/</link><pubDate>Sun, 14 Mar 2021 00:00:00 +0530</pubDate><guid>http://localhost:1313/posts/elt/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Extract&lt;/strong> - typically involves retrieving data from source. This could also be via streaming&lt;/li>
&lt;li>&lt;strong>Transform&lt;/strong> - Apply transformation to the extracted data.&lt;/li>
&lt;li>&lt;strong>Load&lt;/strong> - Loading the data in Operation Data store (ODS) or data warehouse
Refer &lt;a href=https://www.sas.com/en_us/insights/data-management/what-is-etl.html#close
 
 target=_blank rel="noopener noreferrer"
>here&lt;/a> for more details on ETL. ETL has been made easy by tools like &lt;a href=https://www.talend.com/products/talend-open-studio/
 
 target=_blank rel="noopener noreferrer"
>Talend&lt;/a>, &lt;a href=https://docs.microsoft.com/en-us/sql/integration-services/sql-server-integration-services
 
 target=_blank rel="noopener noreferrer"
>SSIS&lt;/a> and so on.&lt;/li>
&lt;/ul>
&lt;p>However, there has been shift from above approach due to,&lt;/p></description></item><item><title>Resiliency Testing with Toxiproxy</title><link>http://localhost:1313/posts/resiliencytoxiproxy/</link><pubDate>Sat, 09 Jan 2021 10:25:04 +0530</pubDate><guid>http://localhost:1313/posts/resiliencytoxiproxy/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>In a typical workflow of software development, Developer implements a Unit/component, tests it and pushes the changes to source control repository. It then goes through Continuous integration, automated testing, provisioning and deployment. Given High availability requirements expected (or should i say assumed) nowadays, As much as functional correctness of the Unit, it is also important to test how a Unit/Component handles failures, delays etc. in distributed environment. Often, such behavior is observed in production itself, unless project team is following practices of &lt;a href=https://netflixtechblog.com/tagged/chaos-engineering
 
 target=_blank rel="noopener noreferrer"
>Chaos engineering&lt;/a>.&lt;/p></description></item></channel></rss>