<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Learnings in IT</title><link>https://sachinsu.github.io/posts/</link><description>Recent content in Posts on Learnings in IT</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sat, 11 May 2024 10:25:04 +0530</lastBuildDate><atom:link href="https://sachinsu.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Using local LLM with Ollama and Semantic Kernel</title><link>https://sachinsu.github.io/posts/ollamasemantickernel/</link><pubDate>Sat, 11 May 2024 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/ollamasemantickernel/</guid><description>Introduction Artificial Intelligence, especially Large language models (LLMs) are all in high demand. Since OpenAI released ChatGPT, interest has gone up multi-fold. Since 2023, Powerful LLMs can be run on local machines. Local Large Language Models offer advantages in terms of data privacy and security and can be enriched using enterprise-specific data using Retrieval augmentation generation (RAG).Several tools exist that make it relatively easy to obtain, run and manage such models locally on our machines.</description></item><item><title>Troubleshooting TLS handshake issue</title><link>https://sachinsu.github.io/posts/tlshandsharefailure/</link><pubDate>Sat, 25 Feb 2023 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/tlshandsharefailure/</guid><description>Background Ever encountered a scenario where REST API consumption works from tools like curl, Web Browser but not from Application. Lets dive in.
The requirement is as simple as consuming REST API from a Application over TLS.
Problem Statement The REST API, to be consumed, is standard API interface which requires access over TLS. The client in this case is Windows 2016 server.
During Development, Windows 10 is used to develop and test the code.</description></item><item><title>URL Shortener in High Throughput Service</title><link>https://sachinsu.github.io/posts/shortidgeneration/</link><pubDate>Sun, 15 May 2022 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/shortidgeneration/</guid><description>Background A Client has E-commerce Application consisting of services aimed at specific domains of business functionality it serves. One of these services is responsible for accepting the order, authenticating it and forwarding it for further processing in terms of inventory checks, payment and so on. For Authentication, this service sends SMS to Customer&amp;rsquo;s Mobile number (and e-mail id) and customer is supposed to confirm this order placement by means of entering Code received in it.</description></item><item><title>Can SQLite be considered for Server Applications?</title><link>https://sachinsu.github.io/posts/is_sqlite_production_ready/</link><pubDate>Thu, 30 Dec 2021 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/is_sqlite_production_ready/</guid><description>Introduction While embarking on building any new server application, one of the key requirement is whether it needs durable, persistent storage of data (and in most cases, it does). This is followed by evaluating suitable data store. Likely evaluation criteria is Application&amp;rsquo;s Requirement (Tolerance for eventual consistency, High Availability etc.), Team&amp;rsquo;s familiarity, Costs, Tech. support availability and so on. In case of choices in relational databases, typical go to options are MySQL, PostgreSQL or even proprietary databases like Oracle , SQL Server.</description></item><item><title>Profiling and benchmarking tools for Applications</title><link>https://sachinsu.github.io/posts/profiling_n_benchmarking/</link><pubDate>Sun, 12 Dec 2021 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/profiling_n_benchmarking/</guid><description>Introduction We develop a piece of software with aim to fulfil specific business requirements in terms of resource usage, throughput, availability among others. Profiling and benchmarking are approaches that developer has in his/her arsenal to gain continuous feedback on whether a piece of code is behaving optimally and adhering to it&amp;rsquo;s objectives.
Lets look at what they mean,
Profiling is defined as process aimed at understanding the behavior of a program.</description></item><item><title>Database Reliability Engineering - My Notes</title><link>https://sachinsu.github.io/posts/dbre/</link><pubDate>Sun, 05 Sep 2021 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/dbre/</guid><description>Introduction I have been reading excellent Database Reliability Engineering book and below are my notes from it.
Key Incentive(s) for Automation
Elimination of Toil - Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows. Important System Characteristics
Latency, also known as response time, is a time-based measurement indicating how long it takes to receive a response from a request.</description></item><item><title>Near real time API Monitoring with Grafana and PostgreSQL</title><link>https://sachinsu.github.io/posts/nrtanalysispostgresql/</link><pubDate>Thu, 15 Jul 2021 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/nrtanalysispostgresql/</guid><description>Introduction Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &amp;ldquo;always on&amp;rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.
What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?</description></item><item><title>Upgrading API: Learnings</title><link>https://sachinsu.github.io/posts/apiupgrade/</link><pubDate>Sat, 15 May 2021 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/apiupgrade/</guid><description>Introduction One of the design considerations stressed upon by Jeffrey richter about APIs (Read more here) is that &amp;ldquo;API is expected to be stable over long period of time&amp;rdquo;. Recently,for a .NET based project, we decided to upgrade some of the ASMX (legacy SOAP based approach) based APIs and were immediately reminded by Customer(s) to avoid any kind of impact on existing users.
This means that upgrade must be done keeping in mind,</description></item><item><title>Presto - A distributed SQL Engine for variety of data stores</title><link>https://sachinsu.github.io/posts/presto/</link><pubDate>Mon, 29 Mar 2021 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/presto/</guid><description>Introduction In a company/enterprise, typically there are multiple sources of data. This could be result of M&amp;amp;A (where each of those add in a new data store) or result of multi year process of using data stores that are in vogue at that time. Result is combination of various types of relational databases, flat file systems, queues and so on. This results in Data Silos. This scenario is typically observed in companies who are running workloads On-prem (i.</description></item><item><title>ELT approach for Data Pipelines</title><link>https://sachinsu.github.io/posts/elt/</link><pubDate>Sun, 14 Mar 2021 00:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/elt/</guid><description>Introduction While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,
Extract - typically involves retrieving data from source. This could also be via streaming Transform - Apply transformation to the extracted data. Load - Loading the data in Operation Data store (ODS) or data warehouse Refer here for more details on ETL. ETL has been made easy by tools like Talend, SSIS and so on.</description></item><item><title>Learnings from Jeff Richter's Designing and Versioning HTTP REST APIs Video Course</title><link>https://sachinsu.github.io/posts/restapiversioning/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/restapiversioning/</guid><description>Background Recently, i went through excellent video series on Designing &amp;amp; Versioning HTTP_REST APIs presented by Jeffrey Richter. It is available here. In the past, i had read Jeff&amp;rsquo;s books on CLR and found his writing to be very clear and understandable. So is my experience with this Video Series. Below is summary of learnings from this Video Series. I do not claim that every aspect is covered here so please do check out the videos.</description></item><item><title>Resiliency Testing with Toxiproxy</title><link>https://sachinsu.github.io/posts/resiliencytoxiproxy/</link><pubDate>Sat, 09 Jan 2021 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/resiliencytoxiproxy/</guid><description>Background In a typical workflow of software development, Developer implements a Unit/component, tests it and pushes the changes to source control repository. It then goes through Continuous integration, automated testing, provisioning and deployment. Given High availability requirements expected (or should i say assumed) nowadays, As much as functional correctness of the Unit, it is also important to test how a Unit/Component handles failures, delays etc. in distributed environment. Often, such behavior is observed in production itself, unless project team is following practices of Chaos engineering.</description></item><item><title>Using Temporal.io to build Long running Workflows</title><link>https://sachinsu.github.io/posts/temporalworkflow/</link><pubDate>Mon, 07 Dec 2020 08:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/temporalworkflow/</guid><description>Background In a typical business Application, there are often requirements for,
Batch processing - Often long running Tasks like data import/export, End of day processing etc. These tasks are often scheduled to be executed at pre-defined interval or on occurance of an Event. Asychronous processing - Tasks, often part of business process / workflow, that can be performed asychronously or offloaded. Such requirements are often fulfilled with custom approaches like batch processing frameworks, ETL Tools or using Queues or specific database features.</description></item><item><title>Getting Started with OpenTelemetry</title><link>https://sachinsu.github.io/posts/opentelemetry/</link><pubDate>Sat, 07 Nov 2020 08:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/opentelemetry/</guid><description>Background How many times have we landed up in a meeting staring at random slowness or such production issues in a distributed Application ? only to experience helplessness with limited (or often times no) visibility available about the runtime behavior of the Application. It often ends up in manually correlating whatever diagnostic data available from Application and combining it with trace/logs that are available from O/S, databases etc. and trying to figure out &amp;ldquo;Root cause&amp;rdquo; of the issue.</description></item><item><title>Ninja - Using lightweight build system for Go projects</title><link>https://sachinsu.github.io/posts/ninjabuildsystem/</link><pubDate>Tue, 27 Oct 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/ninjabuildsystem/</guid><description>Background I primarily work on Windows for development purposes. Whenever its about writing code in Golang, invariably one comes across usage of Make. A quick check on popular Go projects on Github will show Makefile being used to automate tasks like linting, build, testing and deployment.
Being on Windows, i have been looking for alternative build tool that is easy to setup (i.e. doesn&amp;rsquo;t require mingw and such environments) and use compared to Make (which is primarily targetted at Unix and Unix like Operating Systems).</description></item><item><title>Validating urls from 'Useful Links' section using bash / command line tools</title><link>https://sachinsu.github.io/posts/urlhealthchecks/</link><pubDate>Thu, 15 Oct 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/urlhealthchecks/</guid><description>Background I started this blog, https://sachinsu.github.io few months back .
In this relatively short period of time, Blog has sizeable number of useful links across various categories in addition to the detailed blog post like this one.
As an ongoing activity, I think that it is necessary to verify links mentioned on this blog.
So how can it be done ? obviously one way is to do it manually by visiting each link and updating/removing those that are no longer available.</description></item><item><title>Trobleshooting TCP Connection request time outs</title><link>https://sachinsu.github.io/posts/connectiontimeouts/</link><pubDate>Tue, 25 Aug 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/connectiontimeouts/</guid><description>Background I recently had opportunity to support team who has been battling with Intermittent (scary i know :)) issues with TCP connectivity in Production.
Simplified deployment Architecture is as below,
High Level Architecture Technology Stack used is Microsoft .NET Framework 4.8 using ODP.NET for Oracle Connectivity (Oracle Server is 8 CPU box). Each of Web Servers in cluster have IIS hosted on it with multiple Applications (Application domains) serving HTTP(s) based traffic.</description></item><item><title>Tool to mass DM followers on Twitter in Go</title><link>https://sachinsu.github.io/posts/massdmgolang/</link><pubDate>Sat, 25 Jul 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/massdmgolang/</guid><description>Background I recently came across bounty by Balaji Srinivasan to send Direct Message to all twitter followers. Currently, i do not intend to participate in bounty and this is mere exercise.
This is an attempt to write CLI tool in Golang in response to it.
For detailed requirements, refer here
Approach In Brief,
CLI should,
accept arguments like Twitter API Key,Auth token, DM Message Download all followers (with profile details) Rank them by Criteria (e.</description></item><item><title>Web Security Measures in ASP.NET Applications</title><link>https://sachinsu.github.io/posts/websecurity/</link><pubDate>Thu, 04 Jun 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/websecurity/</guid><description>At my current workplace, All Applications are expected to adhere to PCI DSS standards meant for Data protection, Access Regulation and so on. Dedicated SOC Team,consisting of Security analyst who are continously on the prawl to identify breach, conduct periodic auditing of Applications, hardening of Servers.
While all our .NET applications adhere to below guidelines,
ASP.NET Security Overview Secure Coding Guidelines Security Guidelines by OWASP We also use tools like Snyk to perform code vulnerability analysis as part of Jenkins driven CI/CD pipeline.</description></item><item><title>Is WebAssembly future of Web Development</title><link>https://sachinsu.github.io/posts/webassembly/</link><pubDate>Tue, 02 Jun 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/webassembly/</guid><description>Over the last many years, de-facto language of the Web (specifically front-end) has been Javascript (and variants like Typescript, ECMAScript versions and so on). The Web development has been revolving around HTML+CSS+Javascript trio. It all started with support for Javascript in browsers, followed by addition of XMLHTTP API, Rich DOM Manipulation Support in Javascript. To induce order and apply patterns to Javascript&amp;rsquo;s usage in browsers, numerous frameworks and libraries were introduced like React and Vue among others.</description></item><item><title>Using Github Actions for Automated Testing and Deployment</title><link>https://sachinsu.github.io/posts/usinggithubactions/</link><pubDate>Thu, 28 May 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/usinggithubactions/</guid><description>Background The source code of tracfee.com is hosted on Github Private.
At a High level, Tracfee&amp;rsquo;s Architecture involves,
Single Page Application using VueJS, deployed on Netlify API in Go, deployed on Oracle Cloud So far, API testing has been automated and we were looking at ways to automate deployment of both UI and API. Steps required to deploy API are less since we are using Docker to run it on VM.</description></item><item><title>Windows Service with Cancelable Task</title><link>https://sachinsu.github.io/posts/windowsservicecancellabletask/</link><pubDate>Tue, 05 May 2020 12:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/windowsservicecancellabletask/</guid><description>Background Recently, we had requirement wherein a process should,
Periodically (Poll) or Asynchronously (Pub-sub) listen on incoming requests/messages. The whole process is expected to be long running. Should also implement clean disposal of in-flight requests and subsequent cleanup using something similar to Cancelble Context in Go The first of the objective is somewhat dependent on mechanism (Pub/sub, Listener), protocol (TCP, HTTP etc.). For the second one, .NET framework (and .NET Core) offers CancellationToken.</description></item><item><title>Optimizing .NET Code using Benchmarks</title><link>https://sachinsu.github.io/posts/usingbenchmarkdotnet/</link><pubDate>Tue, 05 May 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/usingbenchmarkdotnet/</guid><description>Background Oftentimes, we come across situation where code does not perform as per expectation. What is typically approch to address it,
Performance Testing - Visual Studio Load Tests or Third party tools like Locust, Vegeta, Gatling etc. Visual Studio Diagnostics Tools Or Use tools like Perfview/dotTrace/dotMemory to diagnose bottlenecks What if it is possible to Benchmark code for,
Set of varying parameter(s) Different runtimes (.NET Framework versions, .NET core, Mono etc.</description></item><item><title>ASP.NET Core - Mind the SameSite HTTP Cookie settings</title><link>https://sachinsu.github.io/posts/samesitecookies/</link><pubDate>Thu, 09 Apr 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/samesitecookies/</guid><description>Background A Web Application, developed in ASP.NET Core (Runtime Version 3.1.100) using Razor Pages and Web API, is expected to be launched from within third-party Web Application in iframe, with complete HTML being rendered.
During the Development, a mock HTML Page was developed to simulate launching of ASP.NET core based Web Application in iframe. Note that this page as well as Application was hosted on same IIS Server and it worked fine.</description></item><item><title>Using Channels for High performance Producer consumer implementation</title><link>https://sachinsu.github.io/posts/channelsforproducerconsumer/</link><pubDate>Wed, 12 Feb 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/channelsforproducerconsumer/</guid><description>Background Recently, i got involved in assignment where in an application was facing issues with throughput. Expectation is to support more than 500 transactions per second while load testing results were indicating system was experiencing high latency beyond 100+ transactions per second.
This application is developed in .NET Framework + .NET Core and primarily uses Relational Database for persistence and has point to point integration (mainly over HTTP) with internal &amp;amp; external application(s).</description></item><item><title>Using .NET standard Assembly in .NET core and .NET Framework</title><link>https://sachinsu.github.io/posts/dotnetstandard/</link><pubDate>Fri, 07 Feb 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/dotnetstandard/</guid><description>Background One of the key project(s) at my current organization is developed on .NET 4.6.1. It is developed as Modular Monolith. As part of it&amp;rsquo;s functionality, it supports different channels like Mobiles, Terminals and Web. For the Web channel, there was need to develop a Web application with,
High availability Lightweight, High throughput (Need to support few thousand(s) active users) Accordingly, we have been exploring developing this Web Application in .</description></item><item><title/><link>https://sachinsu.github.io/posts/dddnotes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://sachinsu.github.io/posts/dddnotes/</guid><description>Even Eric Evans explicitly states that DDD isn&amp;rsquo;t suitable for problems when there&amp;rsquo;s substantial technical complexity, but little business domain complexity. Using DDD is most beneficial when the complexity of the domain makes it challenging for the domain experts to communicate their needs to the software developers. By investing your time and effort into modeling the domain and coming up with a set of terminology that&amp;rsquo;s understood for each subdomain, the process of understanding and solving the problem becomes much simpler and smoother</description></item></channel></rss>