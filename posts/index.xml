<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Learnings in IT</title><link>https://sachinsu.github.io/posts/</link><description>Recent content in Posts on Learnings in IT</description><generator>Hugo</generator><language>en</language><lastBuildDate>Sun, 01 Jun 2025 01:00:00 +0530</lastBuildDate><atom:link href="https://sachinsu.github.io/posts/index.xml" rel="self" type="application/rss+xml"/><item><title>Exploring MCP Toolbox for Databases: A New Era of Database Querying</title><link>https://sachinsu.github.io/posts/genaidb/</link><pubDate>Sun, 01 Jun 2025 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/genaidb/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In the ever-evolving landscape of AI and machine learning, Google&amp;rsquo;s &lt;a href=https://googleapis.github.io/genai-toolbox
 
 target=_blank rel="noopener noreferrer"
>Gen AI Toolbox for Databases&lt;/a> stands out. This open-source server enables developers to connect generative AI applications to enterprise databases, facilitating prompt-based querying and natural language processing (NLP). Whether you&amp;rsquo;re setting up your LLM on-premises using OLLAMA or leveraging providers like Gemini, Claude, or OpenAI, this toolbox offers a versatile and powerful solution. Lets explore it in detail.&lt;/p></description></item><item><title>Using Asynchronous programming to manage parallel processing</title><link>https://sachinsu.github.io/posts/parallelprocessing/</link><pubDate>Tue, 30 Jul 2024 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/parallelprocessing/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>There was a requirement to perform series of tasks, involving generation of output files, such that the required throughput is achieved. These tasks involve database read operation, external API invocation and file i/o. Generally, benchmarking showed that executing them in sequential way was not helpful. What if asynchronous programming be used to perform this task.&lt;/p>
&lt;p>So Lets Start.&lt;/p>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>Lets assume that this typical use case requires,&lt;/p>
&lt;ul>
&lt;li>
&lt;p>fetching data from database for the purpose of merging placeholders in a Template and perform mail merge&lt;/p></description></item><item><title>Using local LLM with Ollama and Semantic Kernel</title><link>https://sachinsu.github.io/posts/ollamasemantickernel/</link><pubDate>Sat, 11 May 2024 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/ollamasemantickernel/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Artificial Intelligence, especially Large language models (LLMs) are all in high demand. Since OpenAI released ChatGPT, interest has gone up multi-fold. Since 2023, Powerful LLMs can be run on local machines. Local Large Language Models offer advantages in terms of data privacy and security and can be enriched using enterprise-specific data using Retrieval augmentation generation (RAG).Several tools exist that make it relatively easy to obtain, run and manage such models locally on our machines. Few examples are &lt;a href=https://ollama.com/
 
 target=_blank rel="noopener noreferrer"
>Ollama&lt;/a>, &lt;a href=https://github.com/hwchase17/langchain
 
 target=_blank rel="noopener noreferrer"
>Langchain&lt;/a>, &lt;a href=localai.io
 
 
>LocalAI&lt;/a>.&lt;/p></description></item><item><title>Troubleshooting TLS handshake issue</title><link>https://sachinsu.github.io/posts/tlshandsharefailure/</link><pubDate>Sat, 25 Feb 2023 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/tlshandsharefailure/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>Ever encountered a scenario where REST API consumption works from tools like &lt;a href=https://github.com/jeroen/curl
 
 target=_blank rel="noopener noreferrer"
>curl&lt;/a>, Web Browser but not from Application. Lets dive in.&lt;/p>
&lt;p>The requirement is as simple as consuming REST API from a Application over TLS.&lt;/p>
&lt;h2 id="problem-statement">Problem Statement&lt;/h2>
&lt;p>The REST API, to be consumed, is standard API interface which requires access over TLS. The client in this case is Windows 2016 server.&lt;/p>
&lt;p>During Development, Windows 10 is used to develop and test the code. Later, the same is tested on a Windows 2016 Server. It is at this stage, it fails with cryptic Error &amp;ldquo;The request was aborted: Could not create SSL/TLS secure channel&amp;rdquo;. But it works fine with other tools like curl, PostMan or even from a Web Browser.&lt;/p></description></item><item><title>URL Shortener in High Throughput Service</title><link>https://sachinsu.github.io/posts/shortidgeneration/</link><pubDate>Sun, 15 May 2022 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/shortidgeneration/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>A Client has E-commerce Application consisting of services aimed at specific domains of business functionality it serves. One of these services is responsible for accepting the order, authenticating it and forwarding it for further processing in terms of inventory checks, payment and so on. For Authentication, this service sends &lt;a href=https://en.wikipedia.org/wiki/SMS
 
 target=_blank rel="noopener noreferrer"
>SMS&lt;/a> to Customer&amp;rsquo;s Mobile number (and e-mail id) and customer is supposed to confirm this order placement by means of entering Code received in it. This code is valid for a short duration.&lt;/p></description></item><item><title>Can SQLite be considered for Server Applications?</title><link>https://sachinsu.github.io/posts/is_sqlite_production_ready/</link><pubDate>Thu, 30 Dec 2021 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/is_sqlite_production_ready/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>While embarking on building any new server application, one of the key requirement is whether it needs durable, persistent storage of data (and in most cases, it does). This is followed by evaluating suitable data store. Likely evaluation criteria is Application&amp;rsquo;s Requirement (Tolerance for eventual consistency, High Availability etc.), Team&amp;rsquo;s familiarity, Costs, Tech. support availability and so on.
In case of choices in relational databases, typical go to options are MySQL, PostgreSQL or even proprietary databases like Oracle , SQL Server. Seldom one considers &lt;a href=https://SQLite.org
 
 target=_blank rel="noopener noreferrer"
>SQLite&lt;/a> for this purpose.&lt;/p></description></item><item><title>Profiling and benchmarking tools for Applications</title><link>https://sachinsu.github.io/posts/profiling_n_benchmarking/</link><pubDate>Sun, 12 Dec 2021 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/profiling_n_benchmarking/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>We develop a piece of software with aim to fulfil specific business requirements in terms of resource usage, throughput, availability among others. Profiling and benchmarking are approaches that developer has in his/her arsenal to gain continuous feedback on whether a piece of code is behaving optimally and adhering to it&amp;rsquo;s objectives.&lt;/p>
&lt;p>Lets look at what they mean,&lt;/p>
&lt;ul>
&lt;li>Profiling is defined as process &lt;code>aimed at understanding the behavior of a program. A profile result might be a table of time taken per function,&lt;/code> as per &lt;a href=https://stackoverflow.com/questions/34801622/difference-between-benchmarking-and-profiling
 
 target=_blank rel="noopener noreferrer"
>this&lt;/a> and &lt;a href=https://en.wikipedia.org/wiki/Profiling_%28computer_programming%29
 
 target=_blank rel="noopener noreferrer"
>this&lt;/a>)&lt;/li>
&lt;li>Benchmarking &lt;code>measures the time for some whole operation. e.g. I/O operations per second under some workload. So the result is typically a single number, in either seconds or operations per second. Or a data set with results for different parameters, so you can graph it.&lt;/code>. Refer &lt;a href=https://en.wikipedia.org/wiki/Benchmark_%28computing%29
 
 target=_blank rel="noopener noreferrer"
>this&lt;/a> for more information. Also do check &lt;a href=https://jvns.ca/blog/2016/07/23/rigorous-benchmarking-in-reasonable-time/
 
 target=_blank rel="noopener noreferrer"
>Benchmarking correctly is hard by Julia Evans&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>Typically, Profiling is supported by most of the environments (either via IDEs like &lt;a href=https://docs.microsoft.com/en-us/visualstudio/profiling/profiling-feature-tour?view&amp;#61;vs-2022
 
 target=_blank rel="noopener noreferrer"
>Visual Studio&lt;/a> or through language itself [Like &lt;a href=https://go.dev/blog/pprof
 
 target=_blank rel="noopener noreferrer"
>Go&lt;/a>] has buil-in provision for the same while Benchmarking is typically performed on dedicated testing infrastructure.&lt;/p></description></item><item><title>Database Reliability Engineering - My Notes</title><link>https://sachinsu.github.io/posts/dbre/</link><pubDate>Sun, 05 Sep 2021 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/dbre/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>I have been reading excellent &lt;a href=https://www.oreilly.com/library/view/database-reliability-engineering/9781491925935/
 
 target=_blank rel="noopener noreferrer"
>Database Reliability Engineering&lt;/a> book and below are my notes from it.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Key Incentive(s) for Automation&lt;/p>
&lt;ul>
&lt;li>Elimination of Toil - Toil is the kind of work tied to running a production service that tends to be manual, repetitive, automatable, tactical, devoid of enduring value, and that scales linearly as a service grows.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Important System Characteristics&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;em>Latency&lt;/em>, also known as response time, is a time-based measurement indicating how long it takes to receive a response from a request. It is best to measure this for end-to-end response from the customer rather than breaking it down component by component. This is customer-centric design and is crucial for any system that has customers, which is any system&lt;/p></description></item><item><title>Near real time API Monitoring with Grafana and PostgreSQL</title><link>https://sachinsu.github.io/posts/nrtanalysispostgresql/</link><pubDate>Thu, 15 Jul 2021 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/nrtanalysispostgresql/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &amp;ldquo;always on&amp;rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.&lt;/p>
&lt;p>What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?&lt;/p></description></item><item><title>Upgrading API: Learnings</title><link>https://sachinsu.github.io/posts/apiupgrade/</link><pubDate>Sat, 15 May 2021 01:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/apiupgrade/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>One of the design considerations stressed upon by Jeffrey richter about APIs (Read more &lt;a href=https://sachinsu.github.io/posts/restapiversioning/
 
 
>here&lt;/a>) is that &amp;ldquo;API is expected to be stable over long period of time&amp;rdquo;. Recently,for a .NET based project, we decided to upgrade some of the ASMX (legacy SOAP based approach) based APIs and were immediately reminded by Customer(s) to avoid any kind of impact on existing users.&lt;/p>
&lt;p>This means that upgrade must be done keeping in mind,&lt;/p></description></item><item><title>Presto - A distributed SQL Engine for variety of data stores</title><link>https://sachinsu.github.io/posts/presto/</link><pubDate>Mon, 29 Mar 2021 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/presto/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>In a company/enterprise, typically there are multiple sources of data. This could be result of M&amp;amp;A (where each of those add in a new data store) or result of multi year process of using data stores that are in vogue at that time. Result is combination of various types of relational databases, flat file systems, queues and so on. This results in Data Silos. This scenario is typically observed in companies who are running workloads On-prem (i.e. Pre-cloud, Companies who started on Cloud or have moved to it, typically tend to organize data platform better. This could be because of ease of migrating data on cloud. Typically, they centralize it around cheaper object storage (say AWS S3)).&lt;/p></description></item><item><title>ELT approach for Data Pipelines</title><link>https://sachinsu.github.io/posts/elt/</link><pubDate>Sun, 14 Mar 2021 00:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/elt/</guid><description>&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>While gathering data for Analytics, one often has to source data from multiple sources. Traditionally, the approach has been to do ETL (Extract-Transform-load) where,&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Extract&lt;/strong> - typically involves retrieving data from source. This could also be via streaming&lt;/li>
&lt;li>&lt;strong>Transform&lt;/strong> - Apply transformation to the extracted data.&lt;/li>
&lt;li>&lt;strong>Load&lt;/strong> - Loading the data in Operation Data store (ODS) or data warehouse
Refer &lt;a href=https://www.sas.com/en_us/insights/data-management/what-is-etl.html#close
 
 target=_blank rel="noopener noreferrer"
>here&lt;/a> for more details on ETL. ETL has been made easy by tools like &lt;a href=https://www.talend.com/products/talend-open-studio/
 
 target=_blank rel="noopener noreferrer"
>Talend&lt;/a>, &lt;a href=https://docs.microsoft.com/en-us/sql/integration-services/sql-server-integration-services
 
 target=_blank rel="noopener noreferrer"
>SSIS&lt;/a> and so on.&lt;/li>
&lt;/ul>
&lt;p>However, there has been shift from above approach due to,&lt;/p></description></item><item><title>Learnings from Jeff Richter's Designing and Versioning HTTP REST APIs Video Course</title><link>https://sachinsu.github.io/posts/restapiversioning/</link><pubDate>Wed, 20 Jan 2021 00:00:00 +0530</pubDate><guid>https://sachinsu.github.io/posts/restapiversioning/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>Recently, i went through excellent video series on &lt;code>Designing &amp;amp; Versioning HTTP_REST APIs&lt;/code> presented by &lt;a href=https://www.linkedin.com/in/jeffrichter/
 
 target=_blank rel="noopener noreferrer"
>Jeffrey Richter&lt;/a>. It is available &lt;a href=https://www.youtube.com/watch?v&amp;#61;9Ng00IlBCtw
 
 target=_blank rel="noopener noreferrer"
>here&lt;/a>. In the past, i had read Jeff&amp;rsquo;s books on CLR and found his writing to be very clear and understandable. So is my experience with this Video Series. Below is summary of learnings from this Video Series. I do not claim that every aspect is covered here so please do check out the videos.&lt;/p></description></item><item><title>Resiliency Testing with Toxiproxy</title><link>https://sachinsu.github.io/posts/resiliencytoxiproxy/</link><pubDate>Sat, 09 Jan 2021 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/resiliencytoxiproxy/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>In a typical workflow of software development, Developer implements a Unit/component, tests it and pushes the changes to source control repository. It then goes through Continuous integration, automated testing, provisioning and deployment. Given High availability requirements expected (or should i say assumed) nowadays, As much as functional correctness of the Unit, it is also important to test how a Unit/Component handles failures, delays etc. in distributed environment. Often, such behavior is observed in production itself, unless project team is following practices of &lt;a href=https://netflixtechblog.com/tagged/chaos-engineering
 
 target=_blank rel="noopener noreferrer"
>Chaos engineering&lt;/a>.&lt;/p></description></item><item><title>Using Temporal.io to build Long running Workflows</title><link>https://sachinsu.github.io/posts/temporalworkflow/</link><pubDate>Mon, 07 Dec 2020 08:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/temporalworkflow/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>In a typical business Application, there are often requirements for,&lt;/p>
&lt;ul>
&lt;li>Batch processing - Often long running Tasks like data import/export, End of day processing etc. These tasks are often scheduled to be executed at pre-defined interval or on occurance of an Event.&lt;/li>
&lt;li>Asychronous processing - Tasks, often part of business process / workflow, that can be performed asychronously or offloaded.&lt;/li>
&lt;/ul>
&lt;p>Such requirements are often fulfilled with custom approaches like batch processing frameworks, ETL Tools or using Queues or specific database features.&lt;/p></description></item><item><title>Getting Started with OpenTelemetry</title><link>https://sachinsu.github.io/posts/opentelemetry/</link><pubDate>Sat, 07 Nov 2020 08:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/opentelemetry/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>How many times have we landed up in a meeting staring at random slowness or such production issues in a distributed Application ? only to experience helplessness with limited (or often times no) visibility available about the runtime behavior of the Application. It often ends up in manually correlating whatever diagnostic data available from Application and combining it with trace/logs that are available from O/S, databases etc. and trying to figure out &amp;ldquo;Root cause&amp;rdquo; of the issue.&lt;/p></description></item><item><title>Ninja - Using lightweight build system for Go projects</title><link>https://sachinsu.github.io/posts/ninjabuildsystem/</link><pubDate>Tue, 27 Oct 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/ninjabuildsystem/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>I primarily work on Windows for development purposes. Whenever its about writing code in Golang, invariably one comes across usage of Make. A quick check on popular Go projects on Github will show Makefile being used to automate tasks like linting, build, testing and deployment.&lt;/p>
&lt;p>Being on Windows, i have been looking for alternative build tool that is easy to setup (i.e. doesn&amp;rsquo;t require mingw and such environments) and use compared to Make (which is primarily targetted at Unix and Unix like Operating Systems).&lt;/p></description></item><item><title>Validating urls from 'Useful Links' section using bash / command line tools</title><link>https://sachinsu.github.io/posts/urlhealthchecks/</link><pubDate>Thu, 15 Oct 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/urlhealthchecks/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>I started this blog, &lt;a href=https://sachinsu.github.io
 
 target=_blank rel="noopener noreferrer"
>https://sachinsu.github.io&lt;/a> few months back .&lt;/p>
&lt;p>In this relatively short period of time, Blog has sizeable number of useful links across various categories in addition to the detailed blog post like this one.&lt;/p>
&lt;p>As an ongoing activity, I think that it is necessary to verify links mentioned on this blog.&lt;/p>
&lt;p>So how can it be done ? obviously one way is to do it manually by visiting each link and updating/removing those that are no longer available. but there is always of better way of doing things.&lt;/p></description></item><item><title>Trobleshooting TCP Connection request time outs</title><link>https://sachinsu.github.io/posts/connectiontimeouts/</link><pubDate>Tue, 25 Aug 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/connectiontimeouts/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>I recently had opportunity to support team who has been battling with Intermittent (scary i know :)) issues with TCP connectivity in Production.&lt;/p>
&lt;p>Simplified deployment Architecture is as below,&lt;/p>
&lt;figure>&lt;img src="https://sachinsu.github.io/images/conntimeoutarch.png">&lt;figcaption>
 &lt;h4>High Level Architecture&lt;/h4>
 &lt;/figcaption>
&lt;/figure>

&lt;p>Technology Stack used is Microsoft .NET Framework 4.8 using ODP.NET for Oracle Connectivity (Oracle Server is 8 CPU box). Each of Web Servers in cluster have IIS hosted on it with multiple Applications (Application domains) serving HTTP(s) based traffic. These applications connect to Oracle Database.&lt;/p></description></item><item><title>Tool to mass DM followers on Twitter in Go</title><link>https://sachinsu.github.io/posts/massdmgolang/</link><pubDate>Sat, 25 Jul 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/massdmgolang/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>I recently came across bounty by &lt;a href=https://twitter.com/balajis/status/1271945241881268224?s&amp;#61;20
 
 target=_blank rel="noopener noreferrer"
>Balaji Srinivasan&lt;/a> to send Direct Message to all twitter followers. &lt;em>Currently, i do not intend to participate in bounty and this is mere exercise.&lt;/em>&lt;/p>
&lt;p>This is an attempt to write CLI tool in &lt;a href=https://golang.org
 
 target=_blank rel="noopener noreferrer"
>Golang&lt;/a> in response to it.&lt;/p>
&lt;p>For detailed requirements, refer &lt;a href=https://github.com/balajis/twitter-export
 
 target=_blank rel="noopener noreferrer"
>here&lt;/a>&lt;/p>
&lt;h2 id="approach">Approach&lt;/h2>
&lt;p>In Brief,&lt;/p>
&lt;ul>
&lt;li>
&lt;p>CLI should,&lt;/p>
&lt;ul>
&lt;li>accept arguments like Twitter API Key,Auth token, DM Message&lt;/li>
&lt;li>Download all followers (with profile details)&lt;/li>
&lt;li>Rank them by Criteria (e.g. Location)&lt;/li>
&lt;li>Send each follower a DM with provided message (upto daily DM Limit)&lt;/li>
&lt;li>be easy to use and maintain&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Notes,&lt;/p></description></item><item><title>Web Security Measures in ASP.NET Applications</title><link>https://sachinsu.github.io/posts/websecurity/</link><pubDate>Thu, 04 Jun 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/websecurity/</guid><description>&lt;p>At my current workplace, All Applications are expected to adhere to &lt;a href=https://en.wikipedia.org/wiki/Payment_Card_Industry_Data_Security_Standard
 
 target=_blank rel="noopener noreferrer"
>PCI DSS standards&lt;/a> meant for Data protection, Access Regulation and so on. Dedicated &lt;a href=https://en.wikipedia.org/wiki/Information_security_operations_center
 
 target=_blank rel="noopener noreferrer"
>SOC&lt;/a> Team,consisting of Security analyst who are continously on the prawl to identify breach, conduct periodic auditing of Applications, hardening of Servers.&lt;/p>
&lt;p>While all our .NET applications adhere to below guidelines,&lt;/p>
&lt;ul>
&lt;li>&lt;a href=https://support.microsoft.com/en-in/help/891028/asp-net-security-overview
 
 target=_blank rel="noopener noreferrer"
>ASP.NET Security Overview&lt;/a>&lt;/li>
&lt;li>&lt;a href=https://docs.microsoft.com/en-us/dotnet/standard/security/secure-coding-guidelines
 
 target=_blank rel="noopener noreferrer"
>Secure Coding Guidelines&lt;/a>&lt;/li>
&lt;li>&lt;a href=https://cheatsheetseries.owasp.org/cheatsheets/DotNet_Security_Cheat_Sheet.html
 
 target=_blank rel="noopener noreferrer"
>Security Guidelines by OWASP&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>We also use tools like &lt;a href=https://www.snyk.io/
 
 target=_blank rel="noopener noreferrer"
>Snyk&lt;/a> to perform code vulnerability analysis as part of Jenkins driven CI/CD pipeline. In spite of above, we do come across vulnerabilities identified by SOC Team which we needs to be addressed quickly. SOC team uses tools such as &lt;a href=https://portswigger.net/burp
 
 target=_blank rel="noopener noreferrer"
>Burp Suite&lt;/a>.&lt;/p></description></item><item><title>Is WebAssembly future of Web Development</title><link>https://sachinsu.github.io/posts/webassembly/</link><pubDate>Tue, 02 Jun 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/webassembly/</guid><description>&lt;p>Over the last many years, de-facto language of the Web (specifically front-end) has been Javascript (and variants like Typescript, ECMAScript versions and so on). The Web development has been revolving around HTML+CSS+Javascript trio. It all started with support for Javascript in browsers, followed by addition of XMLHTTP API, Rich DOM Manipulation Support in Javascript. To induce order and apply patterns to Javascript&amp;rsquo;s usage in browsers, numerous frameworks and libraries were introduced like &lt;a href=https://reactjs.org
 
 target=_blank rel="noopener noreferrer"
>React&lt;/a> and &lt;a href=https://vuejs.org
 
 target=_blank rel="noopener noreferrer"
>Vue&lt;/a> among others. To begin with, The target used to be browsers on Large Devices like Desktop &amp;amp; Laptops. However, soon all sorts of devices were targetted with advent of Responsive and Progressive CSS+Javascript libraries eg. &lt;a href=https://getbootstrap.com
 
 target=_blank rel="noopener noreferrer"
>Bootstrap&lt;/a>. Offline Support soon came in ref: &lt;a href=https://electronjs.org
 
 target=_blank rel="noopener noreferrer"
>Electron&lt;/a> and &lt;a href=https://web.dev/progressive-web-apps/
 
 target=_blank rel="noopener noreferrer"
>Progressive Web Applications&lt;/a>.&lt;/p></description></item><item><title>Using Github Actions for Automated Testing and Deployment</title><link>https://sachinsu.github.io/posts/usinggithubactions/</link><pubDate>Thu, 28 May 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/usinggithubactions/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>The source code of &lt;a href=https://tracfee.com
 
 target=_blank rel="noopener noreferrer"
>tracfee.com&lt;/a> is hosted on Github Private.&lt;/p>
&lt;p>At a High level, Tracfee&amp;rsquo;s Architecture involves,&lt;/p>
&lt;ul>
&lt;li>Single Page Application using VueJS, deployed on &lt;a href=https://netlify.com
 
 target=_blank rel="noopener noreferrer"
>Netlify&lt;/a>&lt;/li>
&lt;li>API in &lt;a href=https://golang.org
 
 target=_blank rel="noopener noreferrer"
>Go&lt;/a>, deployed on &lt;a href=https://www.oracle.com/in/cloud/
 
 target=_blank rel="noopener noreferrer"
>Oracle Cloud&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>So far, API testing has been automated and we were looking at ways to automate deployment of both UI and API. Steps required to deploy API are less since we are using Docker to run it on VM. However, in case of Netlify, it is required to build and then upload the output folder on Netlify.&lt;/p></description></item><item><title>Windows Service with Cancelable Task</title><link>https://sachinsu.github.io/posts/windowsservicecancellabletask/</link><pubDate>Tue, 05 May 2020 12:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/windowsservicecancellabletask/</guid><description>&lt;h3 id="background">Background&lt;/h3>
&lt;p>Recently, we had requirement wherein a process should,&lt;/p>
&lt;ul>
&lt;li>Periodically (Poll) or Asynchronously (Pub-sub) listen on incoming requests/messages. The whole process is expected to be long running.&lt;/li>
&lt;li>Should also implement clean disposal of in-flight requests and subsequent cleanup using something similar to Cancelble &lt;a href=https://golang.org/pkg/context/
 
 target=_blank rel="noopener noreferrer"
>Context&lt;/a> in Go&lt;/li>
&lt;/ul>
&lt;p>The first of the objective is somewhat dependent on mechanism (Pub/sub, Listener), protocol (TCP, HTTP etc.). For the second one, .NET framework (and .NET Core) offers &lt;a href=https://docs.microsoft.com/en-us/dotnet/api/system.threading.cancellationtoken?view&amp;#61;netcore-3.1
 
 target=_blank rel="noopener noreferrer"
>CancellationToken&lt;/a>. It is maint for co-operative cancellation between threads and Task Objects. So Armed with this, is it possible to come up with a template that allows cancellation of long running task while also being deployed as Windows Service (or using systemd in Linux) ?&lt;/p></description></item><item><title>Optimizing .NET Code using Benchmarks</title><link>https://sachinsu.github.io/posts/usingbenchmarkdotnet/</link><pubDate>Tue, 05 May 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/usingbenchmarkdotnet/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>Oftentimes, we come across situation where code does not perform as per expectation. What is typically approch to address it,&lt;/p>
&lt;ul>
&lt;li>Performance Testing - Visual Studio Load Tests or Third party tools like &lt;a href=https://locust.io/
 
 target=_blank rel="noopener noreferrer"
>Locust&lt;/a>, &lt;a href=https://github.com/tsenart/vegeta
 
 target=_blank rel="noopener noreferrer"
>Vegeta&lt;/a>, &lt;a href=https://gatling.io/
 
 target=_blank rel="noopener noreferrer"
>Gatling&lt;/a> etc.&lt;/li>
&lt;li>Visual Studio Diagnostics Tools Or&lt;/li>
&lt;li>Use tools like Perfview/dotTrace/dotMemory to diagnose bottlenecks&lt;/li>
&lt;/ul>
&lt;p>What if it is possible to Benchmark code for,&lt;/p>
&lt;ul>
&lt;li>Set of varying parameter(s)&lt;/li>
&lt;li>Different runtimes (.NET Framework versions, .NET core, Mono etc.) with option to Benchmark it&lt;/li>
&lt;li>Observe Memory Allocations for diagnostics&lt;/li>
&lt;li>Get Detailed report on execution timeline&lt;/li>
&lt;li>Have it as part of test suite so that it can be easily executed with every iteration involving optimized code to get immediate feedback&lt;/li>
&lt;/ul>
&lt;p>Enter &lt;a href=https://benchmarkdotnet.org/
 
 target=_blank rel="noopener noreferrer"
>BenchmarkDotNet&lt;/a>, a Powerful .NET library for benchmarking. It is used by &lt;a href=https://github.com/dotnet/performance
 
 target=_blank rel="noopener noreferrer"
>DotNET&lt;/a> Team, Roslyn, ASP.NET Core and many other projects.&lt;/p></description></item><item><title>ASP.NET Core - Mind the SameSite HTTP Cookie settings</title><link>https://sachinsu.github.io/posts/samesitecookies/</link><pubDate>Thu, 09 Apr 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/samesitecookies/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>A Web Application, developed in ASP.NET Core (Runtime Version 3.1.100) using Razor Pages and Web API, is expected to be launched from within third-party Web Application in iframe, with complete HTML being rendered.&lt;/p>
&lt;p>During the Development, a mock HTML Page was developed to simulate launching of ASP.NET core based Web Application in iframe. Note that this page as well as Application was hosted on same IIS Server and it worked fine. Subsequently, Web Application was deployed on Test Server and URL was shared for integration with third party Application and then it happened Boom&amp;hellip;. i.e. Application when launched in iframe rendered HTML but none of the post request would work (returning HTTP Error 400). Careful inspection showed that,&lt;/p></description></item><item><title>Using Channels for High performance Producer consumer implementation</title><link>https://sachinsu.github.io/posts/channelsforproducerconsumer/</link><pubDate>Wed, 12 Feb 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/channelsforproducerconsumer/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>Recently, i got involved in assignment where in an application was facing issues with throughput. Expectation is to support more than 500 transactions per second while load testing results were indicating system was experiencing high latency beyond 100+ transactions per second.&lt;/p>
&lt;p>This application is developed in .NET Framework + .NET Core and primarily uses Relational Database for persistence and has point to point integration (mainly over HTTP) with internal &amp;amp; external application(s).&lt;/p></description></item><item><title>Using .NET standard Assembly in .NET core and .NET Framework</title><link>https://sachinsu.github.io/posts/dotnetstandard/</link><pubDate>Fri, 07 Feb 2020 10:25:04 +0530</pubDate><guid>https://sachinsu.github.io/posts/dotnetstandard/</guid><description>&lt;h2 id="background">Background&lt;/h2>
&lt;p>One of the key project(s) at my current organization is developed on .NET 4.6.1. It is developed as &lt;a href=https://www.youtube.com/watch?v&amp;#61;5OjqD-ow8GE
 
 target=_blank rel="noopener noreferrer"
>Modular Monolith&lt;/a>. As part of it&amp;rsquo;s functionality, it supports different channels like Mobiles, Terminals and Web. For the &lt;em>Web&lt;/em> channel, there was need to develop a Web application with,&lt;/p>
&lt;ul>
&lt;li>High availability&lt;/li>
&lt;li>Lightweight, High throughput (Need to support few thousand(s) active users)&lt;/li>
&lt;/ul>
&lt;p>Accordingly, we have been exploring developing this Web Application in .NET core 3.1. However, it also means that we will have to use class libraries, targeted at .NET framework 4.6.1, in .NET core and vice-versa. How can this be done?&lt;/p></description></item><item><title/><link>https://sachinsu.github.io/posts/dddnotes/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://sachinsu.github.io/posts/dddnotes/</guid><description>&lt;p>Even Eric Evans explicitly states that DDD isn&amp;rsquo;t suitable for problems when there&amp;rsquo;s substantial technical complexity, but little business domain complexity. Using DDD is most beneficial when the complexity of the domain makes it challenging for the domain experts to communicate their needs to the software developers. By investing your time and effort into modeling the domain and coming up with a set of terminology that&amp;rsquo;s understood for each subdomain, the process of understanding and solving the problem becomes much simpler and smoother&lt;/p></description></item></channel></rss>