<!doctype html><html class=no-js lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><title>Near real time API Monitoring with Grafana and PostgreSQL - Learnings in IT</title><script>(function(a,b){a[b]=a[b].replace("no-js","js")})(document.documentElement,"className")</script><meta name=description content><meta property="og:title" content="Near real time API Monitoring with Grafana and PostgreSQL"><meta property="og:description" content="Introduction Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &ldquo;always on&rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.
What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?"><meta property="og:type" content="article"><meta property="og:url" content="https://sachinsu.github.io/posts/nrtanalysispostgresql/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-07-15T10:25:04+05:30"><meta property="article:modified_time" content="2021-07-15T10:25:04+05:30"><meta itemprop=name content="Near real time API Monitoring with Grafana and PostgreSQL"><meta itemprop=description content="Introduction Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &ldquo;always on&rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.
What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?"><meta itemprop=datePublished content="2021-07-15T10:25:04+05:30"><meta itemprop=dateModified content="2021-07-15T10:25:04+05:30"><meta itemprop=wordCount content="1674"><meta itemprop=keywords content="postgresql,real-time,analytics,sql,time series,timescaledb,grafana,"><meta name=twitter:card content="summary"><meta name=twitter:title content="Near real time API Monitoring with Grafana and PostgreSQL"><meta name=twitter:description content="Introduction Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &ldquo;always on&rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.
What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?"><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.css><link rel="shortcut icon" href=/favicon.ico><script type=application/javascript>var doNotTrack=!1;doNotTrack||(window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga('create','UA-169012216-1','auto'),ga('send','pageview'))</script><script async src=https://www.google-analytics.com/analytics.js></script></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title="Learnings in IT" rel=home><div class="logo__item logo__text"><div class=logo__title>Learnings in IT</div><div class=logo__tagline>A Simple Technical Blog</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>Menu</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/about/><span class=menu__text>About</span></a></li><li class=menu__item><a class=menu__link href=/posts/><span class=menu__text>Blog</span></a></li><li class=menu__item><a class=menu__link href=/projects/><span class=menu__text>Projects</span></a></li><li class=menu__item><a class=menu__link href=https://gist.github.com/sachinsu><span class=menu__text>Gists</span></a></li><li class=menu__item><a class=menu__link href=/links/home><span class=menu__text>Useful Links</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>Near real time API Monitoring with Grafana and PostgreSQL</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>Sachin Sunkle</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2021-07-15T10:25:04+05:30>Jul 15 2021</time></div></div></header><div class="post__toc toc"><div class=toc__title>Page content</div><div class=toc__menu><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#approach>Approach</a></li><li><a href=#data-collection-and-storage>Data Collection and Storage</a><ul><li><a href=#timescaledb>TimescaleDB</a></li><li><a href=#fluentbit>FluentBit</a></li></ul></li><li><a href=#visualization>Visualization</a><ul><li><a href=#grafana>Grafana</a></li><li><a href=#useful-links-usefullinks>Useful links (#usefullinks)</a></li></ul></li></ul></nav></div></div><div class="content post__content clearfix"><h2 id=introduction>Introduction</h2><p>Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &ldquo;always on&rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.</p><p>What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?</p><p>Is it even possible to have near real time Monitoring of Application&rsquo;s behaviour using already-in-use technologies (like PostgreSQL) ?</p><p>In this post, Let us have walk through of (one among many) approach to provide Near real time monitoring of APIs using PostgreSQL and open source Visualization tool, Grafana.</p><p>Monitoring and more generically, &ldquo;Observability&rdquo; has three pillars. They are <strong>Logs</strong>, <strong>Metrics</strong> and <strong>traces</strong>. Many of the existing applications are producing either (mostly logs or traces) but seldom all. Hence, it is necessary to use existing logs/traces as basis for Metrics generation.</p><p>There are on-going developments With standards like Opentelemetry in this field. Some have even suggested ( <a href=https://logz.io/blog/opentracing-opencensus-opentelemetry-what-is-distributed-tracing/>here</a> & <a href=https://go.lightstep.com/register-tracing-will-replace-most-logging-webinar.html>here</a>) that traces (distributed) will eventually replace logging.</p><h2 id=approach>Approach</h2><p>The high level architecture looks like below,</p><figure><img src=/images/apparch.png><figcaption><h4>High Level Architecture</h4></figcaption></figure><p>Considering as-is state of Application Architecture and given the constraints (mentioned earlier), this post covers approach that is based upon,</p><ul><li><a href=https://www.postgresql.org>PostgreSQL</a> - Data store for Analytics and Reporting</li><li><a href=https://www.timescale.com>TimescaleDB</a> - Timescale plugin for PostgreSQL</li><li><a href=https://fluentbit.io/>FluentBit</a> - Processing of Web Server logs & forwarding to database</li><li><a href=https://grafana.com/oss/>Grafana</a> - Data Visualization and Monitoring platform.</li></ul><p>Lets see how to get this done step by step.</p><h2 id=data-collection-and-storage>Data Collection and Storage</h2><h3 id=timescaledb>TimescaleDB</h3><p><a href=https://www.timescale.com>Timescale</a> is a Postgresql Plugin for time-series data management.</p><p><em>Rationale</em></p><ul><li>The reports and dashboards expected for near real time API monitoring are time intensive in nature.</li><li>TimescaleDB is optimized for such <a href=http://softwareengineeringdaily.com/wp-content/uploads/2021/06/SED1289-Mike-Freedman.pdf>time intensive reporting</a> and suits well for this use case as it is a plugin over PostgreSQL, which is already being used for analytics/reporting.</li></ul><p>Installation of plugin is straightforward. Step by Step <a href=https://docs.timescale.com/timescaledb/latest/how-to-guides/install-timescaledb/self-hosted/rhel-centos/installation-yum/#yum-installation>tutorial</a> is very helpful.</p><p>Next step is to create a database for the data to be used for Monitoring. <a href=https://docs.timescale.com/timescaledb/latest/overview/core-concepts/hypertables-and-chunks/>Hyper table(s)</a> in this database will contain Metrics data, collected from Application and web server (IIS).</p><p>One of the required dashboard/report was to monitor API request(s) in terms of success & failure (%), Response times (in buckets like 1-5 secs,5-10 secs and so on).</p><p>For each API request, application collects specific details and persists it in database. Currently below attributes are stored in database as part of each log entry,</p><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:center>Time</td><td style=text-align:left>Timestamp of event</td></tr><tr><td style=text-align:center>Service</td><td style=text-align:left>Attribute indicating service name</td></tr><tr><td style=text-align:center>Operation</td><td style=text-align:left>Attribute indicating Operation of the service for which request was received</td></tr><tr><td style=text-align:center>Outcome</td><td style=text-align:left>Outcome of the API Invocation i.e. Success or Failure</td></tr><tr><td style=text-align:center>Timeout</td><td style=text-align:left>Timestamp of completion of API invocation</td></tr></tbody></table><p>The DDL command will look like,</p><pre><code>
create table apilog
(time    timestamptz  not null,
 service  text not null,
 operation  text, 
 outcome  text not null,
 timeout  timestamptz );

</code></pre><p>After creating the table, it will have to be converted into <a href=https://docs.timescale.com/timescaledb/latest/how-to-guides/hypertables/><strong>Hypertable</strong></a> by using command,</p><p><code>SELECT create_hypertable('apilog', 'time');</code></p><p>Note: Timescale transparently manages storage for hyper table and PostgreSQL Developer can continue to use standard SQL/plpgsql with it.</p><p>For the sake of quick testing, One can add dummy data to this table using below SQL,</p><pre><code>insert into apilog
SELECT (current_timestamp - '0 day'::interval), (case when x = 1 then 'finance' 
                                  else 'it' end),(case when x = 1 then 'getPrices' 
                                  else 'getUptime' end),  (case when x &lt; 2 then 'success' else 'failure' end),   (current_timestamp - '0 day'::interval) + trunc(random()  * 20) * '1 second'::interval FROM generate_series(0, 5000, 5) AS t(x);

</code></pre><p>Currently, Application generates log events in OLTP Database and data from this database is replicated to Reporting database. Since we have created new Hyper table to host this data,a simple approach of <a href=https://www.postgresql.org/docs/13/sql-createtrigger.html>Trigger</a> can be used to populate it from current table.</p><p>In real scenario, you may want to consider replicating the data directly to hyper table.</p><h3 id=fluentbit>FluentBit</h3><p>So far , we have collected Application logs in the database. There is one more source which is of importance in the context of Monitoring and that is infrastructure software. It could be Operating System, Web Servers and so on. They generate lot of logs and metrices that can be ingested and consumed in conjuction with Application log to get better picture. We will look at how Web server logs can be sourced in data source.</p><p>There are many monitoring tools (refer <a href=#usefullinks>Useful links</a> below for comparison) available with focus on IT and Network monitoring. Such tools readily include infrastructure software too. For the sake of this article, We can use Log collector tool for this purpose. As such there are many log collector tools available, we will use <a href=https://fluentbit.io>Fluentbit</a>.At a very High level, It has concepts of,</p><ul><li>Input - Log sources</li><li>Parsers - Plugins to parse & transform the logs</li><li>Output - Log Destination like Prometheus, Kafka, PostgreSQL and so on.</li></ul><p>Some of the advantages of Fluentbit are,</p><ul><li>High log delivery performance with efficient resource utilization</li><li>Robut and Lightweight approach</li><li>Log enrichment at Node level itself than on the destination</li><li>Simpler configuration format</li></ul><p>Setup FluentBit - Fluentbit provides binaries that are bundled with package managers in case of Linux and as installers for Windows.</p><p>As of writing of this post, Pre-built binaries do not include output plugin for PostgreSQL. So Fluentbit has to be built from source after modifying <a href=https://github.com/fluent/fluent-bit/blob/master/CMakeLists.txt>Cmakelist</a> so,</p><ul><li><p>Clone the github repository</p></li><li><p>Modify <code>CMakeLists.txt</code> file as below,</p><p><code>option(FLB_OUT_PGSQL "Enable PostgreSQL output plugin" No)</code></p><p>to</p><p><code>option(FLB_OUT_PGSQL "Enable PostgreSQL output plugin" Yes)</code></p></li><li><p>Refer to <a href=https://docs.fluentbit.io/manual/installation/getting-started-with-fluent-bit#compile-from-source-linux-windows-freebsd-macos>Compiling from Source</a> for further details.</p></li></ul><p>Configuration - Once fluentbit is installed, It needs to be configured to read Web server logs , parse them and push them to PostgreSQL.</p><p>Below is sample configuration to periodically read Web Server Logs (in <a href=https://www.w3.org/TR/WD-logfile>w3c log format</a>), parse and push them to PostgreSQL,</p><pre><code>[SERVICE]
    Flush        5
    Daemon       Off
    Log_Level    debug
    Log_File     d:\monitoring\fluentbit.log
    Parsers_File parsers.conf
    Parsers_File generated/parsers_generated.conf
    HTTP_Server  On
    HTTP_Listen  0.0.0.0
    HTTP_Port    2020

[INPUT]
    Name           tail
    Tag            format.iis
    Parser         dips-w3c
    path           d:\temp\iis.log
    DB             d:\temp\raw.iis.db                    


[OUTPUT]
    Name          pgsql
    Match         *
    Host          172.0.0.1
    Port          5432
    User          fluentbit
    Password      fluentbit
    Database      timescalepoc
    Table         iislogs
    Timestamp_Key time

</code></pre><p>Configuration for Parser is as below,</p><pre><code>[PARSER]
    Name           dips-w3c
    Format         regex
    Regex         ^(?&lt;time&gt;\d{4}-\d{2}-\d{2} \d{2}[\:\.]\d{2}[\:\.]\d{2}) (?&lt;serverip&gt;\S+) (?&lt;method&gt;\S+) (?&lt;uristem&gt;\S+) (?&lt;uriquery&gt;\S+) (?&lt;serverport&gt;\S+) (?&lt;username&gt;\S+) (?&lt;clientip&gt;\S+) (?&lt;userAgent&gt;\S+) (?&lt;referrer&gt;\S+) (?&lt;status&gt;\S+) (?&lt;substatus&gt;\S+) (?&lt;win32status&gt;\S+) (?&lt;timetaken&gt;\S+) (?&lt;useragent1&gt;\S+) (?&lt;auth&gt;\S+) (?&lt;contenttype&gt;\S+)
    Time_Key       time
    Time_Format    %F %T
    Time_Keep      True
    types          serverPort:integer httpStatus:integer httpSubStatus:integer win32Status:integer timetaken:integer

</code></pre><p>This parser basically uses Regular Expression to parse each line in log file into key - value pairs with data points of interest.</p><p>In terms of output, Fluentbit&rsquo;s Postgresql <a href=https://docs.fluentbit.io/manual/pipeline/outputs/postgresql>plugin</a> provisions the table itself with a structure that stores entire JSON in field as part of row. Either this table can be used as is or use &ldquo;Before insert&rdquo; trigger as suggested by Fluentbit&rsquo;s manual to parse the Json and populate separate table.</p><p>Fluentbit can be easily configured to run as daemon (on Linux) or Service (on windows).</p><h2 id=visualization>Visualization</h2><p>With data getting added to timescaledb Hyper table,Lets see how it can be visualized.</p><p>Typically, there are 2 approaches to be considered for Visualization,</p><ul><li><p>Custom-built Web UI - This only makes sense if,</p><ul><li>There is already a Reporting/Visualization Web UI in place and adding new dashboards/reports is not much pain</li><li>Not much customization and/or slicing-dicing is expected.</li><li>Limited Efforts available.</li></ul></li><li><p>Off the shelf Tools - This approach makes sense if,</p></li><li><p>It is expected that Monitoring dashboards should be flexible and provide ease of customization by business or power users.</p></li><li><p>Additional dashboards are expected or can be provisioned with minimal or no coding.</p></li></ul><p>There are many paid and open source tools available. Notable OSS options are,</p><ul><li><a href=https://grafana.com/oss/>Grafana</a> - Tailor made for Monitoring and extensive analysis of Time series data.</li><li><a href=https://superset.apache.org/>Apache Superset</a> - open-source application for data exploration and data visualization able to handle data at petabyte scale.</li></ul><p>Lets see how Grafana can be used for visualization (Probably, i may evaluate superset some time and update this post.)</p><h3 id=grafana>Grafana</h3><p>Grafana has multiple offerings and one of them being Open source, Self-hosted Application. It has <a href=https://golang.org>Go</a> backend and is very easy to install. For Windows, Just follow the steps at <a href=https://grafana.com/docs/grafana/latest/installation/windows/>Installation</a>.</p><p>Once grafana is setup, one can quickly start it by running <code>grafana-server</code>. By default, it starts Web server at port <code>3000</code>. With Grafana Web-based GUI up and running, lets perform below steps to get dashboard in place.</p><ul><li>Connectivity to PostgreSQL - One needs to add Data Source in Grafana which in this case is PostgreSQL Database. It can be added from sidebar on Grafana UI, by hovering over &ldquo;Configuration&rdquo; option. In below screenshot, it shows configuration.</li></ul><figure><img src=/images/grafana1.png><figcaption><h4>Grafana: Connect to PostgreSQL</h4></figcaption></figure><ul><li>Add Dashboard - Once the Data source is setup, next step is to add a dashboard. Dashboard essentially is a visualization or a report. It has Query (in this case SQL Query) to fetch the data. Below screenshot shows configuration of simple query for Dashboard,</li></ul><figure><img src=/images/grafana2.png><figcaption><h4>Grafana: Query for Dashboard</h4></figcaption></figure><p>Grafana requires certain functions to be included (like <code>$__time(..)</code> and <code>$__timeFilter(..)</code>) in query so as to facilitate filtering/ordering by user through UI, like shown below,</p><figure><img src=/images/grafana3.png><figcaption><h4>Grafana: View data and apply Filter</h4></figcaption></figure><p>Grafana provides extensive ways to transform on the data fetched by SQL Query. This feature is more aimed at business and power user who may want to perform additional analysis on it. Alternative is to provide desired SQL and get the visualization like Time series or Graph as shown below,</p><figure><img src=/images/grafana4.png><figcaption><h4>Grafana: Complex SQL Query with minimal transformation</h4></figcaption></figure><figure><img src=/images/grafana5.png><figcaption><h4>Grafana: Time Series Visualization</h4></figcaption></figure><p>Note that there are many more features provided by Grafana (in terms of transformations, Visualization options, Access Control to UI itself and so on.</p><p>Key points with this approach are,</p><ul><li>Leveraging tools/products currently in use.</li><li>Greater Flexibility in Visualization over custom built tool containing canned reports/graphs</li><li>Lesser learning curve than inducting new tools.</li></ul><p>This post barely touches surface of what each of the individual tools mentioned have on offer, one would do well to go through their <a href=https://grafana.com/docs/>documentation</a> to derive most value out of it.</p><h3 id=useful-links-usefullinks>Useful links (#usefullinks)</h3><ul><li><a href=https://sematext.com/blog/infrastructure-monitoring-tools/>Comparison of IT Monitoring tools</a></li><li><a href="https://www.youtube.com/watch?v=_OXYCzwFd1Y">Nice Introduction to Modern Observability</a></li></ul><p>Happy Coding !!</p><hr><script src=https://utteranc.es/client.js repo=sachinsu/sachinsu.github.io issue-term=title label=blogcomment theme=github-light crossorigin=anonymous async></script></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/postgresql/ rel=tag>postgresql</a></li><li class=tags__item><a class="tags__link btn" href=/tags/real-time/ rel=tag>real-time</a></li><li class=tags__item><a class="tags__link btn" href=/tags/analytics/ rel=tag>analytics</a></li><li class=tags__item><a class="tags__link btn" href=/tags/sql/ rel=tag>sql</a></li><li class=tags__item><a class="tags__link btn" href=/tags/time-series/ rel=tag>time series</a></li><li class=tags__item><a class="tags__link btn" href=/tags/timescaledb/ rel=tag>timescaledb</a></li><li class=tags__item><a class="tags__link btn" href=/tags/grafana/ rel=tag>grafana</a></li></ul></div></footer></article></main><div class="authorbox clearfix"><div class=authorbox__header><span class=authorbox__name>About Sachin Sunkle</span></div><div class=authorbox__description>A Coder in IT</div></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/posts/apiupgrade/ rel=prev><span class=pager__subtitle>Â«&#8201;Previous</span><p class=pager__title>Upgrading API: Learnings</p></a></div></nav></div></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2021 Sachin Sunkle.
<span class=footer__copyright-credits>Generated with <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> and <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> theme.</span></div></div></footer></div><script async defer src=/js/menu.js></script></body></html>