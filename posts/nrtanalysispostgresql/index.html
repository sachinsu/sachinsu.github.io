<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Near real time API Monitoring with Grafana and PostgreSQL | Learnings in IT</title><meta name=keywords content="postgresql,real-time,analytics,sql,time series,timescaledb,grafana"><meta name=description content="Introduction
Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &ldquo;always on&rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.
What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?"><meta name=author content="Sachin Sunkle"><link rel=canonical href=https://sachinsu.github.io/posts/nrtanalysispostgresql/><link crossorigin=anonymous href=/assets/css/stylesheet.8fe10233a706bc87f2e08b3cf97b8bd4c0a80f10675a143675d59212121037c0.css integrity="sha256-j+ECM6cGvIfy4Is8+XuL1MCoDxBnWhQ2ddWSEhIQN8A=" rel="preload stylesheet" as=style><link rel=icon href=https://sachinsu.github.io/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://sachinsu.github.io/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://sachinsu.github.io/favicon-32x32.png><link rel=apple-touch-icon href=https://sachinsu.github.io/apple-touch-icon.png><link rel=mask-icon href=https://sachinsu.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://sachinsu.github.io/posts/nrtanalysispostgresql/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://sachinsu.github.io/posts/nrtanalysispostgresql/"><meta property="og:site_name" content="Learnings in IT"><meta property="og:title" content="Near real time API Monitoring with Grafana and PostgreSQL"><meta property="og:description" content="Introduction Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being “always on” (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.
What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2021-07-15T10:25:04+05:30"><meta property="article:modified_time" content="2021-07-15T10:25:04+05:30"><meta property="article:tag" content="Postgresql"><meta property="article:tag" content="Real-Time"><meta property="article:tag" content="Analytics"><meta property="article:tag" content="SQL"><meta property="article:tag" content="Time Series"><meta property="article:tag" content="Timescaledb"><meta property="og:image" content="https://sachinsu.github.io/images/papermod-cover.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://sachinsu.github.io/images/papermod-cover.png"><meta name=twitter:title content="Near real time API Monitoring with Grafana and PostgreSQL"><meta name=twitter:description content="Introduction
Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &ldquo;always on&rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.
What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://sachinsu.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Near real time API Monitoring with Grafana and PostgreSQL","item":"https://sachinsu.github.io/posts/nrtanalysispostgresql/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Near real time API Monitoring with Grafana and PostgreSQL","name":"Near real time API Monitoring with Grafana and PostgreSQL","description":"Introduction Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being \u0026ldquo;always on\u0026rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.\nWhat if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?\n","keywords":["postgresql","real-time","analytics","sql","time series","timescaledb","grafana"],"articleBody":"Introduction Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being “always on” (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.\nWhat if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?\nIs it even possible to have near real time Monitoring of Application’s behaviour using already-in-use technologies (like PostgreSQL) ?\nMonitoring and more generically, “Observability” has three pillars. They are Logs, Metrics and traces. Many of the existing applications are producing either (mostly logs or traces) but seldom all. Hence, it is necessary to use existing logs/traces as basis for Metrics generation.\nThere are on-going developments With standards like Opentelemetry in this field. Some have even suggested ( here \u0026 here) that traces (distributed) will eventually replace logging.\nApproach The high level architecture looks like below,\nHigh Level Architecture Considering as-is state of Application Architecture and given the constraints (mentioned earlier), this post covers approach that is based upon,\nPostgreSQL - Data store for Analytics and Reporting TimescaleDB - Timescale plugin for PostgreSQL FluentBit - Processing of Web Server logs \u0026 forwarding to database Grafana - Data Visualization and Monitoring platform. Lets see how to get this done step by step.\nData Collection and Storage TimescaleDB Timescale is a Postgresql Plugin for time-series data management.\nRationale\nThe reports and dashboards expected for near real time API monitoring are time intensive in nature. TimescaleDB is optimized for such time intensive reporting and suits well for this use case as it is a plugin over PostgreSQL, which is already being used for analytics/reporting. Installation of plugin is straightforward. Step by Step tutorial is very helpful.\nNext step is to create a database for the data to be used for Monitoring. Hyper table(s) in this database will contain Metrics data, collected from Application and web server (IIS).\nOne of the required dashboard/report was to monitor API request(s) in terms of success \u0026 failure (%), Response times (in buckets like 1-5 secs,5-10 secs and so on).\nFor each API request, application collects specific details and persists it in database. Currently below attributes are stored in database as part of each log entry,\nAttribute Description Time Timestamp of event Service Attribute indicating service name Operation Attribute indicating Operation of the service for which request was received Outcome Outcome of the API Invocation i.e. Success or Failure Timeout Timestamp of completion of API invocation The DDL command will look like,\ncreate table apilog (time timestamptz not null, service text not null, operation text, outcome text not null, timeout timestamptz ); After creating the table, it will have to be converted into Hypertable by using command,\nSELECT create_hypertable('apilog', 'time');\nNote: Timescale transparently manages storage for hyper table and PostgreSQL Developer can continue to use standard SQL/plpgsql with it.\nFor the sake of quick testing, One can add dummy data to this table using below SQL,\ninsert into apilog SELECT (current_timestamp - '0 day'::interval), (case when x = 1 then 'finance' else 'it' end),(case when x = 1 then 'getPrices' else 'getUptime' end), (case when x \u003c 2 then 'success' else 'failure' end), (current_timestamp - '0 day'::interval) + trunc(random() * 20) * '1 second'::interval FROM generate_series(0, 5000, 5) AS t(x); Currently, Application generates log events in OLTP Database and data from this database is replicated to Reporting database. Since we have created new Hyper table to host this data,a simple approach of Trigger can be used to populate it from current table.\nIn real scenario, you may want to consider replicating the data directly to hyper table.\nFluentBit So far , we have collected Application logs in the database. There is one more source which is of importance in the context of Monitoring and that is infrastructure software. It could be Operating System, Web Servers and so on. They generate lot of logs and metrices that can be ingested and consumed in conjuction with Application log to get better picture. We will look at how Web server logs can be sourced in data source.\nThere are many monitoring tools (refer Useful links below for comparison) available with focus on IT and Network monitoring. Such tools readily include infrastructure software too. For the sake of this article, We can use Log collector tool for this purpose. As such there are many log collector tools available, we will use Fluentbit.At a very High level, It has concepts of,\nInput - Log sources Parsers - Plugins to parse \u0026 transform the logs Output - Log Destination like Prometheus, Kafka, PostgreSQL and so on. Some of the advantages of Fluentbit are,\nHigh log delivery performance with efficient resource utilization Robut and Lightweight approach Log enrichment at Node level itself than on the destination Simpler configuration format Setup FluentBit - Fluentbit provides binaries that are bundled with package managers in case of Linux and as installers for Windows.\nAs of writing of this post, Pre-built binaries do not include output plugin for PostgreSQL. So Fluentbit has to be built from source after modifying Cmakelist so,\nClone the github repository\nModify CMakeLists.txt file as below,\noption(FLB_OUT_PGSQL \"Enable PostgreSQL output plugin\" No)\nto\noption(FLB_OUT_PGSQL \"Enable PostgreSQL output plugin\" Yes)\nRefer to Compiling from Source for further details.\nConfiguration - Once fluentbit is installed, It needs to be configured to read Web server logs , parse them and push them to PostgreSQL.\nBelow is sample configuration to periodically read Web Server Logs (in w3c log format), parse and push them to PostgreSQL,\n[SERVICE] Flush 5 Daemon Off Log_Level debug Log_File d:\\monitoring\\fluentbit.log Parsers_File parsers.conf Parsers_File generated/parsers_generated.conf HTTP_Server On HTTP_Listen 0.0.0.0 HTTP_Port 2020 [INPUT] Name tail Tag format.iis Parser dips-w3c path d:\\temp\\iis.log DB d:\\temp\\raw.iis.db [OUTPUT] Name pgsql Match * Host 172.0.0.1 Port 5432 User fluentbit Password fluentbit Database timescalepoc Table iislogs Timestamp_Key time Configuration for Parser is as below,\n[PARSER] Name dips-w3c Format regex Regex ^(?\\d{4}-\\d{2}-\\d{2} \\d{2}[\\:\\.]\\d{2}[\\:\\.]\\d{2}) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) (?\\S+) Time_Key time Time_Format %F %T Time_Keep True types serverPort:integer httpStatus:integer httpSubStatus:integer win32Status:integer timetaken:integer This parser basically uses Regular Expression to parse each line in log file into key - value pairs with data points of interest.\nIn terms of output, Fluentbit’s Postgresql plugin provisions the table itself with a structure that stores entire JSON in field as part of row. Either this table can be used as is or use “Before insert” trigger as suggested by Fluentbit’s manual to parse the Json and populate separate table.\nFluentbit can be easily configured to run as daemon (on Linux) or Service (on windows).\nVisualization With data getting added to timescaledb Hyper table,Lets see how it can be visualized.\nTypically, there are 2 approaches to be considered for Visualization,\nCustom-built Web UI - This only makes sense if,\nThere is already a Reporting/Visualization Web UI in place and adding new dashboards/reports is not much pain Not much customization and/or slicing-dicing is expected. Limited Efforts available. Off the shelf Tools - This approach makes sense if,\nIt is expected that Monitoring dashboards should be flexible and provide ease of customization by business or power users.\nAdditional dashboards are expected or can be provisioned with minimal or no coding.\nThere are many paid and open source tools available. Notable OSS options are,\nGrafana - Tailor made for Monitoring and extensive analysis of Time series data. Apache Superset - open-source application for data exploration and data visualization able to handle data at petabyte scale. Lets see how Grafana can be used for visualization (Probably, i may evaluate superset some time and update this post.)\nGrafana Grafana has multiple offerings and one of them being Open source, Self-hosted Application. It has Go backend and is very easy to install. For Windows, Just follow the steps at Installation.\nOnce grafana is setup, one can quickly start it by running grafana-server. By default, it starts Web server at port 3000. With Grafana Web-based GUI up and running, lets perform below steps to get dashboard in place.\nConnectivity to PostgreSQL - One needs to add Data Source in Grafana which in this case is PostgreSQL Database. It can be added from sidebar on Grafana UI, by hovering over “Configuration” option. In below screenshot, it shows configuration. Grafana: Connect to PostgreSQL Add Dashboard - Once the Data source is setup, next step is to add a dashboard. Dashboard essentially is a visualization or a report. It has Query (in this case SQL Query) to fetch the data. Below screenshot shows configuration of simple query for Dashboard, Grafana: Query for Dashboard Grafana requires certain functions to be included (like $__time(..) and $__timeFilter(..)) in query so as to facilitate filtering/ordering by user through UI, like shown below,\nGrafana: View data and apply Filter Grafana provides extensive ways to transform on the data fetched by SQL Query. This feature is more aimed at business and power user who may want to perform additional analysis on it. Alternative is to provide desired SQL and get the visualization like Time series or Graph as shown below,\nGrafana: Complex SQL Query with minimal transformation Grafana: Time Series Visualization Note that there are many more features provided by Grafana (in terms of transformations, Visualization options, Access Control to UI itself and so on.\nKey points with this approach are,\nLeveraging tools/products currently in use. Greater Flexibility in Visualization over custom built tool containing canned reports/graphs Lesser learning curve than inducting new tools. This post barely touches surface of what each of the individual tools mentioned have on offer, one would do well to go through their documentation to derive most value out of it.\nUseful links (#usefullinks) Comparison of IT Monitoring tools Nice Introduction to Modern Observability Happy Coding !!\n","wordCount":"1645","inLanguage":"en","image":"https://sachinsu.github.io/images/papermod-cover.png","datePublished":"2021-07-15T10:25:04+05:30","dateModified":"2021-07-15T10:25:04+05:30","author":{"@type":"Person","name":"Sachin Sunkle"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://sachinsu.github.io/posts/nrtanalysispostgresql/"},"publisher":{"@type":"Organization","name":"Learnings in IT","logo":{"@type":"ImageObject","url":"https://sachinsu.github.io/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://sachinsu.github.io/ accesskey=h title="Learnings in IT (Alt + H)">Learnings in IT</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)" aria-label="Toggle theme">
<svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg>
<svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://sachinsu.github.io/about/ title=About><span>About</span></a></li><li><a href=https://sachinsu.github.io/posts/ title=Blog><span>Blog</span></a></li><li><a href=https://gist.github.com/sachinsu title=Gists><span>Gists</span>&nbsp;
<svg fill="none" shape-rendering="geometricPrecision" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2.5" viewBox="0 0 24 24" height="12" width="12"><path d="M18 13v6a2 2 0 01-2 2H5a2 2 0 01-2-2V8a2 2 0 012-2h6"/><path d="M15 3h6v6"/><path d="M10 14 21 3"/></svg></a></li><li><a href=https://sachinsu.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://sachinsu.github.io/links/home title="Useful Links"><span>Useful Links</span></a></li><li><a href=https://sachinsu.github.io/tags title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://sachinsu.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://sachinsu.github.io/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Near real time API Monitoring with Grafana and PostgreSQL</h1><div class=post-meta><span title='2021-07-15 10:25:04 +0530 +0530'>July 15, 2021</span>&nbsp;·&nbsp;8 min&nbsp;·&nbsp;Sachin Sunkle</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#introduction aria-label=Introduction>Introduction</a></li><li><a href=#approach aria-label=Approach>Approach</a></li><li><a href=#data-collection-and-storage aria-label="Data Collection and Storage">Data Collection and Storage</a><ul><li><a href=#timescaledb aria-label=TimescaleDB>TimescaleDB</a></li><li><a href=#fluentbit aria-label=FluentBit>FluentBit</a></li></ul></li><li><a href=#visualization aria-label=Visualization>Visualization</a><ul><li><a href=#grafana aria-label=Grafana>Grafana</a></li><li><a href=#useful-links-usefullinks aria-label="Useful links (#usefullinks)">Useful links (#usefullinks)</a></li></ul></li></ul></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Suppose you have a distributed application running in production and it is based on Micro services/Service Oriented Architecture and have SLA of being &ldquo;always on&rdquo; (be available 24*7, barring deployments of course !!). In such cases, having proper monitoring of Application health in place is absolutely essential.</p><p>What if Monitoring is an afterthought (i.e. application is already in production) ? and that there is little apetite for additional components like (Visualization tools, specialized storage for logs/metrics/traces) for monitoring?</p><p>Is it even possible to have near real time Monitoring of Application&rsquo;s behaviour using already-in-use technologies (like PostgreSQL) ?</p><p>Monitoring and more generically, &ldquo;Observability&rdquo; has three pillars. They are <strong>Logs</strong>, <strong>Metrics</strong> and <strong>traces</strong>. Many of the existing applications are producing either (mostly logs or traces) but seldom all. Hence, it is necessary to use existing logs/traces as basis for Metrics generation.</p><p>There are on-going developments With standards like Opentelemetry in this field. Some have even suggested ( <a href=https://logz.io/blog/opentracing-opencensus-opentelemetry-what-is-distributed-tracing/ target=_blank rel="noopener noreferrer">here</a> & <a href=https://go.lightstep.com/register-tracing-will-replace-most-logging-webinar.html target=_blank rel="noopener noreferrer">here</a>) that traces (distributed) will eventually replace logging.</p><h2 id=approach>Approach<a hidden class=anchor aria-hidden=true href=#approach>#</a></h2><p>The high level architecture looks like below,</p><figure><img loading=lazy src=/images/nrt_architecture.png><figcaption>High Level Architecture</figcaption></figure><p>Considering as-is state of Application Architecture and given the constraints (mentioned earlier), this post covers approach that is based upon,</p><ul><li><a href=https://www.postgresql.org target=_blank rel="noopener noreferrer">PostgreSQL</a> - Data store for Analytics and Reporting</li><li><a href=https://www.timescale.com target=_blank rel="noopener noreferrer">TimescaleDB</a> - Timescale plugin for PostgreSQL</li><li><a href=https://fluentbit.io/ target=_blank rel="noopener noreferrer">FluentBit</a> - Processing of Web Server logs & forwarding to database</li><li><a href=https://grafana.com/oss/ target=_blank rel="noopener noreferrer">Grafana</a> - Data Visualization and Monitoring platform.</li></ul><p>Lets see how to get this done step by step.</p><h2 id=data-collection-and-storage>Data Collection and Storage<a hidden class=anchor aria-hidden=true href=#data-collection-and-storage>#</a></h2><h3 id=timescaledb>TimescaleDB<a hidden class=anchor aria-hidden=true href=#timescaledb>#</a></h3><p><a href=https://www.timescale.com target=_blank rel="noopener noreferrer">Timescale</a> is a Postgresql Plugin for time-series data management.</p><p><em>Rationale</em></p><ul><li>The reports and dashboards expected for near real time API monitoring are time intensive in nature.</li><li>TimescaleDB is optimized for such <a href=http://softwareengineeringdaily.com/wp-content/uploads/2021/06/SED1289-Mike-Freedman.pdf target=_blank rel="noopener noreferrer">time intensive reporting</a> and suits well for this use case as it is a plugin over PostgreSQL, which is already being used for analytics/reporting.</li></ul><p>Installation of plugin is straightforward. Step by Step <a href=https://docs.timescale.com/timescaledb/latest/how-to-guides/install-timescaledb/self-hosted/rhel-centos/installation-yum/#yum-installation target=_blank rel="noopener noreferrer">tutorial</a> is very helpful.</p><p>Next step is to create a database for the data to be used for Monitoring. <a href=https://docs.timescale.com/timescaledb/latest/overview/core-concepts/hypertables-and-chunks/ target=_blank rel="noopener noreferrer">Hyper table(s)</a> in this database will contain Metrics data, collected from Application and web server (IIS).</p><p>One of the required dashboard/report was to monitor API request(s) in terms of success & failure (%), Response times (in buckets like 1-5 secs,5-10 secs and so on).</p><p>For each API request, application collects specific details and persists it in database. Currently below attributes are stored in database as part of each log entry,</p><table><thead><tr><th style=text-align:center>Attribute</th><th style=text-align:left>Description</th></tr></thead><tbody><tr><td style=text-align:center>Time</td><td style=text-align:left>Timestamp of event</td></tr><tr><td style=text-align:center>Service</td><td style=text-align:left>Attribute indicating service name</td></tr><tr><td style=text-align:center>Operation</td><td style=text-align:left>Attribute indicating Operation of the service for which request was received</td></tr><tr><td style=text-align:center>Outcome</td><td style=text-align:left>Outcome of the API Invocation i.e. Success or Failure</td></tr><tr><td style=text-align:center>Timeout</td><td style=text-align:left>Timestamp of completion of API invocation</td></tr></tbody></table><p>The DDL command will look like,</p><pre tabindex=0><code>
create table apilog
(time    timestamptz  not null,
 service  text not null,
 operation  text, 
 outcome  text not null,
 timeout  timestamptz );
</code></pre><p>After creating the table, it will have to be converted into <a href=https://docs.timescale.com/timescaledb/latest/how-to-guides/hypertables/ target=_blank rel="noopener noreferrer">Hypertable</a> by using command,</p><p><code>SELECT create_hypertable('apilog', 'time');</code></p><p>Note: Timescale transparently manages storage for hyper table and PostgreSQL Developer can continue to use standard SQL/plpgsql with it.</p><p>For the sake of quick testing, One can add dummy data to this table using below SQL,</p><pre tabindex=0><code>insert into apilog
SELECT (current_timestamp - &#39;0 day&#39;::interval), (case when x = 1 then &#39;finance&#39; 
                                  else &#39;it&#39; end),(case when x = 1 then &#39;getPrices&#39; 
                                  else &#39;getUptime&#39; end),  (case when x &lt; 2 then &#39;success&#39; else &#39;failure&#39; end),   (current_timestamp - &#39;0 day&#39;::interval) + trunc(random()  * 20) * &#39;1 second&#39;::interval FROM generate_series(0, 5000, 5) AS t(x);
</code></pre><p>Currently, Application generates log events in OLTP Database and data from this database is replicated to Reporting database. Since we have created new Hyper table to host this data,a simple approach of <a href=https://www.postgresql.org/docs/13/sql-createtrigger.html target=_blank rel="noopener noreferrer">Trigger</a> can be used to populate it from current table.</p><p>In real scenario, you may want to consider replicating the data directly to hyper table.</p><h3 id=fluentbit>FluentBit<a hidden class=anchor aria-hidden=true href=#fluentbit>#</a></h3><p>So far , we have collected Application logs in the database. There is one more source which is of importance in the context of Monitoring and that is infrastructure software. It could be Operating System, Web Servers and so on. They generate lot of logs and metrices that can be ingested and consumed in conjuction with Application log to get better picture. We will look at how Web server logs can be sourced in data source.</p><p>There are many monitoring tools (refer <a href=#usefullinks>Useful links</a> below for comparison) available with focus on IT and Network monitoring. Such tools readily include infrastructure software too. For the sake of this article, We can use Log collector tool for this purpose. As such there are many log collector tools available, we will use <a href=https://fluentbit.io target=_blank rel="noopener noreferrer">Fluentbit</a>.At a very High level, It has concepts of,</p><ul><li>Input - Log sources</li><li>Parsers - Plugins to parse & transform the logs</li><li>Output - Log Destination like Prometheus, Kafka, PostgreSQL and so on.</li></ul><p>Some of the advantages of Fluentbit are,</p><ul><li>High log delivery performance with efficient resource utilization</li><li>Robut and Lightweight approach</li><li>Log enrichment at Node level itself than on the destination</li><li>Simpler configuration format</li></ul><p>Setup FluentBit - Fluentbit provides binaries that are bundled with package managers in case of Linux and as installers for Windows.</p><p>As of writing of this post, Pre-built binaries do not include output plugin for PostgreSQL. So Fluentbit has to be built from source after modifying <a href=https://github.com/fluent/fluent-bit/blob/master/CMakeLists.txt target=_blank rel="noopener noreferrer">Cmakelist</a> so,</p><ul><li><p>Clone the github repository</p></li><li><p>Modify <code>CMakeLists.txt</code> file as below,</p><p><code>option(FLB_OUT_PGSQL "Enable PostgreSQL output plugin" No)</code></p><p>to</p><p><code>option(FLB_OUT_PGSQL "Enable PostgreSQL output plugin" Yes)</code></p></li><li><p>Refer to <a href=https://docs.fluentbit.io/manual/installation/getting-started-with-fluent-bit#compile-from-source-linux-windows-freebsd-macos target=_blank rel="noopener noreferrer">Compiling from Source</a> for further details.</p></li></ul><p>Configuration - Once fluentbit is installed, It needs to be configured to read Web server logs , parse them and push them to PostgreSQL.</p><p>Below is sample configuration to periodically read Web Server Logs (in <a href=https://www.w3.org/TR/WD-logfile target=_blank rel="noopener noreferrer">w3c log format</a>), parse and push them to PostgreSQL,</p><pre tabindex=0><code>[SERVICE]
    Flush        5
    Daemon       Off
    Log_Level    debug
    Log_File     d:\monitoring\fluentbit.log
    Parsers_File parsers.conf
    Parsers_File generated/parsers_generated.conf
    HTTP_Server  On
    HTTP_Listen  0.0.0.0
    HTTP_Port    2020

[INPUT]
    Name           tail
    Tag            format.iis
    Parser         dips-w3c
    path           d:\temp\iis.log
    DB             d:\temp\raw.iis.db                    


[OUTPUT]
    Name          pgsql
    Match         *
    Host          172.0.0.1
    Port          5432
    User          fluentbit
    Password      fluentbit
    Database      timescalepoc
    Table         iislogs
    Timestamp_Key time
</code></pre><p>Configuration for Parser is as below,</p><pre tabindex=0><code>[PARSER]
    Name           dips-w3c
    Format         regex
    Regex         ^(?&lt;time&gt;\d{4}-\d{2}-\d{2} \d{2}[\:\.]\d{2}[\:\.]\d{2}) (?&lt;serverip&gt;\S+) (?&lt;method&gt;\S+) (?&lt;uristem&gt;\S+) (?&lt;uriquery&gt;\S+) (?&lt;serverport&gt;\S+) (?&lt;username&gt;\S+) (?&lt;clientip&gt;\S+) (?&lt;userAgent&gt;\S+) (?&lt;referrer&gt;\S+) (?&lt;status&gt;\S+) (?&lt;substatus&gt;\S+) (?&lt;win32status&gt;\S+) (?&lt;timetaken&gt;\S+) (?&lt;useragent1&gt;\S+) (?&lt;auth&gt;\S+) (?&lt;contenttype&gt;\S+)
    Time_Key       time
    Time_Format    %F %T
    Time_Keep      True
    types          serverPort:integer httpStatus:integer httpSubStatus:integer win32Status:integer timetaken:integer
</code></pre><p>This parser basically uses Regular Expression to parse each line in log file into key - value pairs with data points of interest.</p><p>In terms of output, Fluentbit&rsquo;s Postgresql <a href=https://docs.fluentbit.io/manual/pipeline/outputs/postgresql target=_blank rel="noopener noreferrer">plugin</a> provisions the table itself with a structure that stores entire JSON in field as part of row. Either this table can be used as is or use &ldquo;Before insert&rdquo; trigger as suggested by Fluentbit&rsquo;s manual to parse the Json and populate separate table.</p><p>Fluentbit can be easily configured to run as daemon (on Linux) or Service (on windows).</p><h2 id=visualization>Visualization<a hidden class=anchor aria-hidden=true href=#visualization>#</a></h2><p>With data getting added to timescaledb Hyper table,Lets see how it can be visualized.</p><p>Typically, there are 2 approaches to be considered for Visualization,</p><ul><li><p>Custom-built Web UI - This only makes sense if,</p><ul><li>There is already a Reporting/Visualization Web UI in place and adding new dashboards/reports is not much pain</li><li>Not much customization and/or slicing-dicing is expected.</li><li>Limited Efforts available.</li></ul></li><li><p>Off the shelf Tools - This approach makes sense if,</p></li><li><p>It is expected that Monitoring dashboards should be flexible and provide ease of customization by business or power users.</p></li><li><p>Additional dashboards are expected or can be provisioned with minimal or no coding.</p></li></ul><p>There are many paid and open source tools available. Notable OSS options are,</p><ul><li><a href=https://grafana.com/oss/ target=_blank rel="noopener noreferrer">Grafana</a> - Tailor made for Monitoring and extensive analysis of Time series data.</li><li><a href=https://superset.apache.org/ target=_blank rel="noopener noreferrer">Apache Superset</a> - open-source application for data exploration and data visualization able to handle data at petabyte scale.</li></ul><p>Lets see how Grafana can be used for visualization (Probably, i may evaluate superset some time and update this post.)</p><h3 id=grafana>Grafana<a hidden class=anchor aria-hidden=true href=#grafana>#</a></h3><p>Grafana has multiple offerings and one of them being Open source, Self-hosted Application. It has <a href=https://golang.org target=_blank rel="noopener noreferrer">Go</a> backend and is very easy to install. For Windows, Just follow the steps at <a href=https://grafana.com/docs/grafana/latest/installation/windows/ target=_blank rel="noopener noreferrer">Installation</a>.</p><p>Once grafana is setup, one can quickly start it by running <code>grafana-server</code>. By default, it starts Web server at port <code>3000</code>. With Grafana Web-based GUI up and running, lets perform below steps to get dashboard in place.</p><ul><li>Connectivity to PostgreSQL - One needs to add Data Source in Grafana which in this case is PostgreSQL Database. It can be added from sidebar on Grafana UI, by hovering over &ldquo;Configuration&rdquo; option. In below screenshot, it shows configuration.</li></ul><figure><img loading=lazy src=/images/grafana1.png><figcaption>Grafana: Connect to PostgreSQL</figcaption></figure><ul><li>Add Dashboard - Once the Data source is setup, next step is to add a dashboard. Dashboard essentially is a visualization or a report. It has Query (in this case SQL Query) to fetch the data. Below screenshot shows configuration of simple query for Dashboard,</li></ul><figure><img loading=lazy src=/images/grafana2.png><figcaption>Grafana: Query for Dashboard</figcaption></figure><p>Grafana requires certain functions to be included (like <code>$__time(..)</code> and <code>$__timeFilter(..)</code>) in query so as to facilitate filtering/ordering by user through UI, like shown below,</p><figure><img loading=lazy src=/images/grafana3.png><figcaption>Grafana: View data and apply Filter</figcaption></figure><p>Grafana provides extensive ways to transform on the data fetched by SQL Query. This feature is more aimed at business and power user who may want to perform additional analysis on it. Alternative is to provide desired SQL and get the visualization like Time series or Graph as shown below,</p><figure><img loading=lazy src=/images/grafana4.png><figcaption>Grafana: Complex SQL Query with minimal transformation</figcaption></figure><figure><img loading=lazy src=/images/grafana5.png><figcaption>Grafana: Time Series Visualization</figcaption></figure><p>Note that there are many more features provided by Grafana (in terms of transformations, Visualization options, Access Control to UI itself and so on.</p><p>Key points with this approach are,</p><ul><li>Leveraging tools/products currently in use.</li><li>Greater Flexibility in Visualization over custom built tool containing canned reports/graphs</li><li>Lesser learning curve than inducting new tools.</li></ul><p>This post barely touches surface of what each of the individual tools mentioned have on offer, one would do well to go through their <a href=https://grafana.com/docs/ target=_blank rel="noopener noreferrer">documentation</a> to derive most value out of it.</p><h3 id=useful-links-usefullinks>Useful links (#usefullinks)<a hidden class=anchor aria-hidden=true href=#useful-links-usefullinks>#</a></h3><ul><li><a href=https://sematext.com/blog/infrastructure-monitoring-tools/ target=_blank rel="noopener noreferrer">Comparison of IT Monitoring tools</a></li><li><a href="https://www.youtube.com/watch?v=_OXYCzwFd1Y" target=_blank rel="noopener noreferrer">Nice Introduction to Modern Observability</a></li></ul><p>Happy Coding !!</p><hr><script src=https://utteranc.es/client.js repo=sachinsu/sachinsu.github.io issue-term=title label=blogcomment theme=github-light crossorigin=anonymous async></script></div><footer class=post-footer><ul class=post-tags><li><a href=https://sachinsu.github.io/tags/postgresql/>Postgresql</a></li><li><a href=https://sachinsu.github.io/tags/real-time/>Real-Time</a></li><li><a href=https://sachinsu.github.io/tags/analytics/>Analytics</a></li><li><a href=https://sachinsu.github.io/tags/sql/>SQL</a></li><li><a href=https://sachinsu.github.io/tags/time-series/>Time Series</a></li><li><a href=https://sachinsu.github.io/tags/timescaledb/>Timescaledb</a></li><li><a href=https://sachinsu.github.io/tags/grafana/>Grafana</a></li></ul><nav class=paginav><a class=prev href=https://sachinsu.github.io/posts/dbre/><span class=title>« Prev</span><br><span>Database Reliability Engineering - My Notes</span>
</a><a class=next href=https://sachinsu.github.io/posts/apiupgrade/><span class=title>Next »</span><br><span>Upgrading API: Learnings</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Near real time API Monitoring with Grafana and PostgreSQL on x" href="https://x.com/intent/tweet/?text=Near%20real%20time%20API%20Monitoring%20with%20Grafana%20and%20PostgreSQL&amp;url=https%3a%2f%2fsachinsu.github.io%2fposts%2fnrtanalysispostgresql%2f&amp;hashtags=postgresql%2creal-time%2canalytics%2csql%2ctimeseries%2ctimescaledb%2cgrafana"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Near real time API Monitoring with Grafana and PostgreSQL on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fsachinsu.github.io%2fposts%2fnrtanalysispostgresql%2f&amp;title=Near%20real%20time%20API%20Monitoring%20with%20Grafana%20and%20PostgreSQL&amp;summary=Near%20real%20time%20API%20Monitoring%20with%20Grafana%20and%20PostgreSQL&amp;source=https%3a%2f%2fsachinsu.github.io%2fposts%2fnrtanalysispostgresql%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Near real time API Monitoring with Grafana and PostgreSQL on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fsachinsu.github.io%2fposts%2fnrtanalysispostgresql%2f&title=Near%20real%20time%20API%20Monitoring%20with%20Grafana%20and%20PostgreSQL"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Near real time API Monitoring with Grafana and PostgreSQL on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fsachinsu.github.io%2fposts%2fnrtanalysispostgresql%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Near real time API Monitoring with Grafana and PostgreSQL on whatsapp" href="https://api.whatsapp.com/send?text=Near%20real%20time%20API%20Monitoring%20with%20Grafana%20and%20PostgreSQL%20-%20https%3a%2f%2fsachinsu.github.io%2fposts%2fnrtanalysispostgresql%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Near real time API Monitoring with Grafana and PostgreSQL on telegram" href="https://telegram.me/share/url?text=Near%20real%20time%20API%20Monitoring%20with%20Grafana%20and%20PostgreSQL&amp;url=https%3a%2f%2fsachinsu.github.io%2fposts%2fnrtanalysispostgresql%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentColor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Near real time API Monitoring with Grafana and PostgreSQL on ycombinator" href="https://news.ycombinator.com/submitlink?t=Near%20real%20time%20API%20Monitoring%20with%20Grafana%20and%20PostgreSQL&u=https%3a%2f%2fsachinsu.github.io%2fposts%2fnrtanalysispostgresql%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentColor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>©</span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
<a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentColor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script><script>document.querySelectorAll("pre > code").forEach(e=>{const n=e.parentNode.parentNode,t=document.createElement("button");t.classList.add("copy-code"),t.innerHTML="copy";function s(){t.innerHTML="copied!",setTimeout(()=>{t.innerHTML="copy"},2e3)}t.addEventListener("click",t=>{if("clipboard"in navigator){navigator.clipboard.writeText(e.textContent),s();return}const n=document.createRange();n.selectNodeContents(e);const o=window.getSelection();o.removeAllRanges(),o.addRange(n);try{document.execCommand("copy"),s()}catch{}o.removeRange(n)}),n.classList.contains("highlight")?n.appendChild(t):n.parentNode.firstChild==n||(e.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName=="TABLE"?e.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(t):e.parentNode.appendChild(t))})</script></body></html>